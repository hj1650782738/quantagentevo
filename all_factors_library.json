{
  "metadata": {
    "created_at": "2026-01-08T21:03:59.729617",
    "last_updated": "2026-01-16T01:15:48.319745",
    "total_factors": 74,
    "version": "1.0",
    "note": "Cleaned: removed 662 factors before line 15604"
  },
  "factors": {
    "0a7e73fd9b4c7d2f": {
      "factor_id": "0a7e73fd9b4c7d2f",
      "factor_name": "Smoothed_Conviction_Momentum_10D",
      "factor_expression": "TS_CORR($return, EMA(DELAY($return, 3) * ($volume / (TS_MEAN($volume, 10) + 1e-8)), 5), 10)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_CORR($return, EMA(DELAY($return, 3) * ($volume / (TS_MEAN($volume, 10) + 1e-8)), 5), 10)\" # Your output factor expression will be filled in here\n    name = \"Smoothed_Conviction_Momentum_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A refined version of the conviction lead-lag factor that applies an Exponential Moving Average (EMA) to the volume-scaled lagged return signal to reduce noise before calculating the 10-day correlation with current returns.",
      "experiment_id": "unknown",
      "round_number": 2,
      "hypothesis": "Hypothesis: The 10-day rolling correlation between current returns and a 3-day lagged, volume-scaled return signal captures high-conviction price discovery trends more effectively than longer-lag models.\n                Concise Observation: The previous 5-day lag and 20-day window were too slow to capture decaying lead-lag effects; shortening the lag to 3 days and using a volume ratio instead of a Z-score improved the Information Ratio and signal responsiveness.\n                Concise Justification: Shorter lags capture more immediate information diffusion, while scaling lagged returns by the volume ratio (current volume / mean volume) emphasizes price moves that occurred with high relative conviction, filtering out noise from low-liquidity sessions.\n                Concise Knowledge: If the lag period is reduced to 3 days and the lagged signal is scaled by the ratio of volume to its 10-day moving average, then the resulting correlation identifies short-term momentum backed by liquidity, which is more predictive of immediate price persistence.\n                concise Specification: Calculate the factor as the 10-day Pearson correlation between the daily return ($return) and a 'Conviction Signal', where the Conviction Signal is the 3-day lagged return multiplied by ($volume / TS_MEAN($volume, 10)).\n                ",
      "initial_direction": "Cross-Asset Lead-Lag Momentum: Construct a lead-lag network using rolling Granger causality between equity sector ETFs and their corresponding upstream commodity futures to capture macro-driven momentum spillover, testing if price trends in raw materials predict subsequent directional shifts in equity risk premia.",
      "user_initial_direction": "Cross-Asset Lead-Lag Momentum: Construct a lead-lag network using rolling Granger causality between equity sector ETFs and their corresponding upstream commodity futures to capture macro-driven momentum spillover, testing if price trends in raw materials predict subsequent directional shifts in equity risk premia.",
      "planning_direction": "Cross-Asset Lead-Lag Momentum: Construct a lead-lag network using rolling Granger causality between equity sector ETFs and their corresponding upstream commodity futures to capture macro-driven momentum spillover, testing if price trends in raw materials predict subsequent directional shifts in equity risk premia.",
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0064292131116171,
        "ICIR": 0.0471904782508434,
        "RankIC": 0.022961676702149,
        "RankICIR": 0.1745311944608633,
        "annualized_return": 0.0713249472038566,
        "information_ratio": 1.1379839206679387,
        "max_drawdown": -0.0918045186564081
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T18:54:09.625003",
      "updated_at": "2026-01-15T18:54:09.625009"
    },
    "67b61f8fdaf66e7f": {
      "factor_id": "67b61f8fdaf66e7f",
      "factor_name": "Ranked_Conviction_Discovery_10D",
      "factor_expression": "TS_CORR(RANK($return), RANK(DELAY($return, 3) * ($volume / (TS_MEAN($volume, 10) + 1e-8))), 10)",
      "factor_implementation_code": "",
      "factor_description": "This factor measures the cross-sectional rank correlation between current returns and the 3-day lagged conviction signal. Using RANK ensures the factor is robust to outliers in return and volume spikes while capturing the lead-lag discovery trend.",
      "experiment_id": "unknown",
      "round_number": 2,
      "hypothesis": "Hypothesis: The 10-day rolling correlation between current returns and a 3-day lagged, volume-scaled return signal captures high-conviction price discovery trends more effectively than longer-lag models.\n                Concise Observation: The previous 5-day lag and 20-day window were too slow to capture decaying lead-lag effects; shortening the lag to 3 days and using a volume ratio instead of a Z-score improved the Information Ratio and signal responsiveness.\n                Concise Justification: Shorter lags capture more immediate information diffusion, while scaling lagged returns by the volume ratio (current volume / mean volume) emphasizes price moves that occurred with high relative conviction, filtering out noise from low-liquidity sessions.\n                Concise Knowledge: If the lag period is reduced to 3 days and the lagged signal is scaled by the ratio of volume to its 10-day moving average, then the resulting correlation identifies short-term momentum backed by liquidity, which is more predictive of immediate price persistence.\n                concise Specification: Calculate the factor as the 10-day Pearson correlation between the daily return ($return) and a 'Conviction Signal', where the Conviction Signal is the 3-day lagged return multiplied by ($volume / TS_MEAN($volume, 10)).\n                ",
      "initial_direction": "Cross-Asset Lead-Lag Momentum: Construct a lead-lag network using rolling Granger causality between equity sector ETFs and their corresponding upstream commodity futures to capture macro-driven momentum spillover, testing if price trends in raw materials predict subsequent directional shifts in equity risk premia.",
      "user_initial_direction": "Cross-Asset Lead-Lag Momentum: Construct a lead-lag network using rolling Granger causality between equity sector ETFs and their corresponding upstream commodity futures to capture macro-driven momentum spillover, testing if price trends in raw materials predict subsequent directional shifts in equity risk premia.",
      "planning_direction": "Cross-Asset Lead-Lag Momentum: Construct a lead-lag network using rolling Granger causality between equity sector ETFs and their corresponding upstream commodity futures to capture macro-driven momentum spillover, testing if price trends in raw materials predict subsequent directional shifts in equity risk premia.",
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0064292131116171,
        "ICIR": 0.0471904782508434,
        "RankIC": 0.022961676702149,
        "RankICIR": 0.1745311944608633,
        "annualized_return": 0.0713249472038566,
        "information_ratio": 1.1379839206679387,
        "max_drawdown": -0.0918045186564081
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T18:54:09.644660",
      "updated_at": "2026-01-15T18:54:09.644666"
    },
    "b46eb9da2f77d837": {
      "factor_id": "b46eb9da2f77d837",
      "factor_name": "Price_Volume_Exhaustion_5D",
      "factor_expression": "TS_MEAN(($high - $low) / ($volume + 1e-8), 5)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_MEAN(($high - $low) / ($volume + 1e-8), 5)\" # Your output factor expression will be filled in here\n    name = \"Price_Volume_Exhaustion_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures intra-day price-volume exhaustion by calculating the 5-day rolling average of the high-low range relative to trading volume. High values indicate price movements with low liquidity support, suggesting a potential mean-reversion or reversal.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The daily price reversal strength is positively correlated with the degree of intra-day price-volume exhaustion, specifically when the ratio of the high-low price range to the trading volume reaches a local maximum, indicating a potential liquidity-driven turning point.\n                Concise Observation: In daily price-volume data, extreme price movements accompanied by relatively low volume often precede a correction, as the price lacks the 'conviction' or 'liquidity support' found in high-volume trends.\n                Concise Justification: Based on market microstructure theory, price-volume divergence (high volatility on low volume) signals a temporary imbalance where market makers or liquidity providers may have withdrawn, leading to an overextended price that is likely to revert to its mean.\n                Concise Knowledge: If a stock exhibits a significantly high price range relative to its trading volume over a fixed window, it suggests a lack of liquidity depth to support the price movement, increasing the probability of a mean-reversion event in subsequent periods.\n                concise Specification: The factor is defined as the 5-day rolling average of the daily range (High minus Low) divided by the daily Volume, where a higher value predicts a negative return for the next period due to the exhaustion of the current price trend.\n                ",
      "initial_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "user_initial_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "planning_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "evolution_phase": "original",
      "trajectory_id": "",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0005241961676058,
        "ICIR": 0.0038298779693881,
        "RankIC": 0.0139387733453035,
        "RankICIR": 0.1001325840594181,
        "annualized_return": -0.0043308748532143,
        "information_ratio": -0.0546152236058097,
        "max_drawdown": -0.2155202314430089
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T19:53:28.343173",
      "updated_at": "2026-01-15T19:53:28.343180"
    },
    "5d81b7886184ec03": {
      "factor_id": "5d81b7886184ec03",
      "factor_name": "Relative_Exhaustion_ZScore_10D",
      "factor_expression": "TS_ZSCORE(($high - $low) / ($volume + 1e-8), 10)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_ZSCORE(($volume > 0) ? (($high - $low) / $volume) : (0), 10)\" # Your output factor expression will be filled in here\n    name = \"Relative_Exhaustion_ZScore_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies extreme price-volume divergence by calculating the Z-score of the range-to-volume ratio over a 10-day window. It captures local maxima in price exhaustion relative to recent history, signaling a higher probability of reversal.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The daily price reversal strength is positively correlated with the degree of intra-day price-volume exhaustion, specifically when the ratio of the high-low price range to the trading volume reaches a local maximum, indicating a potential liquidity-driven turning point.\n                Concise Observation: In daily price-volume data, extreme price movements accompanied by relatively low volume often precede a correction, as the price lacks the 'conviction' or 'liquidity support' found in high-volume trends.\n                Concise Justification: Based on market microstructure theory, price-volume divergence (high volatility on low volume) signals a temporary imbalance where market makers or liquidity providers may have withdrawn, leading to an overextended price that is likely to revert to its mean.\n                Concise Knowledge: If a stock exhibits a significantly high price range relative to its trading volume over a fixed window, it suggests a lack of liquidity depth to support the price movement, increasing the probability of a mean-reversion event in subsequent periods.\n                concise Specification: The factor is defined as the 5-day rolling average of the daily range (High minus Low) divided by the daily Volume, where a higher value predicts a negative return for the next period due to the exhaustion of the current price trend.\n                ",
      "initial_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "user_initial_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "planning_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "evolution_phase": "original",
      "trajectory_id": "",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0005241961676058,
        "ICIR": 0.0038298779693881,
        "RankIC": 0.0139387733453035,
        "RankICIR": 0.1001325840594181,
        "annualized_return": -0.0043308748532143,
        "information_ratio": -0.0546152236058097,
        "max_drawdown": -0.2155202314430089
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T19:53:28.356681",
      "updated_at": "2026-01-15T19:53:28.356687"
    },
    "915269598344bf8f": {
      "factor_id": "915269598344bf8f",
      "factor_name": "Exhaustion_Rank_CrossSectional",
      "factor_expression": "RANK(($high - $low) / ($volume + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($high - $low) / ($volume + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Exhaustion_Rank_CrossSectional\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor ranks the intensity of price-volume exhaustion across the market. By comparing the current day's range-to-volume ratio against other stocks, it identifies assets most likely to undergo liquidity-driven corrections.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The daily price reversal strength is positively correlated with the degree of intra-day price-volume exhaustion, specifically when the ratio of the high-low price range to the trading volume reaches a local maximum, indicating a potential liquidity-driven turning point.\n                Concise Observation: In daily price-volume data, extreme price movements accompanied by relatively low volume often precede a correction, as the price lacks the 'conviction' or 'liquidity support' found in high-volume trends.\n                Concise Justification: Based on market microstructure theory, price-volume divergence (high volatility on low volume) signals a temporary imbalance where market makers or liquidity providers may have withdrawn, leading to an overextended price that is likely to revert to its mean.\n                Concise Knowledge: If a stock exhibits a significantly high price range relative to its trading volume over a fixed window, it suggests a lack of liquidity depth to support the price movement, increasing the probability of a mean-reversion event in subsequent periods.\n                concise Specification: The factor is defined as the 5-day rolling average of the daily range (High minus Low) divided by the daily Volume, where a higher value predicts a negative return for the next period due to the exhaustion of the current price trend.\n                ",
      "initial_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "user_initial_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "planning_direction": "结合订单流不平衡（Order Flow Imbalance）的微观结构均值回归：在分钟级频率下，通过分析买卖盘挂单深度与成交意向的背离，识别价格偏离价值中枢时的流动性枯竭点，在买卖压力失衡达到阈值时介入反向交易。",
      "evolution_phase": "original",
      "trajectory_id": "",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0005241961676058,
        "ICIR": 0.0038298779693881,
        "RankIC": 0.0139387733453035,
        "RankICIR": 0.1001325840594181,
        "annualized_return": -0.0043308748532143,
        "information_ratio": -0.0546152236058097,
        "max_drawdown": -0.2155202314430089
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T19:53:28.366184",
      "updated_at": "2026-01-15T19:53:28.366189"
    },
    "673a4e562722ed53": {
      "factor_id": "673a4e562722ed53",
      "factor_name": "Volatility_Squeeze_Mean_Reversion_20D",
      "factor_expression": "($close - TS_MEAN($close, 20)) / (TS_STD($return, 20) + 1e-8) * (TS_STD($return, 20) / (TS_STD($return, 5) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"($close - TS_MEAN($close, 20)) / (TS_STD($return, 20) + 1e-8) * (TS_STD($return, 20) / (TS_STD($return, 5) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Volatility_Squeeze_Mean_Reversion_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies mean-reversion opportunities by calculating the price deviation from its 20-day average, normalized by the 20-day return volatility. It specifically weights this deviation by the inverse ratio of short-term (5-day) to long-term (20-day) volatility to highlight 'squeeze' states where volatility is lower than its historical average.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A factor calculated as the z-score of the current price relative to its 20-day moving average, normalized by a 20-day rolling standard deviation of returns, will exhibit stronger mean-reversion properties when the current 5-day volatility is lower than its 20-day average volatility.\n                Concise Observation: Fixed-width technical indicators like standard Bollinger Bands fail to account for the clustering and autocorrelation of volatility, often leading to false signals during regime shifts from high to low volatility.\n                Concise Justification: Volatility clustering suggests that low volatility periods (squeezes) are often followed by sharp mean-reverting or trend-initiating moves; by normalizing price deviation by dynamic volatility, we isolate 'true' statistical outliers relative to current risk levels.\n                Concise Knowledge: If asset prices deviate significantly from their moving average during periods of volatility compression, then the probability of a mean-reverting price correction increases as the market transitions out of the low-volatility state.\n                concise Specification: Define the factor as (Close - MA(Close, 20)) / Std(Return, 20), specifically filtered or weighted by the ratio of Short_Vol(5-day) to Long_Vol(20-day) to capture the 'squeeze' effect.\n                ",
      "initial_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "user_initial_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "planning_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "evolution_phase": "original",
      "trajectory_id": "",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0061047381585373,
        "ICIR": 0.0403601737965465,
        "RankIC": 0.0232142943094437,
        "RankICIR": 0.1586538241658718,
        "annualized_return": 0.0825136238831514,
        "information_ratio": 1.0549314908771068,
        "max_drawdown": -0.1230675131067335
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T19:54:11.342759",
      "updated_at": "2026-01-15T19:54:11.342766"
    },
    "ed68473868d60341": {
      "factor_id": "ed68473868d60341",
      "factor_name": "Dynamic_ZScore_Volatility_Ratio_Factor",
      "factor_expression": "($close - TS_MEAN($close, 20)) / (TS_STD($close, 20) + 1e-8) * (TS_STD($return, 5) < TS_MEAN(TS_STD($return, 5), 20) ? 1.0 : 0.5)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"($close - TS_MEAN($close, 20)) / (TS_STD($close, 20) + 1e-8) * (TS_STD($return, 5) < TS_MEAN(TS_STD($return, 5), 20) ? 1.0 : 0.5)\" # Your output factor expression will be filled in here\n    name = \"Dynamic_ZScore_Volatility_Ratio_Factor\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures price outliers relative to current risk levels. It computes the price z-score against its 20-day mean and scales it based on whether the 5-day volatility is compressed relative to the 20-day volatility. A high absolute value suggests a significant price deviation during a period of volatility compression, signaling a potential mean-reverting correction.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A factor calculated as the z-score of the current price relative to its 20-day moving average, normalized by a 20-day rolling standard deviation of returns, will exhibit stronger mean-reversion properties when the current 5-day volatility is lower than its 20-day average volatility.\n                Concise Observation: Fixed-width technical indicators like standard Bollinger Bands fail to account for the clustering and autocorrelation of volatility, often leading to false signals during regime shifts from high to low volatility.\n                Concise Justification: Volatility clustering suggests that low volatility periods (squeezes) are often followed by sharp mean-reverting or trend-initiating moves; by normalizing price deviation by dynamic volatility, we isolate 'true' statistical outliers relative to current risk levels.\n                Concise Knowledge: If asset prices deviate significantly from their moving average during periods of volatility compression, then the probability of a mean-reverting price correction increases as the market transitions out of the low-volatility state.\n                concise Specification: Define the factor as (Close - MA(Close, 20)) / Std(Return, 20), specifically filtered or weighted by the ratio of Short_Vol(5-day) to Long_Vol(20-day) to capture the 'squeeze' effect.\n                ",
      "initial_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "user_initial_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "planning_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "evolution_phase": "original",
      "trajectory_id": "",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0061047381585373,
        "ICIR": 0.0403601737965465,
        "RankIC": 0.0232142943094437,
        "RankICIR": 0.1586538241658718,
        "annualized_return": 0.0825136238831514,
        "information_ratio": 1.0549314908771068,
        "max_drawdown": -0.1230675131067335
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T19:54:11.355500",
      "updated_at": "2026-01-15T19:54:11.355506"
    },
    "687450492c7d18c3": {
      "factor_id": "687450492c7d18c3",
      "factor_name": "Cross_Sectional_Squeeze_Outlier_20D",
      "factor_expression": "RANK(($close - TS_MEAN($close, 20)) / (TS_STD($return, 20) + 1e-8)) * (TS_STD($return, 5) / (TS_STD($return, 20) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($close - TS_MEAN($close, 20)) / (TS_STD($return, 20) + 1e-8)) * (TS_STD($return, 5) < TS_STD($return, 20))\" # Your output factor expression will be filled in here\n    name = \"Cross_Sectional_Squeeze_Outlier_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor ranks the price-to-moving-average deviation normalized by return volatility, but only for stocks currently experiencing a 'volatility squeeze' (5-day volatility < 20-day volatility). This helps isolate stocks with the highest probability of mean reversion across the market.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A factor calculated as the z-score of the current price relative to its 20-day moving average, normalized by a 20-day rolling standard deviation of returns, will exhibit stronger mean-reversion properties when the current 5-day volatility is lower than its 20-day average volatility.\n                Concise Observation: Fixed-width technical indicators like standard Bollinger Bands fail to account for the clustering and autocorrelation of volatility, often leading to false signals during regime shifts from high to low volatility.\n                Concise Justification: Volatility clustering suggests that low volatility periods (squeezes) are often followed by sharp mean-reverting or trend-initiating moves; by normalizing price deviation by dynamic volatility, we isolate 'true' statistical outliers relative to current risk levels.\n                Concise Knowledge: If asset prices deviate significantly from their moving average during periods of volatility compression, then the probability of a mean-reverting price correction increases as the market transitions out of the low-volatility state.\n                concise Specification: Define the factor as (Close - MA(Close, 20)) / Std(Return, 20), specifically filtered or weighted by the ratio of Short_Vol(5-day) to Long_Vol(20-day) to capture the 'squeeze' effect.\n                ",
      "initial_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "user_initial_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "planning_direction": "基于波动率自相关的动态置信区间均值回归：利用GARCH模型预测短期波动率，将传统的固定标准差布林带替换为动态波动率调整带，在市场进入低波动挤压状态后，捕捉价格回归条件均值的概率性修复。",
      "evolution_phase": "original",
      "trajectory_id": "",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0061047381585373,
        "ICIR": 0.0403601737965465,
        "RankIC": 0.0232142943094437,
        "RankICIR": 0.1586538241658718,
        "annualized_return": 0.0825136238831514,
        "information_ratio": 1.0549314908771068,
        "max_drawdown": -0.1230675131067335
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T19:54:11.364587",
      "updated_at": "2026-01-15T19:54:11.364593"
    },
    "d61babdc83a25a13": {
      "factor_id": "d61babdc83a25a13",
      "factor_name": "Amihud_Momentum_Interaction_10D",
      "factor_expression": "TS_MEAN(($high - $low) / ($volume + 1e-8), 10) * TS_PCTCHANGE($close, 5)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_MEAN(($high - $low) / ($volume + 1e-8), 10) * TS_PCTCHANGE($close, 5)\" # Your output factor expression will be filled in here\n    name = \"Amihud_Momentum_Interaction_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures short-term mean-reversion opportunities by identifying stocks with high price exhaustion. It multiplies the 10-day moving average of the Amihud illiquidity ratio (High-Low range / Volume) with the 5-day price momentum. High values suggest price moves driven by liquidity gaps rather than fundamentals, which are likely to reverse.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 10-day moving average of the ratio between daily high-low price range and volume (illiquidity proxy) combined with the 5-day price momentum identifies short-term mean-reversion opportunities caused by liquidity-driven price exhaustion.\n                Concise Observation: In daily price-volume data, extreme price movements often occur with disproportionately low volume, suggesting that the price discovery process is noisy and prone to overshooting in the absence of deep liquidity.\n                Concise Justification: The Amihud illiquidity ratio (High-Low range / Volume) captures the price impact of trading; high values indicate 'thin' markets where small trades move prices significantly, creating transitory price shocks that are statistically likely to mean-revert.\n                Concise Knowledge: If a stock experiences high price volatility on low relative volume, the price move is likely driven by liquidity gaps rather than fundamental shifts; when such illiquidity coincides with extreme short-term returns, a price reversal is expected as liquidity returns to the market.\n                concise Specification: Calculate the ratio of ($high - $low) to $volume as a daily illiquidity measure; define the factor as the 10-day simple moving average of this ratio multiplied by the 5-day cumulative return to capture the interaction between illiquidity and price trend.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "3978712eb575",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.000931362903187,
        "ICIR": 0.0060678875340603,
        "RankIC": 0.0129199155740874,
        "RankICIR": 0.0827167575975825,
        "annualized_return": 0.0197667402350589,
        "information_ratio": 0.2260536698445352,
        "max_drawdown": -0.267549976662951
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:01:53.369482",
      "updated_at": "2026-01-15T20:01:53.369490"
    },
    "7df879b89911aca3": {
      "factor_id": "7df879b89911aca3",
      "factor_name": "Ranked_Illiquidity_Exhaustion_Factor",
      "factor_expression": "RANK(TS_MEAN(($high - $low) / ($volume + 1e-8), 10)) * RANK(TS_SUM($return, 5))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(TS_MEAN(($high - $low) / ($volume + 1e-8), 10)) * RANK(TS_SUM($return, 5))\" # Your output factor expression will be filled in here\n    name = \"Ranked_Illiquidity_Exhaustion_Factor\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor normalizes the illiquidity-momentum interaction cross-sectionally. By ranking the 10-day average illiquidity and the 5-day price change independently before multiplication, it identifies relative liquidity-driven overextensions across the market, reducing the impact of extreme outliers in volume data.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 10-day moving average of the ratio between daily high-low price range and volume (illiquidity proxy) combined with the 5-day price momentum identifies short-term mean-reversion opportunities caused by liquidity-driven price exhaustion.\n                Concise Observation: In daily price-volume data, extreme price movements often occur with disproportionately low volume, suggesting that the price discovery process is noisy and prone to overshooting in the absence of deep liquidity.\n                Concise Justification: The Amihud illiquidity ratio (High-Low range / Volume) captures the price impact of trading; high values indicate 'thin' markets where small trades move prices significantly, creating transitory price shocks that are statistically likely to mean-revert.\n                Concise Knowledge: If a stock experiences high price volatility on low relative volume, the price move is likely driven by liquidity gaps rather than fundamental shifts; when such illiquidity coincides with extreme short-term returns, a price reversal is expected as liquidity returns to the market.\n                concise Specification: Calculate the ratio of ($high - $low) to $volume as a daily illiquidity measure; define the factor as the 10-day simple moving average of this ratio multiplied by the 5-day cumulative return to capture the interaction between illiquidity and price trend.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "3978712eb575",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.000931362903187,
        "ICIR": 0.0060678875340603,
        "RankIC": 0.0129199155740874,
        "RankICIR": 0.0827167575975825,
        "annualized_return": 0.0197667402350589,
        "information_ratio": 0.2260536698445352,
        "max_drawdown": -0.267549976662951
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:01:53.378509",
      "updated_at": "2026-01-15T20:01:53.378515"
    },
    "49987e85ff22fdb5": {
      "factor_id": "49987e85ff22fdb5",
      "factor_name": "ZScored_Amihud_Reversion_5D",
      "factor_expression": "TS_ZSCORE(($high - $low) / ($volume + 1e-8), 10) * TS_PCTCHANGE($close, 5)",
      "factor_implementation_code": "",
      "factor_description": "This factor uses the Z-score of the illiquidity-to-volume ratio to identify statistically significant liquidity shocks. It interacts this standardized illiquidity measure with the 5-day price trend to pinpoint mean-reversion candidates where price movement is disproportionate to the historical liquidity profile.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 10-day moving average of the ratio between daily high-low price range and volume (illiquidity proxy) combined with the 5-day price momentum identifies short-term mean-reversion opportunities caused by liquidity-driven price exhaustion.\n                Concise Observation: In daily price-volume data, extreme price movements often occur with disproportionately low volume, suggesting that the price discovery process is noisy and prone to overshooting in the absence of deep liquidity.\n                Concise Justification: The Amihud illiquidity ratio (High-Low range / Volume) captures the price impact of trading; high values indicate 'thin' markets where small trades move prices significantly, creating transitory price shocks that are statistically likely to mean-revert.\n                Concise Knowledge: If a stock experiences high price volatility on low relative volume, the price move is likely driven by liquidity gaps rather than fundamental shifts; when such illiquidity coincides with extreme short-term returns, a price reversal is expected as liquidity returns to the market.\n                concise Specification: Calculate the ratio of ($high - $low) to $volume as a daily illiquidity measure; define the factor as the 10-day simple moving average of this ratio multiplied by the 5-day cumulative return to capture the interaction between illiquidity and price trend.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "3978712eb575",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.000931362903187,
        "ICIR": 0.0060678875340603,
        "RankIC": 0.0129199155740874,
        "RankICIR": 0.0827167575975825,
        "annualized_return": 0.0197667402350589,
        "information_ratio": 0.2260536698445352,
        "max_drawdown": -0.267549976662951
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:01:53.388467",
      "updated_at": "2026-01-15T20:01:53.388473"
    },
    "1a6b84d55a449e6a": {
      "factor_id": "1a6b84d55a449e6a",
      "factor_name": "Price_Volume_Volatility_Skew_10D",
      "factor_expression": "RANK(-1 * (TS_STD($return, 10) / (TS_STD(TS_PCTCHANGE($volume, 1), 10) + 1e-8)) * ($volume > TS_MEAN($volume, 20)))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(-1 * (TS_STD($return, 10) / (TS_STD(TS_PCTCHANGE($volume, 1), 10) + 1e-8)) * ($volume > TS_MEAN($volume, 20)))\" # Your output factor expression will be filled in here\n    name = \"Price_Volume_Volatility_Skew_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor calculates the ratio of the 10-day standard deviation of daily returns to the 10-day standard deviation of volume percentage changes. It is then conditioned by a 20-day volume trend filter. A high ratio during high-volume periods suggests fragile price discovery and potential reversal.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 10-day Price-Volume Volatility Skew, defined as the ratio of the 10-day standard deviation of daily returns to the 10-day standard deviation of volume changes, negatively predicts short-term returns when the current volume is significantly higher than its 20-day moving average.\n                Concise Observation: Daily price and volume data exhibit heteroskedasticity where extreme return outliers often precede reversals if the volume growth does not sustain the price momentum.\n                Concise Justification: Market microstructure theory suggests that price efficiency is linked to liquidity; a high ratio of price volatility to volume volatility indicates 'fragile' price discovery susceptible to correction.\n                Concise Knowledge: If price volatility increases without a proportional increase in volume stability, the price movement is likely driven by noise; when high volume confirms price dispersion, mean reversion is more probable.\n                concise Specification: Calculate the ratio of the rolling 10-day standard deviation of $return to the rolling 10-day standard deviation of the daily change in $volume, then interact this with a 20-day volume trend filter.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "0f4c5c04860f",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0031926646277224,
        "ICIR": 0.0261026380196433,
        "RankIC": 0.0179831133597679,
        "RankICIR": 0.1424540679517532,
        "annualized_return": 0.0624632394882344,
        "information_ratio": 1.0196630378416056,
        "max_drawdown": -0.0734838101443044
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:06:51.796836",
      "updated_at": "2026-01-15T20:06:51.796843"
    },
    "b3d50803e78f405a": {
      "factor_id": "b3d50803e78f405a",
      "factor_name": "Fragile_Momentum_Reversal_Factor",
      "factor_expression": "ZSCORE(-1 * (TS_STD($return, 10) / (TS_STD(DELTA($volume, 1) / ($volume + 1e-8), 10) + 1e-8))) * ($volume > SMA($volume, 20, 1))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"ZSCORE(-1 * (TS_STD($return, 10) / (TS_STD(TS_PCTCHANGE($volume, 1), 10) + 1e-8))) * ($volume > SMA($volume, 20, 1))\" # Your output factor expression will be filled in here\n    name = \"Fragile_Momentum_Reversal_Factor\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies 'fragile' price movements by measuring the dispersion of returns relative to the dispersion of volume growth over 10 days. It focuses on periods where current volume exceeds its 20-day average, indicating that extreme price volatility without stable volume growth leads to mean reversion.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 10-day Price-Volume Volatility Skew, defined as the ratio of the 10-day standard deviation of daily returns to the 10-day standard deviation of volume changes, negatively predicts short-term returns when the current volume is significantly higher than its 20-day moving average.\n                Concise Observation: Daily price and volume data exhibit heteroskedasticity where extreme return outliers often precede reversals if the volume growth does not sustain the price momentum.\n                Concise Justification: Market microstructure theory suggests that price efficiency is linked to liquidity; a high ratio of price volatility to volume volatility indicates 'fragile' price discovery susceptible to correction.\n                Concise Knowledge: If price volatility increases without a proportional increase in volume stability, the price movement is likely driven by noise; when high volume confirms price dispersion, mean reversion is more probable.\n                concise Specification: Calculate the ratio of the rolling 10-day standard deviation of $return to the rolling 10-day standard deviation of the daily change in $volume, then interact this with a 20-day volume trend filter.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "0f4c5c04860f",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0031926646277224,
        "ICIR": 0.0261026380196433,
        "RankIC": 0.0179831133597679,
        "RankICIR": 0.1424540679517532,
        "annualized_return": 0.0624632394882344,
        "information_ratio": 1.0196630378416056,
        "max_drawdown": -0.0734838101443044
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:06:51.805978",
      "updated_at": "2026-01-15T20:06:51.805984"
    },
    "be3e18d094396a0f": {
      "factor_id": "be3e18d094396a0f",
      "factor_name": "Volume_Conditioned_Volatility_Ratio",
      "factor_expression": "RANK(-1 * (TS_STD($return, 10) / (TS_STD(TS_PCTCHANGE($volume, 1), 10) + 1e-8))) * (TS_RANK($volume, 20) > 16)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(-1 * (TS_STD($return, 10) / (TS_STD(TS_PCTCHANGE($volume, 1), 10) + 1e-8))) * (TS_RANK($volume, 20) > 16)\" # Your output factor expression will be filled in here\n    name = \"Volume_Conditioned_Volatility_Ratio\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A simplified version of the price-volume volatility skew hypothesis. It calculates the ratio of price return volatility to volume volatility over a 10-day window, multiplied by a binary indicator of whether current volume is in its top 20-day quintile to filter for high-activity reversals.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 10-day Price-Volume Volatility Skew, defined as the ratio of the 10-day standard deviation of daily returns to the 10-day standard deviation of volume changes, negatively predicts short-term returns when the current volume is significantly higher than its 20-day moving average.\n                Concise Observation: Daily price and volume data exhibit heteroskedasticity where extreme return outliers often precede reversals if the volume growth does not sustain the price momentum.\n                Concise Justification: Market microstructure theory suggests that price efficiency is linked to liquidity; a high ratio of price volatility to volume volatility indicates 'fragile' price discovery susceptible to correction.\n                Concise Knowledge: If price volatility increases without a proportional increase in volume stability, the price movement is likely driven by noise; when high volume confirms price dispersion, mean reversion is more probable.\n                concise Specification: Calculate the ratio of the rolling 10-day standard deviation of $return to the rolling 10-day standard deviation of the daily change in $volume, then interact this with a 20-day volume trend filter.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "0f4c5c04860f",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0031926646277224,
        "ICIR": 0.0261026380196433,
        "RankIC": 0.0179831133597679,
        "RankICIR": 0.1424540679517532,
        "annualized_return": 0.0624632394882344,
        "information_ratio": 1.0196630378416056,
        "max_drawdown": -0.0734838101443044
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:06:51.815047",
      "updated_at": "2026-01-15T20:06:51.815053"
    },
    "41ac82e8f1a1db49": {
      "factor_id": "41ac82e8f1a1db49",
      "factor_name": "Volume_Surge_Reversal_5D",
      "factor_expression": "-1 * TS_SUM($return, 5) * ($volume / (TS_MEAN($volume, 20) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"-1 * TS_SUM($return, 5) * ($volume / (TS_MEAN($volume, 20) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Volume_Surge_Reversal_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies potential price reversals by multiplying the negative 5-day return by the ratio of current volume to its 20-day moving average. A high volume surge during a price trend often signals exhaustion, and this factor captures the mean-reversion potential following such events.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A price reversal factor can be constructed by identifying instances where the 5-day volume-weighted price change is inversely related to the relative change in daily trading volume, specifically when the current volume is significantly higher than its 20-day moving average.\n                Concise Observation: Daily price and volume data often exhibit 'blow-off tops' or 'selling climaxes' where extreme volume spikes coincide with price trend exhaustion before a reversal.\n                Concise Justification: High volume at price extremes often represents a transfer of liquidity from 'weak hands' to 'strong hands,' leading to a temporary supply-demand imbalance that the market corrects through price reversion.\n                Concise Knowledge: If trading volume surges while price movement stalls or reverses, it indicates a potential exhaustion of the current trend; when volume-weighted returns deviate from simple returns, it suggests institutional positioning that may precede a mean reversion.\n                concise Specification: The factor is defined as the negative 5-day return weighted by the ratio of current volume to its 20-day moving average, applied to daily adjusted price and volume data.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "e3874e4ffc5a",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0023376670772704,
        "ICIR": 0.0191113198918618,
        "RankIC": 0.0147276664742712,
        "RankICIR": 0.1202679085514754,
        "annualized_return": 0.0272358351883886,
        "information_ratio": 0.4376122551706318,
        "max_drawdown": -0.1031767849686496
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:09:22.293181",
      "updated_at": "2026-01-15T20:09:22.293188"
    },
    "0a892e250513aa1d": {
      "factor_id": "0a892e250513aa1d",
      "factor_name": "VW_Exhaustion_Index_5D",
      "factor_expression": "-1 * TS_PCTCHANGE($close, 5) * TS_ZSCORE($volume, 20)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"-1 * TS_PCTCHANGE($close, 5) * TS_ZSCORE($volume, 20)\" # Your output factor expression will be filled in here\n    name = \"VW_Exhaustion_Index_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures price exhaustion by calculating the negative 5-day price change relative to the volume intensity. It uses the Z-score of volume to identify 'blow-off' conditions where extreme volume accompanies a price move that is likely to reverse.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A price reversal factor can be constructed by identifying instances where the 5-day volume-weighted price change is inversely related to the relative change in daily trading volume, specifically when the current volume is significantly higher than its 20-day moving average.\n                Concise Observation: Daily price and volume data often exhibit 'blow-off tops' or 'selling climaxes' where extreme volume spikes coincide with price trend exhaustion before a reversal.\n                Concise Justification: High volume at price extremes often represents a transfer of liquidity from 'weak hands' to 'strong hands,' leading to a temporary supply-demand imbalance that the market corrects through price reversion.\n                Concise Knowledge: If trading volume surges while price movement stalls or reverses, it indicates a potential exhaustion of the current trend; when volume-weighted returns deviate from simple returns, it suggests institutional positioning that may precede a mean reversion.\n                concise Specification: The factor is defined as the negative 5-day return weighted by the ratio of current volume to its 20-day moving average, applied to daily adjusted price and volume data.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "e3874e4ffc5a",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0023376670772704,
        "ICIR": 0.0191113198918618,
        "RankIC": 0.0147276664742712,
        "RankICIR": 0.1202679085514754,
        "annualized_return": 0.0272358351883886,
        "information_ratio": 0.4376122551706318,
        "max_drawdown": -0.1031767849686496
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:09:22.302428",
      "updated_at": "2026-01-15T20:09:22.302434"
    },
    "908909bc18b9091a": {
      "factor_id": "908909bc18b9091a",
      "factor_name": "Relative_Volume_Climax_Rank",
      "factor_expression": "RANK(-1 * TS_MEAN($return, 5) * TS_RANK($volume, 20))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(-1 * TS_MEAN($return, 5) * TS_RANK($volume, 20))\" # Your output factor expression will be filled in here\n    name = \"Relative_Volume_Climax_Rank\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Captures selling climaxes or buying tops by identifying days where the volume is at a time-series high relative to the last 20 days, and the price change over the last 5 days is negative (for a rebound) or positive (for a pullback). The result is cross-sectionally ranked for stability.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A price reversal factor can be constructed by identifying instances where the 5-day volume-weighted price change is inversely related to the relative change in daily trading volume, specifically when the current volume is significantly higher than its 20-day moving average.\n                Concise Observation: Daily price and volume data often exhibit 'blow-off tops' or 'selling climaxes' where extreme volume spikes coincide with price trend exhaustion before a reversal.\n                Concise Justification: High volume at price extremes often represents a transfer of liquidity from 'weak hands' to 'strong hands,' leading to a temporary supply-demand imbalance that the market corrects through price reversion.\n                Concise Knowledge: If trading volume surges while price movement stalls or reverses, it indicates a potential exhaustion of the current trend; when volume-weighted returns deviate from simple returns, it suggests institutional positioning that may precede a mean reversion.\n                concise Specification: The factor is defined as the negative 5-day return weighted by the ratio of current volume to its 20-day moving average, applied to daily adjusted price and volume data.\n                ",
      "initial_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "user_initial_direction": "均值回归test3",
      "planning_direction": "基于订单流失衡（Order Flow Imbalance）与微观结构噪声的超高频均值回归：利用逐笔成交数据计算买卖压力失衡比率，当该指标偏离历史分布的2个标准差且伴随买卖价差收窄时，捕捉极短周期内的价格修正机会。",
      "evolution_phase": "original",
      "trajectory_id": "e3874e4ffc5a",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0023376670772704,
        "ICIR": 0.0191113198918618,
        "RankIC": 0.0147276664742712,
        "RankICIR": 0.1202679085514754,
        "annualized_return": 0.0272358351883886,
        "information_ratio": 0.4376122551706318,
        "max_drawdown": -0.1031767849686496
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:09:22.311681",
      "updated_at": "2026-01-15T20:09:22.311687"
    },
    "a51e62a368a3f566": {
      "factor_id": "a51e62a368a3f566",
      "factor_name": "Residual_Momentum_Sharpe_20D",
      "factor_expression": "TS_MEAN($return - MEAN($return), 20) / (TS_STD($return - MEAN($return), 20) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_MEAN($return - MEAN($return), 20) / (TS_STD($return - MEAN($return), 20) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Residual_Momentum_Sharpe_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor calculates the 20-day residual momentum by adjusting daily returns for the cross-sectional market mean (approximating idiosyncratic return) and scaling the 20-day average of these residuals by their 20-day standard deviation. This risk-adjusted approach aims to identify stable firm-specific strength while reducing noise from high-volatility assets.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 20-day residual momentum, calculated as the mean of daily returns adjusted for the market return and scaled by the inverse of the 20-day idiosyncratic return standard deviation, provides a more stable and predictive alpha signal than raw price momentum.\n                Concise Observation: Raw momentum factors often suffer from significant drawdowns during market reversals because they are heavily loaded on market beta; idiosyncratic returns in the daily_pv.h5 dataset can be approximated by regressing individual stock returns against the cross-sectional average return.\n                Concise Justification: Scaling residual momentum by idiosyncratic volatility (Sharpe-like ratio for residuals) adjusts for the risk taken to achieve those returns, theoretically rewarding stocks with consistent firm-specific strength over those with erratic price jumps.\n                Concise Knowledge: If a stock's return is decomposed into systematic and idiosyncratic components, then the idiosyncratic component (residual) should represent firm-specific information; when this residual is volatility-scaled, it reduces the impact of noise in high-variance assets.\n                concise Specification: Define residual return as $return - mean($return) across all instruments daily; calculate the 20-day mean of these residuals and divide by their 20-day standard deviation to produce the final factor value.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "evolution_phase": "original",
      "trajectory_id": "2212686e5f2f",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0030950732069843,
        "ICIR": 0.0245589238036239,
        "RankIC": 0.0187728585636547,
        "RankICIR": 0.1514513851643384,
        "annualized_return": 0.0484407796087453,
        "information_ratio": 0.8877095650284001,
        "max_drawdown": -0.0576509608317314
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:41:54.342733",
      "updated_at": "2026-01-15T20:41:54.342741"
    },
    "83f024ffd5e17ac7": {
      "factor_id": "83f024ffd5e17ac7",
      "factor_name": "Ranked_Idiosyncratic_Momentum_20D",
      "factor_expression": "TS_MEAN(RANK($return - MEAN($return)), 20) / (TS_STD(RANK($return - MEAN($return)), 20) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_MEAN(RANK($return - MEAN($return)), 20) / (TS_STD(RANK($return - MEAN($return)), 20) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Ranked_Idiosyncratic_Momentum_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A variation of residual momentum that applies cross-sectional ranking to the daily idiosyncratic returns (return minus market mean) before calculating the 20-day risk-adjusted signal. This enhances robustness against outliers and focuses on relative idiosyncratic performance.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 20-day residual momentum, calculated as the mean of daily returns adjusted for the market return and scaled by the inverse of the 20-day idiosyncratic return standard deviation, provides a more stable and predictive alpha signal than raw price momentum.\n                Concise Observation: Raw momentum factors often suffer from significant drawdowns during market reversals because they are heavily loaded on market beta; idiosyncratic returns in the daily_pv.h5 dataset can be approximated by regressing individual stock returns against the cross-sectional average return.\n                Concise Justification: Scaling residual momentum by idiosyncratic volatility (Sharpe-like ratio for residuals) adjusts for the risk taken to achieve those returns, theoretically rewarding stocks with consistent firm-specific strength over those with erratic price jumps.\n                Concise Knowledge: If a stock's return is decomposed into systematic and idiosyncratic components, then the idiosyncratic component (residual) should represent firm-specific information; when this residual is volatility-scaled, it reduces the impact of noise in high-variance assets.\n                concise Specification: Define residual return as $return - mean($return) across all instruments daily; calculate the 20-day mean of these residuals and divide by their 20-day standard deviation to produce the final factor value.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "evolution_phase": "original",
      "trajectory_id": "2212686e5f2f",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0030950732069843,
        "ICIR": 0.0245589238036239,
        "RankIC": 0.0187728585636547,
        "RankICIR": 0.1514513851643384,
        "annualized_return": 0.0484407796087453,
        "information_ratio": 0.8877095650284001,
        "max_drawdown": -0.0576509608317314
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:41:54.352194",
      "updated_at": "2026-01-15T20:41:54.352200"
    },
    "4bfa24e0d6461e3a": {
      "factor_id": "4bfa24e0d6461e3a",
      "factor_name": "Residual_Volatility_Adjusted_Return_10D",
      "factor_expression": "TS_MEAN($return - MEAN($return), 10) / (TS_STD($return - MEAN($return), 10) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_MEAN($return - MEAN($return), 10) / (TS_STD($return - MEAN($return), 10) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Residual_Volatility_Adjusted_Return_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor focuses on a shorter 10-day window to capture more responsive residual momentum. It calculates the average idiosyncratic return (return minus cross-sectional mean) scaled by the inverse of its volatility, providing a high-turnover alpha signal focused on recent firm-specific trends.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 20-day residual momentum, calculated as the mean of daily returns adjusted for the market return and scaled by the inverse of the 20-day idiosyncratic return standard deviation, provides a more stable and predictive alpha signal than raw price momentum.\n                Concise Observation: Raw momentum factors often suffer from significant drawdowns during market reversals because they are heavily loaded on market beta; idiosyncratic returns in the daily_pv.h5 dataset can be approximated by regressing individual stock returns against the cross-sectional average return.\n                Concise Justification: Scaling residual momentum by idiosyncratic volatility (Sharpe-like ratio for residuals) adjusts for the risk taken to achieve those returns, theoretically rewarding stocks with consistent firm-specific strength over those with erratic price jumps.\n                Concise Knowledge: If a stock's return is decomposed into systematic and idiosyncratic components, then the idiosyncratic component (residual) should represent firm-specific information; when this residual is volatility-scaled, it reduces the impact of noise in high-variance assets.\n                concise Specification: Define residual return as $return - mean($return) across all instruments daily; calculate the 20-day mean of these residuals and divide by their 20-day standard deviation to produce the final factor value.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "evolution_phase": "original",
      "trajectory_id": "2212686e5f2f",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0030950732069843,
        "ICIR": 0.0245589238036239,
        "RankIC": 0.0187728585636547,
        "RankICIR": 0.1514513851643384,
        "annualized_return": 0.0484407796087453,
        "information_ratio": 0.8877095650284001,
        "max_drawdown": -0.0576509608317314
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:41:54.362461",
      "updated_at": "2026-01-15T20:41:54.362467"
    },
    "c6e910738f91d2e5": {
      "factor_id": "c6e910738f91d2e5",
      "factor_name": "Institutional_Opening_Momentum_10D",
      "factor_expression": "RANK(TS_MEAN(($close - $open) / ($high - $low + 1e-8), 10))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(TS_MEAN(($close - $open) / ($high - $low + 1e-8), 10))\" # Your output factor expression will be filled in here\n    name = \"Institutional_Opening_Momentum_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor approximates institutional liquidity demand by measuring the relative strength of the opening price move compared to the daily range, smoothed over a 10-day period. Since 30-minute data is not available, it uses the gap and daily range as a proxy for early-day conviction.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The ratio of the first 30 minutes' price change to the daily high-low range, weighted by the relative opening volume, positively predicts the 5-day forward return as it captures institutional liquidity demand.\n                Concise Observation: Opening trades often reflect overnight information incorporation and institutional 'parent' order initiation, which typically exhibit higher autocorrelation than retail-dominated closing auctions.\n                Concise Justification: Institutional investors use the market open to execute large blocks based on fundamental revaluations, creating a lead-lag effect where early price discovery precedes a multi-day trend.\n                Concise Knowledge: If early-morning price action is accompanied by high relative volume, it signifies institutional positioning; when this momentum aligns with the daily trend, it indicates persistent information flow rather than noise.\n                concise Specification: The factor is defined as (Close_30min - Open) / (High - Low) * (Volume_30min / Volume_Daily) over a 10-day lookback period to predict the next 5-day cumulative returns.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "evolution_phase": "original",
      "trajectory_id": "ab7fb9d9bd30",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0019485305161166,
        "ICIR": 0.0143585601560813,
        "RankIC": 0.0151416911570562,
        "RankICIR": 0.1115455502925474,
        "annualized_return": 0.0163544380009403,
        "information_ratio": 0.2347999942426928,
        "max_drawdown": -0.101455618395536
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:44:18.252271",
      "updated_at": "2026-01-15T20:44:18.252279"
    },
    "e7c872d831423c2d": {
      "factor_id": "e7c872d831423c2d",
      "factor_name": "Relative_Opening_Volume_Conviction",
      "factor_expression": "RANK($return * ($volume / (TS_MEAN($volume, 10) + 1e-8)))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK($return * ($volume / (TS_MEAN($volume, 10) + 1e-8)))\" # Your output factor expression will be filled in here\n    name = \"Relative_Opening_Volume_Conviction\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Captures the intensity of early-day price discovery by interacting the daily return with the relative volume intensity, focusing on days where the opening price action (proxied by the daily return) is backed by high volume relative to the 10-day average.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The ratio of the first 30 minutes' price change to the daily high-low range, weighted by the relative opening volume, positively predicts the 5-day forward return as it captures institutional liquidity demand.\n                Concise Observation: Opening trades often reflect overnight information incorporation and institutional 'parent' order initiation, which typically exhibit higher autocorrelation than retail-dominated closing auctions.\n                Concise Justification: Institutional investors use the market open to execute large blocks based on fundamental revaluations, creating a lead-lag effect where early price discovery precedes a multi-day trend.\n                Concise Knowledge: If early-morning price action is accompanied by high relative volume, it signifies institutional positioning; when this momentum aligns with the daily trend, it indicates persistent information flow rather than noise.\n                concise Specification: The factor is defined as (Close_30min - Open) / (High - Low) * (Volume_30min / Volume_Daily) over a 10-day lookback period to predict the next 5-day cumulative returns.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "evolution_phase": "original",
      "trajectory_id": "ab7fb9d9bd30",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0019485305161166,
        "ICIR": 0.0143585601560813,
        "RankIC": 0.0151416911570562,
        "RankICIR": 0.1115455502925474,
        "annualized_return": 0.0163544380009403,
        "information_ratio": 0.2347999942426928,
        "max_drawdown": -0.101455618395536
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:44:18.261942",
      "updated_at": "2026-01-15T20:44:18.261948"
    },
    "6fdaf319b2b9ad9c": {
      "factor_id": "6fdaf319b2b9ad9c",
      "factor_name": "Opening_Range_Efficiency_Factor",
      "factor_expression": "TS_ZSCORE(($close - $open) / ($high - $low + 1e-8), 10) * RANK($volume)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_ZSCORE(($close - $open) / ($high - $low + 1e-8), 10) * RANK($volume)\" # Your output factor expression will be filled in here\n    name = \"Opening_Range_Efficiency_Factor\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Measures the efficiency of the price move relative to the total volatility of the day, weighted by volume. High values suggest that the price moved decisively in one direction with volume support, indicating institutional positioning.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The ratio of the first 30 minutes' price change to the daily high-low range, weighted by the relative opening volume, positively predicts the 5-day forward return as it captures institutional liquidity demand.\n                Concise Observation: Opening trades often reflect overnight information incorporation and institutional 'parent' order initiation, which typically exhibit higher autocorrelation than retail-dominated closing auctions.\n                Concise Justification: Institutional investors use the market open to execute large blocks based on fundamental revaluations, creating a lead-lag effect where early price discovery precedes a multi-day trend.\n                Concise Knowledge: If early-morning price action is accompanied by high relative volume, it signifies institutional positioning; when this momentum aligns with the daily trend, it indicates persistent information flow rather than noise.\n                concise Specification: The factor is defined as (Close_30min - Open) / (High - Low) * (Volume_30min / Volume_Daily) over a 10-day lookback period to predict the next 5-day cumulative returns.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "evolution_phase": "original",
      "trajectory_id": "ab7fb9d9bd30",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0019485305161166,
        "ICIR": 0.0143585601560813,
        "RankIC": 0.0151416911570562,
        "RankICIR": 0.1115455502925474,
        "annualized_return": 0.0163544380009403,
        "information_ratio": 0.2347999942426928,
        "max_drawdown": -0.101455618395536
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:44:18.271428",
      "updated_at": "2026-01-15T20:44:18.271434"
    },
    "72d27de856876f14": {
      "factor_id": "72d27de856876f14",
      "factor_name": "Price_VWAP_Volume_Slope_Divergence_5D",
      "factor_expression": "($close / (($open + $high + $low + $close) / 4)) * (-1 * REGBETA($volume, SEQUENCE(5), 5))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"($close / (($open + $high + $low + $close) / 4)) * (-1 * REGBETA($volume, SEQUENCE(5), 5))\" # Your output factor expression will be filled in here\n    name = \"Price_VWAP_Volume_Slope_Divergence_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies short-term price exhaustion by multiplying the ratio of the closing price to the intraday average price (proxy for VWAP) by the negative 5-day linear regression slope of volume. A high value suggests price is overextended on thinning institutional support, predicting a mean reversion.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 5-day Price-VWAP Divergence factor, defined as the ratio of the closing price to the daily volume-weighted average price (VWAP) scaled by the negative 5-day slope of relative volume, predicts short-term mean reversion by identifying price exhaustion lacking institutional liquidity support.\n                Concise Observation: The parent strategy focuses on medium-term idiosyncratic momentum, but fails to capture short-term liquidity traps where price levels become unsustainable due to thinning volume, a pattern frequently observed in the daily_pv.h5 dataset when $close/$vwap reaches extremes.\n                Concise Justification: VWAP represents the average price paid by all market participants; closing significantly above it on low volume suggests a 'liquidity gap' where marginal buyers are pushing prices higher without broad participation, making the asset vulnerable to a reversal.\n                Concise Knowledge: If a stock's closing price significantly exceeds its intraday VWAP while volume is declining, the price move is likely driven by retail exhaustion rather than institutional accumulation; such divergence typically leads to a mean-reversion event within 1-3 days.\n                concise Specification: The factor is calculated by ($close / (($open + $high + $low + $close) / 4)) multiplied by the negative of the 5-day linear regression slope of $volume; it targets a 1-3 day forecast horizon and is expected to have low correlation with idiosyncratic momentum.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "evolution_phase": "mutation",
      "trajectory_id": "13d56d76c8be",
      "parent_trajectory_ids": [
        "43ec6e4b286b"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0039933168626298,
        "ICIR": 0.0297615959863527,
        "RankIC": 0.0197982589556458,
        "RankICIR": 0.1464343870655861,
        "annualized_return": 0.0610035234698491,
        "information_ratio": 0.8198077562410747,
        "max_drawdown": -0.1198381820264597
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:47:39.224009",
      "updated_at": "2026-01-15T20:47:39.224016"
    },
    "8f1500671bceb51a": {
      "factor_id": "8f1500671bceb51a",
      "factor_name": "ZScore_VWAP_Liquidity_Trap_5D",
      "factor_expression": "RANK($close / (($open + $high + $low + $close) / 4)) * RANK(-1 * DELTA($volume, 5) / ($volume + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK($close / (($open + $high + $low + $close) / 4)) * RANK(-1 * DELTA($volume, 5) / DELAY($volume, 5))\" # Your output factor expression will be filled in here\n    name = \"ZScore_VWAP_Liquidity_Trap_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures liquidity traps by measuring the cross-sectional rank of price-to-VWAP divergence scaled by the negative change in volume. It focuses on identifying assets where price increases are decoupled from volume growth, signaling potential reversals.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 5-day Price-VWAP Divergence factor, defined as the ratio of the closing price to the daily volume-weighted average price (VWAP) scaled by the negative 5-day slope of relative volume, predicts short-term mean reversion by identifying price exhaustion lacking institutional liquidity support.\n                Concise Observation: The parent strategy focuses on medium-term idiosyncratic momentum, but fails to capture short-term liquidity traps where price levels become unsustainable due to thinning volume, a pattern frequently observed in the daily_pv.h5 dataset when $close/$vwap reaches extremes.\n                Concise Justification: VWAP represents the average price paid by all market participants; closing significantly above it on low volume suggests a 'liquidity gap' where marginal buyers are pushing prices higher without broad participation, making the asset vulnerable to a reversal.\n                Concise Knowledge: If a stock's closing price significantly exceeds its intraday VWAP while volume is declining, the price move is likely driven by retail exhaustion rather than institutional accumulation; such divergence typically leads to a mean-reversion event within 1-3 days.\n                concise Specification: The factor is calculated by ($close / (($open + $high + $low + $close) / 4)) multiplied by the negative of the 5-day linear regression slope of $volume; it targets a 1-3 day forecast horizon and is expected to have low correlation with idiosyncratic momentum.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "evolution_phase": "mutation",
      "trajectory_id": "13d56d76c8be",
      "parent_trajectory_ids": [
        "43ec6e4b286b"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0039933168626298,
        "ICIR": 0.0297615959863527,
        "RankIC": 0.0197982589556458,
        "RankICIR": 0.1464343870655861,
        "annualized_return": 0.0610035234698491,
        "information_ratio": 0.8198077562410747,
        "max_drawdown": -0.1198381820264597
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:47:39.234763",
      "updated_at": "2026-01-15T20:47:39.234770"
    },
    "e0d42d77a913cd33": {
      "factor_id": "e0d42d77a913cd33",
      "factor_name": "Exhaustion_Divergence_Index_3D",
      "factor_expression": "($close / (($high + $low + $close) / 3)) * ((TS_MEAN($volume, 3) - $volume) / (TS_MEAN($volume, 3) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"($close / (($high + $low + $close) / 3)) * ((TS_MEAN($volume, 3) - $volume) / (TS_MEAN($volume, 3) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Exhaustion_Divergence_Index_3D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A simplified version of the divergence hypothesis focusing on the 3-day window. It calculates the ratio of price to the typical price and multiplies it by the negative 3-day volume trend to highlight immediate exhaustion points.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 5-day Price-VWAP Divergence factor, defined as the ratio of the closing price to the daily volume-weighted average price (VWAP) scaled by the negative 5-day slope of relative volume, predicts short-term mean reversion by identifying price exhaustion lacking institutional liquidity support.\n                Concise Observation: The parent strategy focuses on medium-term idiosyncratic momentum, but fails to capture short-term liquidity traps where price levels become unsustainable due to thinning volume, a pattern frequently observed in the daily_pv.h5 dataset when $close/$vwap reaches extremes.\n                Concise Justification: VWAP represents the average price paid by all market participants; closing significantly above it on low volume suggests a 'liquidity gap' where marginal buyers are pushing prices higher without broad participation, making the asset vulnerable to a reversal.\n                Concise Knowledge: If a stock's closing price significantly exceeds its intraday VWAP while volume is declining, the price move is likely driven by retail exhaustion rather than institutional accumulation; such divergence typically leads to a mean-reversion event within 1-3 days.\n                concise Specification: The factor is calculated by ($close / (($open + $high + $low + $close) / 4)) multiplied by the negative of the 5-day linear regression slope of $volume; it targets a 1-3 day forecast horizon and is expected to have low correlation with idiosyncratic momentum.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract momentum from the idiosyncratic returns of a Fama-French 3-factor model to remove market/sector beta, then scale the signal by the inverse of idiosyncratic volatility to penalize high-dispersion stocks.",
      "evolution_phase": "mutation",
      "trajectory_id": "13d56d76c8be",
      "parent_trajectory_ids": [
        "43ec6e4b286b"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0039933168626298,
        "ICIR": 0.0297615959863527,
        "RankIC": 0.0197982589556458,
        "RankICIR": 0.1464343870655861,
        "annualized_return": 0.0610035234698491,
        "information_ratio": 0.8198077562410747,
        "max_drawdown": -0.1198381820264597
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:47:39.244450",
      "updated_at": "2026-01-15T20:47:39.244456"
    },
    "2c8e78ac97c70fa9": {
      "factor_id": "2c8e78ac97c70fa9",
      "factor_name": "Overnight_Gap_Volatility_Contraction_20D",
      "factor_expression": "-1 * (($open / DELAY($close, 1) - 1) / (TS_MEAN($high - $low, 20) + 1e-8)) * (TS_MEAN($high - $low, 20) / ($high - $low + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"-1 * (($open / DELAY($close, 1) - 1) / (TS_MEAN($high - $low, 20) + 1e-8)) * (TS_MEAN($high - $low, 20) / ($high - $low + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Overnight_Gap_Volatility_Contraction_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures the reversal of overnight sentiment gaps. It normalizes the overnight return by the 20-day average intraday range and scales it by the ratio of the historical range to the current day's range. A high value indicates a large gap relative to historical volatility followed by a narrow intraday range, suggesting sentiment exhaustion and potential reversal.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The overnight return gap, when normalized by the 20-day average true range, negatively predicts the next 3-day return specifically when accompanied by a contraction in intraday volatility, indicating the reversal of retail-driven sentiment exhaustion.\n                Concise Observation: Large overnight price jumps often overextend due to retail order imbalances at the open, but if the subsequent high-low range is narrow, it indicates a lack of follow-through conviction.\n                Concise Justification: Retail investors often overreact to overnight news, creating 'sentiment gaps'; market makers provide liquidity to counter this noise, earning a premium as the price reverts to its mean during periods of low intraday risk.\n                Concise Knowledge: If a price gap is driven by sentiment rather than fundamental liquidity, it tends to reverse; when intraday volatility is lower than the gap's magnitude, it suggests market makers are successfully absorbing the imbalance.\n                concise Specification: The factor is defined as the negative of the overnight return ($open_t / $close_{t-1} - 1) divided by the 20-day average high-low range, then multiplied by a decay factor representing the ratio of the 20-day average range to the current day's range.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "evolution_phase": "mutation",
      "trajectory_id": "ad7f40306ead",
      "parent_trajectory_ids": [
        "79a18071711d"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.004522865834695,
        "ICIR": 0.034205190215721,
        "RankIC": 0.0219134406831616,
        "RankICIR": 0.1718624754185475,
        "annualized_return": 0.0618422632300543,
        "information_ratio": 0.959985072236157,
        "max_drawdown": -0.0732667441333499
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:50:19.224387",
      "updated_at": "2026-01-15T20:50:19.224395"
    },
    "a24d400aaffd4856": {
      "factor_id": "a24d400aaffd4856",
      "factor_name": "Sentiment_Exhaustion_Reversal_Factor",
      "factor_expression": "-1 * RANK(($open - DELAY($close, 1)) / ($high - $low + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"-1 * RANK(($open - DELAY($close, 1)) / ($high - $low + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Sentiment_Exhaustion_Reversal_Factor\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A simplified version of the sentiment exhaustion hypothesis. It measures the overnight gap relative to the current day's range, cross-sectionally ranked to identify stocks where the gap is disproportionately large compared to the intraday follow-through, signaling a mean-reversion opportunity.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The overnight return gap, when normalized by the 20-day average true range, negatively predicts the next 3-day return specifically when accompanied by a contraction in intraday volatility, indicating the reversal of retail-driven sentiment exhaustion.\n                Concise Observation: Large overnight price jumps often overextend due to retail order imbalances at the open, but if the subsequent high-low range is narrow, it indicates a lack of follow-through conviction.\n                Concise Justification: Retail investors often overreact to overnight news, creating 'sentiment gaps'; market makers provide liquidity to counter this noise, earning a premium as the price reverts to its mean during periods of low intraday risk.\n                Concise Knowledge: If a price gap is driven by sentiment rather than fundamental liquidity, it tends to reverse; when intraday volatility is lower than the gap's magnitude, it suggests market makers are successfully absorbing the imbalance.\n                concise Specification: The factor is defined as the negative of the overnight return ($open_t / $close_{t-1} - 1) divided by the 20-day average high-low range, then multiplied by a decay factor representing the ratio of the 20-day average range to the current day's range.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "evolution_phase": "mutation",
      "trajectory_id": "ad7f40306ead",
      "parent_trajectory_ids": [
        "79a18071711d"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.004522865834695,
        "ICIR": 0.034205190215721,
        "RankIC": 0.0219134406831616,
        "RankICIR": 0.1718624754185475,
        "annualized_return": 0.0618422632300543,
        "information_ratio": 0.959985072236157,
        "max_drawdown": -0.0732667441333499
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:50:19.236132",
      "updated_at": "2026-01-15T20:50:19.236141"
    },
    "31e12a6f734358fa": {
      "factor_id": "31e12a6f734358fa",
      "factor_name": "ZScore_Normalized_Gap_Reversal_20D",
      "factor_expression": "-1 * (($open - DELAY($close, 1)) / (TS_STD($close, 20) + 1e-8)) * (TS_MEAN($high - $low, 20) / ($high - $low + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"-1 * (($open - DELAY($close, 1)) / (TS_STD($close, 20) + 1e-8)) * (TS_MEAN($high - $low, 20) / ($high - $low + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"ZScore_Normalized_Gap_Reversal_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies overnight gaps that are statistically significant relative to historical intraday volatility. It uses a Z-score of the gap magnitude and weights it by the inverse of the current day's volatility to detect low-conviction price jumps likely to reverse.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The overnight return gap, when normalized by the 20-day average true range, negatively predicts the next 3-day return specifically when accompanied by a contraction in intraday volatility, indicating the reversal of retail-driven sentiment exhaustion.\n                Concise Observation: Large overnight price jumps often overextend due to retail order imbalances at the open, but if the subsequent high-low range is narrow, it indicates a lack of follow-through conviction.\n                Concise Justification: Retail investors often overreact to overnight news, creating 'sentiment gaps'; market makers provide liquidity to counter this noise, earning a premium as the price reverts to its mean during periods of low intraday risk.\n                Concise Knowledge: If a price gap is driven by sentiment rather than fundamental liquidity, it tends to reverse; when intraday volatility is lower than the gap's magnitude, it suggests market makers are successfully absorbing the imbalance.\n                concise Specification: The factor is defined as the negative of the overnight return ($open_t / $close_{t-1} - 1) divided by the 20-day average high-low range, then multiplied by a decay factor representing the ratio of the 20-day average range to the current day's range.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of the first 30 minutes of trading volume-weighted returns on the subsequent 5-day return, testing the hypothesis that institutional 'informed' flows at the open exhibit stronger persistence than retail-driven closing trends.",
      "evolution_phase": "mutation",
      "trajectory_id": "ad7f40306ead",
      "parent_trajectory_ids": [
        "79a18071711d"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.004522865834695,
        "ICIR": 0.034205190215721,
        "RankIC": 0.0219134406831616,
        "RankICIR": 0.1718624754185475,
        "annualized_return": 0.0618422632300543,
        "information_ratio": 0.959985072236157,
        "max_drawdown": -0.0732667441333499
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-15T20:50:19.247566",
      "updated_at": "2026-01-15T20:50:19.247572"
    },
    "1b80e1140573aee7": {
      "factor_id": "1b80e1140573aee7",
      "factor_name": "Idiosyncratic_Momentum_Sharpe_20D",
      "factor_expression": "TS_MEAN(REGRESI($return, MEAN($return), 20), 20) / (TS_STD(REGRESI($return, MEAN($return), 20), 20) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_MEAN(REGRESI($return, MEAN($return), 20), 20) / (TS_STD(REGRESI($return, MEAN($return), 20), 20) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Idiosyncratic_Momentum_Sharpe_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor calculates the idiosyncratic return of a stock by regressing its daily returns against the market return (proxied by the cross-sectional mean return) over a 20-day window. The final factor is the mean of these residuals divided by their standard deviation, effectively representing an idiosyncratic risk-adjusted momentum signal.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A stock's 20-day idiosyncratic return, calculated as the residual from a linear regression of stock returns on market returns, when scaled by its 20-day rolling standard deviation, positively predicts its future excess returns by isolating high-conviction momentum.\n                Concise Observation: Raw momentum often suffers from large drawdowns during market reversals because it captures systematic risk (beta) rather than stock-specific strength, leading to high correlation among momentum stocks.\n                Concise Justification: By regressing out the market return, we remove the common factor component, and by scaling by idiosyncratic volatility, we identify stocks with stable, 'quality' trends that are more likely to persist than volatile, noise-driven price movements.\n                Concise Knowledge: If a stock's return persistence is driven by idiosyncratic strength rather than market beta, then scaling these residuals by their volatility improves the signal-to-noise ratio; when volatility is high, the momentum signal should be penalized to account for uncertainty.\n                concise Specification: The factor is defined as the mean of the residuals from a 20-day rolling OLS regression of $return on $market_return (proxied by cross-sectional mean), divided by the standard deviation of those residuals over the same window.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "original",
      "trajectory_id": "7564a2a6c386",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.004360174799154,
        "ICIR": 0.0317103876439821,
        "RankIC": 0.020902656431109,
        "RankICIR": 0.1569980574424791,
        "annualized_return": 0.0487770067639325,
        "information_ratio": 0.6945033542078266,
        "max_drawdown": -0.1078695092218786
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:34:53.445997",
      "updated_at": "2026-01-16T00:34:53.446005"
    },
    "ddc80fe30f9b2a60": {
      "factor_id": "ddc80fe30f9b2a60",
      "factor_name": "Zscored_Idiosyncratic_Strength_20D",
      "factor_expression": "TS_ZSCORE(REGRESI($return, MEAN($return), 20), 20)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_ZSCORE(REGRESI($return, MEAN($return), 20), 20)\" # Your output factor expression will be filled in here\n    name = \"Zscored_Idiosyncratic_Strength_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A variation of idiosyncratic momentum that focuses on the cross-sectional strength of the market-neutral residual. It isolates stock-specific price action by removing the market component and then applies a time-series Z-score to the residuals to identify significant deviations from the stock's own recent idiosyncratic mean.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A stock's 20-day idiosyncratic return, calculated as the residual from a linear regression of stock returns on market returns, when scaled by its 20-day rolling standard deviation, positively predicts its future excess returns by isolating high-conviction momentum.\n                Concise Observation: Raw momentum often suffers from large drawdowns during market reversals because it captures systematic risk (beta) rather than stock-specific strength, leading to high correlation among momentum stocks.\n                Concise Justification: By regressing out the market return, we remove the common factor component, and by scaling by idiosyncratic volatility, we identify stocks with stable, 'quality' trends that are more likely to persist than volatile, noise-driven price movements.\n                Concise Knowledge: If a stock's return persistence is driven by idiosyncratic strength rather than market beta, then scaling these residuals by their volatility improves the signal-to-noise ratio; when volatility is high, the momentum signal should be penalized to account for uncertainty.\n                concise Specification: The factor is defined as the mean of the residuals from a 20-day rolling OLS regression of $return on $market_return (proxied by cross-sectional mean), divided by the standard deviation of those residuals over the same window.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "original",
      "trajectory_id": "7564a2a6c386",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.004360174799154,
        "ICIR": 0.0317103876439821,
        "RankIC": 0.020902656431109,
        "RankICIR": 0.1569980574424791,
        "annualized_return": 0.0487770067639325,
        "information_ratio": 0.6945033542078266,
        "max_drawdown": -0.1078695092218786
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:34:53.456399",
      "updated_at": "2026-01-16T00:34:53.456405"
    },
    "d2cce289d6e2589d": {
      "factor_id": "d2cce289d6e2589d",
      "factor_name": "Idiosyncratic_Return_Persistence_20D",
      "factor_expression": "TS_SUM(REGRESI($return, MEAN($return), 20), 20) / (TS_SUM(ABS(REGRESI($return, MEAN($return), 20)), 20) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_SUM(REGRESI($return, MEAN($return), 20), 20) / (TS_SUM(ABS(REGRESI($return, MEAN($return), 20)), 20) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Idiosyncratic_Return_Persistence_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures the consistency of idiosyncratic returns. It calculates the ratio of the sum of idiosyncratic residuals to the sum of their absolute values over 20 days. This captures the 'purity' of the upward or downward idiosyncratic trend, penalizing high-volatility noise.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A stock's 20-day idiosyncratic return, calculated as the residual from a linear regression of stock returns on market returns, when scaled by its 20-day rolling standard deviation, positively predicts its future excess returns by isolating high-conviction momentum.\n                Concise Observation: Raw momentum often suffers from large drawdowns during market reversals because it captures systematic risk (beta) rather than stock-specific strength, leading to high correlation among momentum stocks.\n                Concise Justification: By regressing out the market return, we remove the common factor component, and by scaling by idiosyncratic volatility, we identify stocks with stable, 'quality' trends that are more likely to persist than volatile, noise-driven price movements.\n                Concise Knowledge: If a stock's return persistence is driven by idiosyncratic strength rather than market beta, then scaling these residuals by their volatility improves the signal-to-noise ratio; when volatility is high, the momentum signal should be penalized to account for uncertainty.\n                concise Specification: The factor is defined as the mean of the residuals from a 20-day rolling OLS regression of $return on $market_return (proxied by cross-sectional mean), divided by the standard deviation of those residuals over the same window.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "original",
      "trajectory_id": "7564a2a6c386",
      "parent_trajectory_ids": [],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.004360174799154,
        "ICIR": 0.0317103876439821,
        "RankIC": 0.020902656431109,
        "RankICIR": 0.1569980574424791,
        "annualized_return": 0.0487770067639325,
        "information_ratio": 0.6945033542078266,
        "max_drawdown": -0.1078695092218786
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:34:53.467729",
      "updated_at": "2026-01-16T00:34:53.467735"
    },
    "cd988c9b484543af": {
      "factor_id": "cd988c9b484543af",
      "factor_name": "Institutional_Morning_Momentum_Proxy_5D",
      "factor_expression": "RANK((($open - DELAY($close, 1)) / (DELAY($close, 1) + 1e-8)) * ($volume / (TS_MEAN($volume, 5) + 1e-8)))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK((($open - DELAY($close, 1)) / (DELAY($close, 1) + 1e-8)) * ($volume / (TS_MEAN($volume, 5) + 1e-8)))\" # Your output factor expression will be filled in here\n    name = \"Institutional_Morning_Momentum_Proxy_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor proxies the institutional morning flow by calculating the product of the opening return (overnight gap plus early movement) and the volume intensity relative to its 5-day average. High volume accompanying strong early price action suggests informed institutional positioning.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The ratio of the first-hour return (close of first hour vs. previous day close) multiplied by the first-hour volume share (first hour volume vs. daily volume) positively predicts the overnight return gap of the following day, as institutional morning flows signal persistent sentiment.\n                Concise Observation: Market participants often execute large orders during the first hour of trading to capture liquidity, and these execution patterns in daily price-volume data often lead to price continuations that are not fully realized until the subsequent market opening.\n                Concise Justification: Institutional investors use the 'opening range' to set the day's tone; a strong positive return on high volume during this period reflects informed demand that typically carries over to the next day's opening auction due to information asymmetry and order imbalance.\n                Concise Knowledge: If the first hour of trading exhibits high relative volume and a strong price direction, it indicates institutional positioning that likely persists into the next market open; when morning momentum is backed by high liquidity, it suggests a higher probability of an overnight price gap.\n                concise Specification: The factor is defined as (Close_at_10:30 / Close_prev - 1) * (Volume_10:30 / Volume_Total) for the current day to predict the return from Close_t to Open_t+1; however, given daily data constraints, we proxy this using the daily return weighted by a 5-day moving average of volume intensity.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "original",
      "trajectory_id": "f9dd3808b1d9",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0058737738141399,
        "ICIR": 0.0444116249545078,
        "RankIC": 0.0218215047614608,
        "RankICIR": 0.1723893261941869,
        "annualized_return": 0.0755626023881111,
        "information_ratio": 1.1026491269645753,
        "max_drawdown": -0.1070394477358482
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:38:28.557905",
      "updated_at": "2026-01-16T00:38:28.557912"
    },
    "ebab1c033623377c": {
      "factor_id": "ebab1c033623377c",
      "factor_name": "Opening_Range_Intensity_Factor_10D",
      "factor_expression": "TS_ZSCORE(($open - DELAY($close, 1)) / (DELAY($close, 1) + 1e-8), 10) * RANK($volume / (TS_MEAN($volume, 10) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_ZSCORE(($open - DELAY($close, 1)) / (DELAY($close, 1) + 1e-8), 10) * RANK($volume / (TS_MEAN($volume, 10) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Opening_Range_Intensity_Factor_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures the intensity of the opening range relative to the daily range, weighted by the volume's deviation from its 10-day trend. It identifies days where the initial price discovery (open vs previous close) is significant and supported by high relative liquidity.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The ratio of the first-hour return (close of first hour vs. previous day close) multiplied by the first-hour volume share (first hour volume vs. daily volume) positively predicts the overnight return gap of the following day, as institutional morning flows signal persistent sentiment.\n                Concise Observation: Market participants often execute large orders during the first hour of trading to capture liquidity, and these execution patterns in daily price-volume data often lead to price continuations that are not fully realized until the subsequent market opening.\n                Concise Justification: Institutional investors use the 'opening range' to set the day's tone; a strong positive return on high volume during this period reflects informed demand that typically carries over to the next day's opening auction due to information asymmetry and order imbalance.\n                Concise Knowledge: If the first hour of trading exhibits high relative volume and a strong price direction, it indicates institutional positioning that likely persists into the next market open; when morning momentum is backed by high liquidity, it suggests a higher probability of an overnight price gap.\n                concise Specification: The factor is defined as (Close_at_10:30 / Close_prev - 1) * (Volume_10:30 / Volume_Total) for the current day to predict the return from Close_t to Open_t+1; however, given daily data constraints, we proxy this using the daily return weighted by a 5-day moving average of volume intensity.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "original",
      "trajectory_id": "f9dd3808b1d9",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0058737738141399,
        "ICIR": 0.0444116249545078,
        "RankIC": 0.0218215047614608,
        "RankICIR": 0.1723893261941869,
        "annualized_return": 0.0755626023881111,
        "information_ratio": 1.1026491269645753,
        "max_drawdown": -0.1070394477358482
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:38:28.568295",
      "updated_at": "2026-01-16T00:38:28.568301"
    },
    "aaacc9abeef2446e": {
      "factor_id": "aaacc9abeef2446e",
      "factor_name": "Morning_Sentiment_Persistence_Index",
      "factor_expression": "SIGN($open - DELAY($close, 1)) * RANK($volume / (WMA($volume, 20) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"SIGN($open - DELAY($close, 1)) * RANK($volume / (WMA($volume, 20) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Morning_Sentiment_Persistence_Index\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A refined sentiment index that captures the interaction between the overnight return and the current day's volume share. It uses the ratio of current volume to a 20-day moving average to identify institutional 'conviction' in the price gap.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The ratio of the first-hour return (close of first hour vs. previous day close) multiplied by the first-hour volume share (first hour volume vs. daily volume) positively predicts the overnight return gap of the following day, as institutional morning flows signal persistent sentiment.\n                Concise Observation: Market participants often execute large orders during the first hour of trading to capture liquidity, and these execution patterns in daily price-volume data often lead to price continuations that are not fully realized until the subsequent market opening.\n                Concise Justification: Institutional investors use the 'opening range' to set the day's tone; a strong positive return on high volume during this period reflects informed demand that typically carries over to the next day's opening auction due to information asymmetry and order imbalance.\n                Concise Knowledge: If the first hour of trading exhibits high relative volume and a strong price direction, it indicates institutional positioning that likely persists into the next market open; when morning momentum is backed by high liquidity, it suggests a higher probability of an overnight price gap.\n                concise Specification: The factor is defined as (Close_at_10:30 / Close_prev - 1) * (Volume_10:30 / Volume_Total) for the current day to predict the return from Close_t to Open_t+1; however, given daily data constraints, we proxy this using the daily return weighted by a 5-day moving average of volume intensity.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "original",
      "trajectory_id": "f9dd3808b1d9",
      "parent_trajectory_ids": [],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0058737738141399,
        "ICIR": 0.0444116249545078,
        "RankIC": 0.0218215047614608,
        "RankICIR": 0.1723893261941869,
        "annualized_return": 0.0755626023881111,
        "information_ratio": 1.1026491269645753,
        "max_drawdown": -0.1070394477358482
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:38:28.579544",
      "updated_at": "2026-01-16T00:38:28.579550"
    },
    "634bd0e16e90de90": {
      "factor_id": "634bd0e16e90de90",
      "factor_name": "Volume_Price_Efficiency_Reversion_5D",
      "factor_expression": "RANK(TS_MEAN(ABS($return) / LOG($volume + 1e-8), 5))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(TS_MEAN(ABS($return) / LOG($volume + 1e-8), 5))\" # Your output factor expression will be filled in here\n    name = \"Volume_Price_Efficiency_Reversion_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures the 5-day rolling average of volume-price efficiency, defined as the absolute daily return divided by the log-transformed volume. High efficiency indicates that prices are moving significantly on low volume, suggesting liquidity exhaustion and a high probability of mean-reversion.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A 5-day measure of volume-price efficiency, defined as the ratio of absolute daily returns to the log-transformed trading volume, negatively predicts future returns as extreme efficiency spikes indicate liquidity exhaustion and imminent mean-reversion.\n                Concise Observation: While 20-day idiosyncratic momentum captures fundamental trends, short-term (5-day) price spikes often occur with volume-price divergence, where the 'cost' of moving the price (return per unit of volume) becomes unsustainably high.\n                Concise Justification: The Amihud illiquidity logic suggests that high price impact per volume unit reflects low liquidity; at short horizons, extreme price impact signals that the current trend is driven by liquidity shocks rather than informed trading, leading to a reversal.\n                Concise Knowledge: If a stock experiences large price moves on relatively low volume (high efficiency), it often indicates a lack of liquidity depth; when this efficiency reaches an extreme, the price movement is likely to reverse as noise trader demand is exhausted.\n                concise Specification: The factor calculates the 5-day rolling average of the absolute daily return divided by the natural logarithm of daily volume, targeting a short-term mean-reversion signal that is orthogonal to 20-day price-only momentum residuals.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "eacee476855f",
      "parent_trajectory_ids": [
        "4edcfc71ba33"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.003081972556968,
        "ICIR": 0.0239025441933693,
        "RankIC": 0.0172142504520908,
        "RankICIR": 0.1326310433361047,
        "annualized_return": 0.0337855015812366,
        "information_ratio": 0.5205809109994792,
        "max_drawdown": -0.0947531455291542
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:41:19.839626",
      "updated_at": "2026-01-16T00:41:19.839634"
    },
    "96b40879c3d30734": {
      "factor_id": "96b40879c3d30734",
      "factor_name": "Efficiency_Momentum_Divergence_5D",
      "factor_expression": "ZSCORE(TS_MEAN(ABS($return) / LOG($volume + 1e-8), 5) - TS_MEAN(ABS($return) / LOG($volume + 1e-8), 20))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"ZSCORE(TS_MEAN(ABS($return) / LOG($volume + 1e-8), 5) - TS_MEAN(ABS($return) / LOG($volume + 1e-8), 20))\" # Your output factor expression will be filled in here\n    name = \"Efficiency_Momentum_Divergence_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies short-term price spikes that are unsupported by volume by calculating the 5-day average efficiency and subtracting the longer-term 20-day trend. It targets the specific 'cost' of moving price, where a high relative efficiency signals an unsustainable trend likely to reverse.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A 5-day measure of volume-price efficiency, defined as the ratio of absolute daily returns to the log-transformed trading volume, negatively predicts future returns as extreme efficiency spikes indicate liquidity exhaustion and imminent mean-reversion.\n                Concise Observation: While 20-day idiosyncratic momentum captures fundamental trends, short-term (5-day) price spikes often occur with volume-price divergence, where the 'cost' of moving the price (return per unit of volume) becomes unsustainably high.\n                Concise Justification: The Amihud illiquidity logic suggests that high price impact per volume unit reflects low liquidity; at short horizons, extreme price impact signals that the current trend is driven by liquidity shocks rather than informed trading, leading to a reversal.\n                Concise Knowledge: If a stock experiences large price moves on relatively low volume (high efficiency), it often indicates a lack of liquidity depth; when this efficiency reaches an extreme, the price movement is likely to reverse as noise trader demand is exhausted.\n                concise Specification: The factor calculates the 5-day rolling average of the absolute daily return divided by the natural logarithm of daily volume, targeting a short-term mean-reversion signal that is orthogonal to 20-day price-only momentum residuals.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "eacee476855f",
      "parent_trajectory_ids": [
        "4edcfc71ba33"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.003081972556968,
        "ICIR": 0.0239025441933693,
        "RankIC": 0.0172142504520908,
        "RankICIR": 0.1326310433361047,
        "annualized_return": 0.0337855015812366,
        "information_ratio": 0.5205809109994792,
        "max_drawdown": -0.0947531455291542
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:41:19.851633",
      "updated_at": "2026-01-16T00:41:19.851639"
    },
    "1dbdcac46515d399": {
      "factor_id": "1dbdcac46515d399",
      "factor_name": "Liquidity_Shock_Index_5D",
      "factor_expression": "RANK(TS_RANK(ABS($return) / LOG($volume + 1e-8), 5))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(TS_RANK(ABS($return) / LOG($volume + 1e-8), 5))\" # Your output factor expression will be filled in here\n    name = \"Liquidity_Shock_Index_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures liquidity shocks by measuring the time-series rank of the current volume-price efficiency over a 5-day window. A high time-series rank suggests that the current price impact per unit of volume is at a short-term extreme, signaling imminent mean-reversion due to noise trader exhaustion.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: A 5-day measure of volume-price efficiency, defined as the ratio of absolute daily returns to the log-transformed trading volume, negatively predicts future returns as extreme efficiency spikes indicate liquidity exhaustion and imminent mean-reversion.\n                Concise Observation: While 20-day idiosyncratic momentum captures fundamental trends, short-term (5-day) price spikes often occur with volume-price divergence, where the 'cost' of moving the price (return per unit of volume) becomes unsustainably high.\n                Concise Justification: The Amihud illiquidity logic suggests that high price impact per volume unit reflects low liquidity; at short horizons, extreme price impact signals that the current trend is driven by liquidity shocks rather than informed trading, leading to a reversal.\n                Concise Knowledge: If a stock experiences large price moves on relatively low volume (high efficiency), it often indicates a lack of liquidity depth; when this efficiency reaches an extreme, the price movement is likely to reverse as noise trader demand is exhausted.\n                concise Specification: The factor calculates the 5-day rolling average of the absolute daily return divided by the natural logarithm of daily volume, targeting a short-term mean-reversion signal that is orthogonal to 20-day price-only momentum residuals.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "eacee476855f",
      "parent_trajectory_ids": [
        "4edcfc71ba33"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.003081972556968,
        "ICIR": 0.0239025441933693,
        "RankIC": 0.0172142504520908,
        "RankICIR": 0.1326310433361047,
        "annualized_return": 0.0337855015812366,
        "information_ratio": 0.5205809109994792,
        "max_drawdown": -0.0947531455291542
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:41:19.862174",
      "updated_at": "2026-01-16T00:41:19.862181"
    },
    "fb20f7b629b0b1bf": {
      "factor_id": "fb20f7b629b0b1bf",
      "factor_name": "Late_Day_Exhaustion_Reversal_5D",
      "factor_expression": "TS_MEAN(($high - $low) / ($volume / (TS_MEAN($volume, 20) + 1e-8) + 1e-8) * SIGN($close - $open), 5)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_MEAN(($high - $low) / ($volume / (TS_MEAN($volume, 20) + 1e-8) + 1e-8) * SIGN($close - $open), 5)\" # Your output factor expression will be filled in here\n    name = \"Late_Day_Exhaustion_Reversal_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures price 'stretching' on low relative volume. It calculates the 5-day moving average of the ratio between the daily range and normalized volume, signed by the day's price direction. High values suggest price exhaustion on thin liquidity, predicting a reversal.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Late-Day Exhaustion Factor', defined as the ratio of the final hour's price range to its relative volume, negatively predicts the next day's return when the closing price is at an extreme relative to the intraday range, signaling a liquidity-driven reversal.\n                Concise Observation: While morning volume often signals institutional entry and trend persistence, late-day price 'stretching' on low volume frequently precedes a correction as the market lacks the depth to sustain extreme price levels into the next session.\n                Concise Justification: Price movements unsupported by volume (liquidity exhaustion) are fragile; specifically, a high price-range-to-volume ratio at the close suggests that small trades are moving the price disproportionately, creating an unsustainable premium or discount.\n                Concise Knowledge: If a stock experiences high price volatility on declining volume during the market close, the price movement is likely driven by retail 'weak hands' or liquidity gaps rather than institutional conviction, leading to a mean-reversion reversal.\n                concise Specification: The factor is calculated as the 5-day moving average of the ratio [($high - $low) / ($volume / TS_MEAN($volume, 20))] multiplied by the sign of the late-day price change, focusing on cases where the closing price is in the top or bottom decile of the daily range.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "5a7d820694ab",
      "parent_trajectory_ids": [
        "13927f298658"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0020581968339796,
        "ICIR": 0.0143779058695892,
        "RankIC": 0.0181693560355686,
        "RankICIR": 0.1243136289374309,
        "annualized_return": 0.021181579593995,
        "information_ratio": 0.3123164104133376,
        "max_drawdown": -0.1893400839256434
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:44:42.508273",
      "updated_at": "2026-01-16T00:44:42.508280"
    },
    "1505a71baf7d868f": {
      "factor_id": "1505a71baf7d868f",
      "factor_name": "Liquidity_Exhaustion_Extreme_Filter_10D",
      "factor_expression": "RANK(TS_MEAN(($high - $low) / ($volume / (TS_MEAN($volume, 20) + 1e-8) + 1e-8), 10)) * SIGN(($close - $low) / ($high - $low + 1e-8) - 0.5)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(TS_MEAN(($high - $low) / ($volume / (TS_MEAN($volume, 20) + 1e-8) + 1e-8), 10)) * SIGN(($close - $low) / ($high - $low + 1e-8) - 0.5)\" # Your output factor expression will be filled in here\n    name = \"Liquidity_Exhaustion_Extreme_Filter_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies potential reversals by focusing on days where the price range is large relative to volume and the close is at an extreme of the daily range (top or bottom 10%). It uses a cross-sectional rank to identify the most 'exhausted' stocks.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Late-Day Exhaustion Factor', defined as the ratio of the final hour's price range to its relative volume, negatively predicts the next day's return when the closing price is at an extreme relative to the intraday range, signaling a liquidity-driven reversal.\n                Concise Observation: While morning volume often signals institutional entry and trend persistence, late-day price 'stretching' on low volume frequently precedes a correction as the market lacks the depth to sustain extreme price levels into the next session.\n                Concise Justification: Price movements unsupported by volume (liquidity exhaustion) are fragile; specifically, a high price-range-to-volume ratio at the close suggests that small trades are moving the price disproportionately, creating an unsustainable premium or discount.\n                Concise Knowledge: If a stock experiences high price volatility on declining volume during the market close, the price movement is likely driven by retail 'weak hands' or liquidity gaps rather than institutional conviction, leading to a mean-reversion reversal.\n                concise Specification: The factor is calculated as the 5-day moving average of the ratio [($high - $low) / ($volume / TS_MEAN($volume, 20))] multiplied by the sign of the late-day price change, focusing on cases where the closing price is in the top or bottom decile of the daily range.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "5a7d820694ab",
      "parent_trajectory_ids": [
        "13927f298658"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0020581968339796,
        "ICIR": 0.0143779058695892,
        "RankIC": 0.0181693560355686,
        "RankICIR": 0.1243136289374309,
        "annualized_return": 0.021181579593995,
        "information_ratio": 0.3123164104133376,
        "max_drawdown": -0.1893400839256434
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:44:42.519416",
      "updated_at": "2026-01-16T00:44:42.519423"
    },
    "ff674eb4dd81637a": {
      "factor_id": "ff674eb4dd81637a",
      "factor_name": "Price_Volume_Divergence_ZScore_20D",
      "factor_expression": "TS_ZSCORE(($high - $low) / ($volume + 1e-8), 20)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_ZSCORE(($high - $low) / (TS_MEAN($volume, 20) + 1e-8), 20)\" # Your output factor expression will be filled in here\n    name = \"Price_Volume_Divergence_ZScore_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Measures the divergence between price volatility and trading volume. It calculates the ratio of the daily range to a 20-day volume moving average, then applies a time-series Z-score to detect statistically significant exhaustion events.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Late-Day Exhaustion Factor', defined as the ratio of the final hour's price range to its relative volume, negatively predicts the next day's return when the closing price is at an extreme relative to the intraday range, signaling a liquidity-driven reversal.\n                Concise Observation: While morning volume often signals institutional entry and trend persistence, late-day price 'stretching' on low volume frequently precedes a correction as the market lacks the depth to sustain extreme price levels into the next session.\n                Concise Justification: Price movements unsupported by volume (liquidity exhaustion) are fragile; specifically, a high price-range-to-volume ratio at the close suggests that small trades are moving the price disproportionately, creating an unsustainable premium or discount.\n                Concise Knowledge: If a stock experiences high price volatility on declining volume during the market close, the price movement is likely driven by retail 'weak hands' or liquidity gaps rather than institutional conviction, leading to a mean-reversion reversal.\n                concise Specification: The factor is calculated as the 5-day moving average of the ratio [($high - $low) / ($volume / TS_MEAN($volume, 20))] multiplied by the sign of the late-day price change, focusing on cases where the closing price is in the top or bottom decile of the daily range.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "5a7d820694ab",
      "parent_trajectory_ids": [
        "13927f298658"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0020581968339796,
        "ICIR": 0.0143779058695892,
        "RankIC": 0.0181693560355686,
        "RankICIR": 0.1243136289374309,
        "annualized_return": 0.021181579593995,
        "information_ratio": 0.3123164104133376,
        "max_drawdown": -0.1893400839256434
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:44:42.530431",
      "updated_at": "2026-01-16T00:44:42.530437"
    },
    "e18a21bc7e9ac541": {
      "factor_id": "e18a21bc7e9ac541",
      "factor_name": "Clean_Idio_Sharpe_Exhaustion_Adj_20D",
      "factor_expression": "(TS_MEAN($return - MEAN($return), 20) / (TS_STD($return - MEAN($return), 20) + 1e-8)) / (TS_MEAN(($high - $low) / ($volume + 1e-8), 5) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"(TS_MEAN($return - MEAN($return), 20) / (TS_STD($return - MEAN($return), 20) + 1e-8)) / (TS_MEAN(($high - $low) / ($volume + 1e-8), 5) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Clean_Idio_Sharpe_Exhaustion_Adj_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor calculates the 20-day idiosyncratic Sharpe ratio (using the residual of stock returns against the cross-sectional mean as a market proxy) and divides it by a 5-day price exhaustion proxy (range relative to volume). It aims to identify high-conviction trends that are not driven by liquidity-induced price blow-offs.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Clean Idiosyncratic Momentum' factor, defined as the 20-day idiosyncratic return Sharpe ratio divided by a 5-day late-day exhaustion proxy (the ratio of high-low range to volume), positively predicts future returns by isolating market-independent trends that lack signs of liquidity-driven reversal.\n                Concise Observation: Parent 1 showed that risk-adjusted idiosyncratic returns are predictive (RankIC=0.021), while Parent 2 highlighted that late-day price extremes relative to volume signal exhaustion (RankIC=0.018); combining them addresses the tendency of momentum to fail during liquidity-driven blow-offs.\n                Concise Justification: By using the exhaustion metric as a denominator for the idiosyncratic Sharpe ratio, we create a liquidity-adjusted performance metric that penalizes 'low-quality' momentum driven by thin-market volatility, thereby focusing on institutional-led accumulation.\n                Concise Knowledge: If a stock's idiosyncratic return is high relative to its volatility, it indicates strong firm-specific conviction; when this is coupled with low late-day price exhaustion (small range relative to volume), the trend is more likely to persist rather than reverse due to retail-driven liquidity shocks.\n                concise Specification: Calculate the 20-day idiosyncratic return (residual of stock return vs market return), divide its mean by its standard deviation over 20 days, and then divide this result by the 5-day average of (High - Low) / Volume to generate the final factor.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "b85c495c2143",
      "parent_trajectory_ids": [
        "4edcfc71ba33",
        "3898fba39461"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0009561382392437,
        "ICIR": 0.007155781385246,
        "RankIC": 0.0145715380871763,
        "RankICIR": 0.109959677637194,
        "annualized_return": 0.0134292205268664,
        "information_ratio": 0.1897314116494671,
        "max_drawdown": -0.1868726684946972
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:47:36.565156",
      "updated_at": "2026-01-16T00:47:36.565163"
    },
    "137f41f3beb9fbfb": {
      "factor_id": "137f41f3beb9fbfb",
      "factor_name": "Idio_Momentum_Quality_Score_15D",
      "factor_expression": "(TS_MEAN($return, 15) / (TS_STD($return, 15) + 1e-8)) / (TS_MEAN(TS_STD($return, 5) / ($volume + 1e-8), 5) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"(TS_MEAN($return, 15) / (TS_STD($return, 15) + 1e-8)) / (TS_MEAN(TS_STD($return, 5) / ($volume + 1e-8), 5) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Idio_Momentum_Quality_Score_15D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A simplified version of the idiosyncratic momentum quality factor. It measures the risk-adjusted idiosyncratic return over 15 days, penalized by the recent 5-day volatility-to-volume ratio to filter out low-liquidity price spikes.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Clean Idiosyncratic Momentum' factor, defined as the 20-day idiosyncratic return Sharpe ratio divided by a 5-day late-day exhaustion proxy (the ratio of high-low range to volume), positively predicts future returns by isolating market-independent trends that lack signs of liquidity-driven reversal.\n                Concise Observation: Parent 1 showed that risk-adjusted idiosyncratic returns are predictive (RankIC=0.021), while Parent 2 highlighted that late-day price extremes relative to volume signal exhaustion (RankIC=0.018); combining them addresses the tendency of momentum to fail during liquidity-driven blow-offs.\n                Concise Justification: By using the exhaustion metric as a denominator for the idiosyncratic Sharpe ratio, we create a liquidity-adjusted performance metric that penalizes 'low-quality' momentum driven by thin-market volatility, thereby focusing on institutional-led accumulation.\n                Concise Knowledge: If a stock's idiosyncratic return is high relative to its volatility, it indicates strong firm-specific conviction; when this is coupled with low late-day price exhaustion (small range relative to volume), the trend is more likely to persist rather than reverse due to retail-driven liquidity shocks.\n                concise Specification: Calculate the 20-day idiosyncratic return (residual of stock return vs market return), divide its mean by its standard deviation over 20 days, and then divide this result by the 5-day average of (High - Low) / Volume to generate the final factor.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "b85c495c2143",
      "parent_trajectory_ids": [
        "4edcfc71ba33",
        "3898fba39461"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0009561382392437,
        "ICIR": 0.007155781385246,
        "RankIC": 0.0145715380871763,
        "RankICIR": 0.109959677637194,
        "annualized_return": 0.0134292205268664,
        "information_ratio": 0.1897314116494671,
        "max_drawdown": -0.1868726684946972
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:47:36.575928",
      "updated_at": "2026-01-16T00:47:36.575934"
    },
    "3aa298f3fab2d209": {
      "factor_id": "3aa298f3fab2d209",
      "factor_name": "Exhaustion_Filtered_Idio_Trend_20D",
      "factor_expression": "RANK(TS_MEAN($return - MEAN($return), 20)) * RANK(INV(TS_MEAN(($high - $low) / ($volume + 1e-8), 10)))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(TS_MEAN($return - MEAN($return), 20)) * RANK(INV(TS_MEAN(($high - $low) / ($volume + 1e-8), 10)))\" # Your output factor expression will be filled in here\n    name = \"Exhaustion_Filtered_Idio_Trend_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies sustainable idiosyncratic trends by calculating the 20-day idiosyncratic return (relative to market mean) and scaling it by the inverse of the 10-day price range-volume ratio. This focuses on stocks with steady price appreciation and high volume support.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Clean Idiosyncratic Momentum' factor, defined as the 20-day idiosyncratic return Sharpe ratio divided by a 5-day late-day exhaustion proxy (the ratio of high-low range to volume), positively predicts future returns by isolating market-independent trends that lack signs of liquidity-driven reversal.\n                Concise Observation: Parent 1 showed that risk-adjusted idiosyncratic returns are predictive (RankIC=0.021), while Parent 2 highlighted that late-day price extremes relative to volume signal exhaustion (RankIC=0.018); combining them addresses the tendency of momentum to fail during liquidity-driven blow-offs.\n                Concise Justification: By using the exhaustion metric as a denominator for the idiosyncratic Sharpe ratio, we create a liquidity-adjusted performance metric that penalizes 'low-quality' momentum driven by thin-market volatility, thereby focusing on institutional-led accumulation.\n                Concise Knowledge: If a stock's idiosyncratic return is high relative to its volatility, it indicates strong firm-specific conviction; when this is coupled with low late-day price exhaustion (small range relative to volume), the trend is more likely to persist rather than reverse due to retail-driven liquidity shocks.\n                concise Specification: Calculate the 20-day idiosyncratic return (residual of stock return vs market return), divide its mean by its standard deviation over 20 days, and then divide this result by the 5-day average of (High - Low) / Volume to generate the final factor.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "b85c495c2143",
      "parent_trajectory_ids": [
        "4edcfc71ba33",
        "3898fba39461"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0009561382392437,
        "ICIR": 0.007155781385246,
        "RankIC": 0.0145715380871763,
        "RankICIR": 0.109959677637194,
        "annualized_return": 0.0134292205268664,
        "information_ratio": 0.1897314116494671,
        "max_drawdown": -0.1868726684946972
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:47:36.586645",
      "updated_at": "2026-01-16T00:47:36.586651"
    },
    "88a8d5e665ea6dbc": {
      "factor_id": "88a8d5e665ea6dbc",
      "factor_name": "Overnight_Efficiency_Inverse_5D",
      "factor_expression": "($open / DELAY($close, 1) - 1) / (TS_MEAN(ABS($return) / LOG($volume + 1.0001), 5) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"($open / DELAY($close, 1) - 1) / (TS_MEAN(ABS($return) / LOG($volume + 1.0001), 5) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Overnight_Efficiency_Inverse_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures the interaction between overnight momentum and price-volume efficiency. It calculates the overnight return (open to previous close) and scales it by the inverse of the 5-day average price efficiency (absolute return per log volume). High efficiency signals trend exhaustion, so the factor penalizes momentum when efficiency is high, favoring 'healthy' institutional accumulation on lower relative efficiency.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The interaction between the overnight-to-open return and the 5-day volume-price efficiency (absolute return over log volume) predicts future returns, where overnight momentum is predictive only under conditions of low price efficiency, while high efficiency signals trend exhaustion.\n                Concise Observation: Parent 1 shows that early-session momentum captures institutional intent (RankIC 0.021), while Parent 2 identifies that extreme price-to-volume ratios (efficiency) act as a mean-reversion signal (RankIC 0.017).\n                Concise Justification: Combining these allows the factor to distinguish between 'healthy' institutional flow (momentum with volume support) and 'fragile' price moves (momentum on thin liquidity), enhancing the signal-to-noise ratio of morning returns.\n                Concise Knowledge: If price movement occurs with low volume-price efficiency, it suggests institutional accumulation with sufficient liquidity; when efficiency spikes, it indicates liquidity exhaustion where even small volumes cause large price swings, often preceding reversals.\n                concise Specification: Define Morning Momentum as ($open / $close[t-1] - 1). Define 5-day Efficiency as the 5-day average of (abs($return) / log($volume + 1)). The final factor is Morning Momentum multiplied by the inverse of 5-day Efficiency to penalize exhausted trends.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "11031212217d",
      "parent_trajectory_ids": [
        "13927f298658",
        "ea8458f9db40"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0033708712970988,
        "ICIR": 0.0257745492721858,
        "RankIC": 0.0197337828373769,
        "RankICIR": 0.155771531867544,
        "annualized_return": 0.033277953418183,
        "information_ratio": 0.5781312006398605,
        "max_drawdown": -0.0863480427293774
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:50:26.262101",
      "updated_at": "2026-01-16T00:50:26.262109"
    },
    "2f20e429fd8b17c2": {
      "factor_id": "2f20e429fd8b17c2",
      "factor_name": "Ranked_Morning_Efficiency_Signal_5D",
      "factor_expression": "RANK($open / DELAY($close, 1) - 1) / (TS_MEAN(ABS($return) / LOG($volume + 1.0001), 5) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK($open / DELAY($close, 1) - 1) / (TS_MEAN(ABS($return) / LOG($volume + 1.0001), 5) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Ranked_Morning_Efficiency_Signal_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A cross-sectionally robust version of the morning momentum and efficiency interaction. It ranks the overnight return and divides it by the 5-day rolling average of price efficiency. By using RANK on the overnight return, it normalizes the institutional intent signal before adjusting for the liquidity-driven exhaustion signaled by high price-volume efficiency.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The interaction between the overnight-to-open return and the 5-day volume-price efficiency (absolute return over log volume) predicts future returns, where overnight momentum is predictive only under conditions of low price efficiency, while high efficiency signals trend exhaustion.\n                Concise Observation: Parent 1 shows that early-session momentum captures institutional intent (RankIC 0.021), while Parent 2 identifies that extreme price-to-volume ratios (efficiency) act as a mean-reversion signal (RankIC 0.017).\n                Concise Justification: Combining these allows the factor to distinguish between 'healthy' institutional flow (momentum with volume support) and 'fragile' price moves (momentum on thin liquidity), enhancing the signal-to-noise ratio of morning returns.\n                Concise Knowledge: If price movement occurs with low volume-price efficiency, it suggests institutional accumulation with sufficient liquidity; when efficiency spikes, it indicates liquidity exhaustion where even small volumes cause large price swings, often preceding reversals.\n                concise Specification: Define Morning Momentum as ($open / $close[t-1] - 1). Define 5-day Efficiency as the 5-day average of (abs($return) / log($volume + 1)). The final factor is Morning Momentum multiplied by the inverse of 5-day Efficiency to penalize exhausted trends.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "11031212217d",
      "parent_trajectory_ids": [
        "13927f298658",
        "ea8458f9db40"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0033708712970988,
        "ICIR": 0.0257745492721858,
        "RankIC": 0.0197337828373769,
        "RankICIR": 0.155771531867544,
        "annualized_return": 0.033277953418183,
        "information_ratio": 0.5781312006398605,
        "max_drawdown": -0.0863480427293774
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:50:26.273110",
      "updated_at": "2026-01-16T00:50:26.273116"
    },
    "bdac5c03f6089018": {
      "factor_id": "bdac5c03f6089018",
      "factor_name": "ZScored_Overnight_Efficiency_Impact",
      "factor_expression": "ZSCORE($open / DELAY($close, 1) - 1) - ZSCORE(TS_MEAN(ABS($return) / LOG($volume + 1.0001), 5))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"ZSCORE($open / DELAY($close, 1) - 1) - ZSCORE(TS_MEAN(ABS($return) / LOG($volume + 1.0001), 5))\" # Your output factor expression will be filled in here\n    name = \"ZScored_Overnight_Efficiency_Impact\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures the overnight return relative to its efficiency context using Z-scores. It identifies stocks where overnight momentum is strong but efficiency (price change per unit of log volume) is low, suggesting sustainable institutional flow rather than a liquidity-gap driven spike.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The interaction between the overnight-to-open return and the 5-day volume-price efficiency (absolute return over log volume) predicts future returns, where overnight momentum is predictive only under conditions of low price efficiency, while high efficiency signals trend exhaustion.\n                Concise Observation: Parent 1 shows that early-session momentum captures institutional intent (RankIC 0.021), while Parent 2 identifies that extreme price-to-volume ratios (efficiency) act as a mean-reversion signal (RankIC 0.017).\n                Concise Justification: Combining these allows the factor to distinguish between 'healthy' institutional flow (momentum with volume support) and 'fragile' price moves (momentum on thin liquidity), enhancing the signal-to-noise ratio of morning returns.\n                Concise Knowledge: If price movement occurs with low volume-price efficiency, it suggests institutional accumulation with sufficient liquidity; when efficiency spikes, it indicates liquidity exhaustion where even small volumes cause large price swings, often preceding reversals.\n                concise Specification: Define Morning Momentum as ($open / $close[t-1] - 1). Define 5-day Efficiency as the 5-day average of (abs($return) / log($volume + 1)). The final factor is Morning Momentum multiplied by the inverse of 5-day Efficiency to penalize exhausted trends.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "11031212217d",
      "parent_trajectory_ids": [
        "13927f298658",
        "ea8458f9db40"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0033708712970988,
        "ICIR": 0.0257745492721858,
        "RankIC": 0.0197337828373769,
        "RankICIR": 0.155771531867544,
        "annualized_return": 0.033277953418183,
        "information_ratio": 0.5781312006398605,
        "max_drawdown": -0.0863480427293774
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:50:26.285262",
      "updated_at": "2026-01-16T00:50:26.285268"
    },
    "bb4ed84282454721": {
      "factor_id": "bb4ed84282454721",
      "factor_name": "Intraday_Resolution_Ratio_1D",
      "factor_expression": "RANK(($open / DELAY($close, 1) - 1) / (($high - $low) / ($open + 1e-8)))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($open / DELAY($close, 1) - 1) / ((($high - $low) / $open) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Intraday_Resolution_Ratio_1D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Calculates the ratio of the overnight gap to the intraday high-low range. High intraday range relative to the gap suggests information exhaustion and potential mean reversion.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Intraday Information Resolution' factor, calculated as the ratio of the overnight gap to the daily high-low range, negatively predicts next-day returns when the daily price range is significantly wider than the gap, signaling a mean reversion after the exhaustion of information-driven shocks.\n                Concise Observation: While long-term idiosyncratic momentum captures persistent trends, short-term dislocations often show that the ratio of the overnight jump to the total daily volatility (high-low range) serves as a proxy for 'informational cooling' or exhaustion.\n                Concise Justification: Stocks with high intraday volatility relative to their overnight gap suggest that market participants have actively re-priced the asset throughout the session, often leading to overshooting and subsequent mean reversion once the liquidity shock stabilizes.\n                Concise Knowledge: If a large overnight price gap is followed by an even larger intraday trading range, the initial information shock is likely fully absorbed or overextended; thus, a price reversal is expected as the uncertainty premium dissipates.\n                concise Specification: The factor is defined as ($open / DELAY($close, 1) - 1) divided by (($high - $low) / $open + 1e-8), using a 1-day lookback to capture immediate mean reversion, orthogonal to 20-day momentum trends.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "4e1b9af6e4d1",
      "parent_trajectory_ids": [
        "21f386cfc7b9"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0075952502790511,
        "ICIR": 0.0551435952910843,
        "RankIC": 0.0249395644776861,
        "RankICIR": 0.182252682451474,
        "annualized_return": 0.0782994228244036,
        "information_ratio": 1.164956926470362,
        "max_drawdown": -0.1103314040296943
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:53:03.943489",
      "updated_at": "2026-01-16T00:53:03.943496"
    },
    "7f0eabe11514be1a": {
      "factor_id": "7f0eabe11514be1a",
      "factor_name": "Volatility_Adjusted_Gap_Exhaustion",
      "factor_expression": "ZSCORE(ABS($open - DELAY($close, 1)) / (TS_MEAN($high - $low, 5) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"ZSCORE(ABS($open - DELAY($close, 1)) / (TS_MEAN($high - $low, 5) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Volatility_Adjusted_Gap_Exhaustion\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Measures the overnight gap normalized by the 5-day average intraday range. It captures short-term dislocations where the gap is small relative to the recent intraday volatility, signaling informational cooling.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Intraday Information Resolution' factor, calculated as the ratio of the overnight gap to the daily high-low range, negatively predicts next-day returns when the daily price range is significantly wider than the gap, signaling a mean reversion after the exhaustion of information-driven shocks.\n                Concise Observation: While long-term idiosyncratic momentum captures persistent trends, short-term dislocations often show that the ratio of the overnight jump to the total daily volatility (high-low range) serves as a proxy for 'informational cooling' or exhaustion.\n                Concise Justification: Stocks with high intraday volatility relative to their overnight gap suggest that market participants have actively re-priced the asset throughout the session, often leading to overshooting and subsequent mean reversion once the liquidity shock stabilizes.\n                Concise Knowledge: If a large overnight price gap is followed by an even larger intraday trading range, the initial information shock is likely fully absorbed or overextended; thus, a price reversal is expected as the uncertainty premium dissipates.\n                concise Specification: The factor is defined as ($open / DELAY($close, 1) - 1) divided by (($high - $low) / $open + 1e-8), using a 1-day lookback to capture immediate mean reversion, orthogonal to 20-day momentum trends.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "4e1b9af6e4d1",
      "parent_trajectory_ids": [
        "21f386cfc7b9"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0075952502790511,
        "ICIR": 0.0551435952910843,
        "RankIC": 0.0249395644776861,
        "RankICIR": 0.182252682451474,
        "annualized_return": 0.0782994228244036,
        "information_ratio": 1.164956926470362,
        "max_drawdown": -0.1103314040296943
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:53:03.954525",
      "updated_at": "2026-01-16T00:53:03.954531"
    },
    "d2a69ad23addab30": {
      "factor_id": "d2a69ad23addab30",
      "factor_name": "Range_Gap_Divergence_Factor",
      "factor_expression": "RANK(($high - $low) / (ABS($open - DELAY($close, 1)) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($high - $low) / (ABS($open - DELAY($close, 1)) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Range_Gap_Divergence_Factor\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Identifies stocks where the intraday price movement (high-low) significantly exceeds the overnight jump, suggesting that the initial shock was overextended by intraday trading activity.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Intraday Information Resolution' factor, calculated as the ratio of the overnight gap to the daily high-low range, negatively predicts next-day returns when the daily price range is significantly wider than the gap, signaling a mean reversion after the exhaustion of information-driven shocks.\n                Concise Observation: While long-term idiosyncratic momentum captures persistent trends, short-term dislocations often show that the ratio of the overnight jump to the total daily volatility (high-low range) serves as a proxy for 'informational cooling' or exhaustion.\n                Concise Justification: Stocks with high intraday volatility relative to their overnight gap suggest that market participants have actively re-priced the asset throughout the session, often leading to overshooting and subsequent mean reversion once the liquidity shock stabilizes.\n                Concise Knowledge: If a large overnight price gap is followed by an even larger intraday trading range, the initial information shock is likely fully absorbed or overextended; thus, a price reversal is expected as the uncertainty premium dissipates.\n                concise Specification: The factor is defined as ($open / DELAY($close, 1) - 1) divided by (($high - $low) / $open + 1e-8), using a 1-day lookback to capture immediate mean reversion, orthogonal to 20-day momentum trends.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "4e1b9af6e4d1",
      "parent_trajectory_ids": [
        "21f386cfc7b9"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0075952502790511,
        "ICIR": 0.0551435952910843,
        "RankIC": 0.0249395644776861,
        "RankICIR": 0.182252682451474,
        "annualized_return": 0.0782994228244036,
        "information_ratio": 1.164956926470362,
        "max_drawdown": -0.1103314040296943
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:53:03.965465",
      "updated_at": "2026-01-16T00:53:03.965471"
    },
    "2944cf1b3ee9aa78": {
      "factor_id": "2944cf1b3ee9aa78",
      "factor_name": "IVVR_Liquidity_Shock_1D",
      "factor_expression": "RANK(($high - $low) / (LOG($volume + 1.0) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($high - $low) / (LOG($volume + 1.0) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"IVVR_Liquidity_Shock_1D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures liquidity-driven price dislocations by calculating the ratio of the daily price range (high-low) to the log-transformed volume. High values indicate extreme volatility on low relative volume, suggesting market-maker withdrawal and potential mean reversion.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Intraday Volatility-to-Volume Ratio (IVVR) during the final 30 minutes of trading serves as a proxy for liquidity-driven price dislocations, where high volatility on low relative volume indicates market-maker withdrawal and predicts a mean-reverting price correction on the following day.\n                Concise Observation: Parent strategies focused on volume-price efficiency over 5 days, but failed to account for the 'liquidity shock' effect where extreme price swings on thin volume at market close create temporary mispricings.\n                Concise Justification: Market makers widen spreads or withdraw during periods of high uncertainty (order flow toxicity), leading to high realized volatility relative to volume; as they return the next day, the 'uncertainty premium' decays, causing mean reversion.\n                Concise Knowledge: If a price move occurs with high volatility but disproportionately low volume, it is likely driven by liquidity gaps rather than informed conviction; such moves tend to mean-revert when liquidity stabilizes in the next session.\n                concise Specification: The factor is defined as the ratio of the 1-day intraday range (High-Low) to the log-transformed volume, specifically targeting the terminal trading state to capture liquidity exhaustion, expected to have a negative correlation with next-day returns.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "39c18c29bcfd",
      "parent_trajectory_ids": [
        "826fd82e0154"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0001248179070563,
        "ICIR": 0.0009153145009984,
        "RankIC": 0.0155852116799633,
        "RankICIR": 0.1123863779758524,
        "annualized_return": 0.016179698070603,
        "information_ratio": 0.2156781066493316,
        "max_drawdown": -0.216186646436848
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:55:21.956622",
      "updated_at": "2026-01-16T00:55:21.956630"
    },
    "db818e9f3678b705": {
      "factor_id": "db818e9f3678b705",
      "factor_name": "Relative_Volatility_Volume_Efficiency_5D",
      "factor_expression": "ZSCORE(TS_MEAN(($high - $low) / ($volume + 1.0), 5))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"ZSCORE(TS_MEAN(($high - $low) / ($volume + 1.0), 5))\" # Your output factor expression will be filled in here\n    name = \"Relative_Volatility_Volume_Efficiency_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures the 5-day average of the ratio between price range and volume, standardized cross-sectionally. It identifies assets where price moves are 'expensive' in terms of volatility relative to the liquidity provided, targeting mean-reversion candidates.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Intraday Volatility-to-Volume Ratio (IVVR) during the final 30 minutes of trading serves as a proxy for liquidity-driven price dislocations, where high volatility on low relative volume indicates market-maker withdrawal and predicts a mean-reverting price correction on the following day.\n                Concise Observation: Parent strategies focused on volume-price efficiency over 5 days, but failed to account for the 'liquidity shock' effect where extreme price swings on thin volume at market close create temporary mispricings.\n                Concise Justification: Market makers widen spreads or withdraw during periods of high uncertainty (order flow toxicity), leading to high realized volatility relative to volume; as they return the next day, the 'uncertainty premium' decays, causing mean reversion.\n                Concise Knowledge: If a price move occurs with high volatility but disproportionately low volume, it is likely driven by liquidity gaps rather than informed conviction; such moves tend to mean-revert when liquidity stabilizes in the next session.\n                concise Specification: The factor is defined as the ratio of the 1-day intraday range (High-Low) to the log-transformed volume, specifically targeting the terminal trading state to capture liquidity exhaustion, expected to have a negative correlation with next-day returns.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "39c18c29bcfd",
      "parent_trajectory_ids": [
        "826fd82e0154"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0001248179070563,
        "ICIR": 0.0009153145009984,
        "RankIC": 0.0155852116799633,
        "RankICIR": 0.1123863779758524,
        "annualized_return": 0.016179698070603,
        "information_ratio": 0.2156781066493316,
        "max_drawdown": -0.216186646436848
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:55:21.968923",
      "updated_at": "2026-01-16T00:55:21.968929"
    },
    "b3f0f6e201d1289b": {
      "factor_id": "b3f0f6e201d1289b",
      "factor_name": "Terminal_Liquidity_Exhaustion_Index",
      "factor_expression": "TS_RANK(($high - $low) / (LOG($volume + 1.0) + 1e-8), 10)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_RANK(($high - $low) / (LOG($volume + 1.0) + 1e-8), 10)\" # Your output factor expression will be filled in here\n    name = \"Terminal_Liquidity_Exhaustion_Index\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Captures the intensity of price range relative to volume growth. By comparing the current day's range-to-volume ratio against its 10-day historical rank, it isolates 'liquidity shocks' where volatility spikes disproportionately to volume.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Intraday Volatility-to-Volume Ratio (IVVR) during the final 30 minutes of trading serves as a proxy for liquidity-driven price dislocations, where high volatility on low relative volume indicates market-maker withdrawal and predicts a mean-reverting price correction on the following day.\n                Concise Observation: Parent strategies focused on volume-price efficiency over 5 days, but failed to account for the 'liquidity shock' effect where extreme price swings on thin volume at market close create temporary mispricings.\n                Concise Justification: Market makers widen spreads or withdraw during periods of high uncertainty (order flow toxicity), leading to high realized volatility relative to volume; as they return the next day, the 'uncertainty premium' decays, causing mean reversion.\n                Concise Knowledge: If a price move occurs with high volatility but disproportionately low volume, it is likely driven by liquidity gaps rather than informed conviction; such moves tend to mean-revert when liquidity stabilizes in the next session.\n                concise Specification: The factor is defined as the ratio of the 1-day intraday range (High-Low) to the log-transformed volume, specifically targeting the terminal trading state to capture liquidity exhaustion, expected to have a negative correlation with next-day returns.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "39c18c29bcfd",
      "parent_trajectory_ids": [
        "826fd82e0154"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0001248179070563,
        "ICIR": 0.0009153145009984,
        "RankIC": 0.0155852116799633,
        "RankICIR": 0.1123863779758524,
        "annualized_return": 0.016179698070603,
        "information_ratio": 0.2156781066493316,
        "max_drawdown": -0.216186646436848
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:55:21.980122",
      "updated_at": "2026-01-16T00:55:21.980128"
    },
    "0453ab689c4a772f": {
      "factor_id": "0453ab689c4a772f",
      "factor_name": "Info_Efficiency_Resonance_5D",
      "factor_expression": "ZSCORE(($open - DELAY($close, 1)) / ($high - $low + 1e-6)) * ZSCORE(ABS($close - DELAY($close, 5)) / (LOG(TS_SUM($volume, 5) + 1e-8) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"ZSCORE(($open - DELAY($close, 1)) / ($high - $low + 1e-6)) * ZSCORE(ABS($close - DELAY($close, 5)) / (LOG(TS_SUM($volume, 5) + 1e-8) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Info_Efficiency_Resonance_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures the interaction between the overnight gap ratio and liquidity-adjusted price efficiency. It identifies high-quality price discovery when a gap is supported by efficient price movement relative to volume, while signaling potential exhaustion when they diverge. The gap ratio is normalized by the daily range, and efficiency is measured as the absolute return relative to the log of cumulative volume.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Information-Efficiency Resonance' factor, defined as the product of the overnight gap ratio (gap divided by daily range) and the 5-day liquidity-adjusted price efficiency, positively predicts returns when both metrics align, and signals mean-reversion when they diverge.\n                Concise Observation: Parent 1 showed that the ratio of gap to range (RankIC 0.0249) captures intraday resolution, while Parent 2 showed that volume-price efficiency (RankIC 0.0197) identifies momentum quality, suggesting their interaction could filter noise from true price discovery.\n                Concise Justification: Combining price-based exhaustion (range) with volume-based efficiency (liquidity impact) creates a multi-dimensional filter that distinguishes between institutional-led trend continuations and retail-driven speculative gaps that lack structural support.\n                Concise Knowledge: If an overnight price shock is accompanied by high volume-price efficiency and low intraday volatility relative to that shock, it indicates high-quality information assimilation; conversely, if a gap is followed by high intraday range and low efficiency, it suggests information exhaustion and price reversal.\n                concise Specification: Calculate the gap ratio as ($open - $close.shift(1)) / ($high - $low + 1e-6); calculate 5-day efficiency as the absolute 5-day return divided by the log of 5-day cumulative volume; the factor is the product of the Z-scored gap ratio and the Z-scored 5-day efficiency.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "ba7a484e03aa",
      "parent_trajectory_ids": [
        "30ba440c1721",
        "826fd82e0154"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0018740809028642,
        "ICIR": 0.0131308552830252,
        "RankIC": 0.0160552433975495,
        "RankICIR": 0.1112628879058081,
        "annualized_return": 0.0691970942554311,
        "information_ratio": 0.9455638720192642,
        "max_drawdown": -0.0998719450420972
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:58:32.733207",
      "updated_at": "2026-01-16T00:58:32.733214"
    },
    "8487c40587974172": {
      "factor_id": "8487c40587974172",
      "factor_name": "Gap_Efficiency_Momentum_Filter",
      "factor_expression": "RANK(($open - DELAY($close, 1)) / ($high - $low + 1e-6)) + RANK(($close - DELAY($close, 5)) / (TS_SUM($volume, 5) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($open - DELAY($close, 1)) / ($high - $low + 1e-6)) + RANK(($close - DELAY($close, 5)) / (TS_SUM($volume, 5) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Gap_Efficiency_Momentum_Filter\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A simplified version of the information-efficiency hypothesis focusing on the cross-sectional rank of the gap ratio and the 5-day price-volume efficiency. It aims to filter for stocks where overnight momentum is validated by high-volume efficiency, reducing noise from speculative gaps.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Information-Efficiency Resonance' factor, defined as the product of the overnight gap ratio (gap divided by daily range) and the 5-day liquidity-adjusted price efficiency, positively predicts returns when both metrics align, and signals mean-reversion when they diverge.\n                Concise Observation: Parent 1 showed that the ratio of gap to range (RankIC 0.0249) captures intraday resolution, while Parent 2 showed that volume-price efficiency (RankIC 0.0197) identifies momentum quality, suggesting their interaction could filter noise from true price discovery.\n                Concise Justification: Combining price-based exhaustion (range) with volume-based efficiency (liquidity impact) creates a multi-dimensional filter that distinguishes between institutional-led trend continuations and retail-driven speculative gaps that lack structural support.\n                Concise Knowledge: If an overnight price shock is accompanied by high volume-price efficiency and low intraday volatility relative to that shock, it indicates high-quality information assimilation; conversely, if a gap is followed by high intraday range and low efficiency, it suggests information exhaustion and price reversal.\n                concise Specification: Calculate the gap ratio as ($open - $close.shift(1)) / ($high - $low + 1e-6); calculate 5-day efficiency as the absolute 5-day return divided by the log of 5-day cumulative volume; the factor is the product of the Z-scored gap ratio and the Z-scored 5-day efficiency.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "ba7a484e03aa",
      "parent_trajectory_ids": [
        "30ba440c1721",
        "826fd82e0154"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0018740809028642,
        "ICIR": 0.0131308552830252,
        "RankIC": 0.0160552433975495,
        "RankICIR": 0.1112628879058081,
        "annualized_return": 0.0691970942554311,
        "information_ratio": 0.9455638720192642,
        "max_drawdown": -0.0998719450420972
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:58:32.745033",
      "updated_at": "2026-01-16T00:58:32.745039"
    },
    "db87b70656785dc5": {
      "factor_id": "db87b70656785dc5",
      "factor_name": "Liquidity_Adjusted_Gap_Resonance",
      "factor_expression": "TS_ZSCORE(($open - DELAY($close, 1)) / ($high - $low + 1e-6), 10) * RANK(TS_PCTCHANGE($close, 5) / (LOG(TS_SUM($volume, 5) + 1e-8) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_ZSCORE(($open - DELAY($close, 1)) / ($high - $low + 1e-6), 10) * RANK(TS_PCTCHANGE($close, 5) / (LOG(TS_SUM($volume, 5) + 1e-8) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Liquidity_Adjusted_Gap_Resonance\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures the resonance between the overnight gap and the 5-day price trend efficiency, adjusted for volume. By using the TS_ZSCORE of the gap ratio, it identifies statistically significant shocks that align with efficient volume-driven trends.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Information-Efficiency Resonance' factor, defined as the product of the overnight gap ratio (gap divided by daily range) and the 5-day liquidity-adjusted price efficiency, positively predicts returns when both metrics align, and signals mean-reversion when they diverge.\n                Concise Observation: Parent 1 showed that the ratio of gap to range (RankIC 0.0249) captures intraday resolution, while Parent 2 showed that volume-price efficiency (RankIC 0.0197) identifies momentum quality, suggesting their interaction could filter noise from true price discovery.\n                Concise Justification: Combining price-based exhaustion (range) with volume-based efficiency (liquidity impact) creates a multi-dimensional filter that distinguishes between institutional-led trend continuations and retail-driven speculative gaps that lack structural support.\n                Concise Knowledge: If an overnight price shock is accompanied by high volume-price efficiency and low intraday volatility relative to that shock, it indicates high-quality information assimilation; conversely, if a gap is followed by high intraday range and low efficiency, it suggests information exhaustion and price reversal.\n                concise Specification: Calculate the gap ratio as ($open - $close.shift(1)) / ($high - $low + 1e-6); calculate 5-day efficiency as the absolute 5-day return divided by the log of 5-day cumulative volume; the factor is the product of the Z-scored gap ratio and the Z-scored 5-day efficiency.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "ba7a484e03aa",
      "parent_trajectory_ids": [
        "30ba440c1721",
        "826fd82e0154"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0018740809028642,
        "ICIR": 0.0131308552830252,
        "RankIC": 0.0160552433975495,
        "RankICIR": 0.1112628879058081,
        "annualized_return": 0.0691970942554311,
        "information_ratio": 0.9455638720192642,
        "max_drawdown": -0.0998719450420972
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T00:58:32.757830",
      "updated_at": "2026-01-16T00:58:32.757837"
    },
    "98ac2317f829e9d4": {
      "factor_id": "98ac2317f829e9d4",
      "factor_name": "LVIQ_Factor_20D_5D",
      "factor_expression": "(TS_MEAN($return, 20) / (TS_STD($return, 20) + 1e-8)) / (TS_MEAN(($high - $low) / ($volume + 1e-8), 5) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"(TS_MEAN($return, 20) / (TS_STD($return, 20) + 1e-8)) / (TS_MEAN(($high - $low) / ($volume + 1e-8), 5) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"LVIQ_Factor_20D_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "The Liquidity-Validated Idiosyncratic Quality (LVIQ) factor identifies sustainable trends by dividing the 20-day idiosyncratic Sharpe ratio by the 5-day average of the price-range-to-volume ratio. This penalizes momentum driven by liquidity exhaustion (high volatility on low volume).",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Liquidity-Validated Idiosyncratic Quality (LVIQ) factor, defined as the 20-day idiosyncratic Sharpe ratio divided by the 5-day average of the daily high-low range to volume ratio, identifies sustainable trends by penalizing momentum driven by liquidity exhaustion.\n                Concise Observation: Parent 1 showed that high volatility-to-volume ratios indicate price dislocations (RankIC 0.0156), while Parent 2 demonstrated that idiosyncratic Sharpe ratios capture trend quality (RankIC 0.0146), suggesting their ratio can filter out 'fragile' momentum.\n                Concise Justification: By dividing the idiosyncratic Sharpe ratio (signal strength) by the volatility-to-volume ratio (exhaustion proxy), we isolate assets where price discovery is supported by liquidity rather than market-maker withdrawal or temporary imbalances.\n                Concise Knowledge: If a strong idiosyncratic return trend is accompanied by low price-range-to-volume ratios, it indicates high liquidity efficiency and trend sustainability; conversely, when high volatility occurs on low volume, it signals liquidity-driven exhaustion and likely mean reversion.\n                concise Specification: Calculate the 20-day idiosyncratic Sharpe ratio (mean return / std of returns) and divide it by the 5-day moving average of ($high - $low) / $volume; the resulting LVIQ factor is expected to have a positive correlation with future returns.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "33d22fca404f",
      "parent_trajectory_ids": [
        "acc9804d13e6",
        "21f386cfc7b9"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0016183452159953,
        "ICIR": 0.0120537456783712,
        "RankIC": 0.0148661119474565,
        "RankICIR": 0.1120655554275733,
        "annualized_return": 0.018961923839242,
        "information_ratio": 0.2422309898431016,
        "max_drawdown": -0.1414861789055896
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:01:11.017343",
      "updated_at": "2026-01-16T01:01:11.017350"
    },
    "d67480aea1514f0b": {
      "factor_id": "d67480aea1514f0b",
      "factor_name": "Z_LVIQ_Rank_CrossSectional",
      "factor_expression": "RANK((TS_MEAN($return, 20) / (TS_STD($return, 20) + 1e-8)) / (TS_MEAN(($high - $low) / ($volume + 1e-8), 5) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK((TS_MEAN($return, 20) / (TS_STD($return, 20) + 1e-8)) / (TS_MEAN(($high - $low) / ($volume + 1e-8), 5) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Z_LVIQ_Rank_CrossSectional\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A cross-sectionally standardized version of the LVIQ hypothesis. It calculates the ratio of the 20-day return-based Sharpe ratio to the 5-day liquidity-adjusted volatility, applying RANK to ensure comparability across different stocks.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Liquidity-Validated Idiosyncratic Quality (LVIQ) factor, defined as the 20-day idiosyncratic Sharpe ratio divided by the 5-day average of the daily high-low range to volume ratio, identifies sustainable trends by penalizing momentum driven by liquidity exhaustion.\n                Concise Observation: Parent 1 showed that high volatility-to-volume ratios indicate price dislocations (RankIC 0.0156), while Parent 2 demonstrated that idiosyncratic Sharpe ratios capture trend quality (RankIC 0.0146), suggesting their ratio can filter out 'fragile' momentum.\n                Concise Justification: By dividing the idiosyncratic Sharpe ratio (signal strength) by the volatility-to-volume ratio (exhaustion proxy), we isolate assets where price discovery is supported by liquidity rather than market-maker withdrawal or temporary imbalances.\n                Concise Knowledge: If a strong idiosyncratic return trend is accompanied by low price-range-to-volume ratios, it indicates high liquidity efficiency and trend sustainability; conversely, when high volatility occurs on low volume, it signals liquidity-driven exhaustion and likely mean reversion.\n                concise Specification: Calculate the 20-day idiosyncratic Sharpe ratio (mean return / std of returns) and divide it by the 5-day moving average of ($high - $low) / $volume; the resulting LVIQ factor is expected to have a positive correlation with future returns.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "33d22fca404f",
      "parent_trajectory_ids": [
        "acc9804d13e6",
        "21f386cfc7b9"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0016183452159953,
        "ICIR": 0.0120537456783712,
        "RankIC": 0.0148661119474565,
        "RankICIR": 0.1120655554275733,
        "annualized_return": 0.018961923839242,
        "information_ratio": 0.2422309898431016,
        "max_drawdown": -0.1414861789055896
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:01:11.028944",
      "updated_at": "2026-01-16T01:01:11.028950"
    },
    "9c20379ddfc41c29": {
      "factor_id": "9c20379ddfc41c29",
      "factor_name": "Liquidity_Efficiency_Trend_Quality",
      "factor_expression": "SMA(TS_MEAN($return, 20) * ($volume / ($high - $low + 1e-8)), 10, 1)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"SMA(TS_MEAN($return, 20) * ($volume / ($high - $low + 1e-8)), 10, 1)\" # Your output factor expression will be filled in here\n    name = \"Liquidity_Efficiency_Trend_Quality\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor focuses on the 'Knowledge' aspect of the hypothesis: identifying high liquidity efficiency. It uses the 20-day rolling mean return scaled by the inverse of the 5-day price-range-to-volume ratio, smoothed by a 10-day moving average.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Liquidity-Validated Idiosyncratic Quality (LVIQ) factor, defined as the 20-day idiosyncratic Sharpe ratio divided by the 5-day average of the daily high-low range to volume ratio, identifies sustainable trends by penalizing momentum driven by liquidity exhaustion.\n                Concise Observation: Parent 1 showed that high volatility-to-volume ratios indicate price dislocations (RankIC 0.0156), while Parent 2 demonstrated that idiosyncratic Sharpe ratios capture trend quality (RankIC 0.0146), suggesting their ratio can filter out 'fragile' momentum.\n                Concise Justification: By dividing the idiosyncratic Sharpe ratio (signal strength) by the volatility-to-volume ratio (exhaustion proxy), we isolate assets where price discovery is supported by liquidity rather than market-maker withdrawal or temporary imbalances.\n                Concise Knowledge: If a strong idiosyncratic return trend is accompanied by low price-range-to-volume ratios, it indicates high liquidity efficiency and trend sustainability; conversely, when high volatility occurs on low volume, it signals liquidity-driven exhaustion and likely mean reversion.\n                concise Specification: Calculate the 20-day idiosyncratic Sharpe ratio (mean return / std of returns) and divide it by the 5-day moving average of ($high - $low) / $volume; the resulting LVIQ factor is expected to have a positive correlation with future returns.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "33d22fca404f",
      "parent_trajectory_ids": [
        "acc9804d13e6",
        "21f386cfc7b9"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0016183452159953,
        "ICIR": 0.0120537456783712,
        "RankIC": 0.0148661119474565,
        "RankICIR": 0.1120655554275733,
        "annualized_return": 0.018961923839242,
        "information_ratio": 0.2422309898431016,
        "max_drawdown": -0.1414861789055896
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:01:11.041498",
      "updated_at": "2026-01-16T01:01:11.041504"
    },
    "8c7f9cf8385ff8ed": {
      "factor_id": "8c7f9cf8385ff8ed",
      "factor_name": "Price_Volume_Discrepancy_20D",
      "factor_expression": "RANK(($high - $low) / (TS_STD($return, 20) + 1e-8)) / (RANK(TS_MEAN($volume, 20)) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($high - $low) / (TS_STD($return, 20) + 1e-8)) / (RANK(TS_MEAN($volume, 20)) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Price_Volume_Discrepancy_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures the discrepancy between price range and return volatility, adjusted by volume. It identifies periods where intraday price movement (High-Low) is high relative to the consistency of daily returns and average volume, signaling potential trend exhaustion or noise-driven moves.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Intraday Volatility-Volume Entropy' factor, defined as the ratio of high-low price range to the standard deviation of volume-weighted price changes over a rolling window, predicts next-day returns by identifying periods where price movements are unsupported by consistent volume distribution.\n                Concise Observation: The parent strategy focused on overnight gaps and 5-day price efficiency (low-frequency), but failed to capture the intraday 'texture' of price discovery which often leads to higher Information Ratios by filtering out noise-driven price moves.\n                Concise Justification: By comparing the total daily range to the intraday variance of returns, we can distinguish between 'clean' trends (low entropy/high conviction) and 'noisy' mean-reverting fluctuations (high entropy/low conviction), providing a signal orthogonal to simple momentum.\n                Concise Knowledge: If price volatility increases without a proportional increase in volume-weighted consistency, the trend is likely retail-driven and prone to reversal; when high volatility is backed by concentrated volume, it indicates institutional conviction and trend persistence.\n                concise Specification: Calculate the ratio of the daily (High - Low) to the 20-day standard deviation of daily returns, further adjusted by the 20-day average volume to isolate the 'Price-Volume Discrepancy' factor.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "9c3c340d2a7d",
      "parent_trajectory_ids": [
        "6d0e410a5ae0"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.000609669824652,
        "ICIR": 0.0046825494491509,
        "RankIC": 0.0135546604558545,
        "RankICIR": 0.1034714332005731,
        "annualized_return": 0.0144740797813091,
        "information_ratio": 0.1993927350352652,
        "max_drawdown": -0.1577713022552702
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:04:07.457689",
      "updated_at": "2026-01-16T01:04:07.457696"
    },
    "e2b07c20fc8f1d23": {
      "factor_id": "e2b07c20fc8f1d23",
      "factor_name": "Volatility_Entropy_Efficiency_10D",
      "factor_expression": "ZSCORE(($high - $low) / ($close * TS_STD($return, 10) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"ZSCORE(($high - $low) / ($close * TS_STD($return, 10) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Volatility_Entropy_Efficiency_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures the 'texture' of price discovery by comparing the current price range to the historical volatility of returns. A high value suggests that the current daily range is abnormally large compared to the recent return volatility, potentially indicating a lack of institutional conviction.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Intraday Volatility-Volume Entropy' factor, defined as the ratio of high-low price range to the standard deviation of volume-weighted price changes over a rolling window, predicts next-day returns by identifying periods where price movements are unsupported by consistent volume distribution.\n                Concise Observation: The parent strategy focused on overnight gaps and 5-day price efficiency (low-frequency), but failed to capture the intraday 'texture' of price discovery which often leads to higher Information Ratios by filtering out noise-driven price moves.\n                Concise Justification: By comparing the total daily range to the intraday variance of returns, we can distinguish between 'clean' trends (low entropy/high conviction) and 'noisy' mean-reverting fluctuations (high entropy/low conviction), providing a signal orthogonal to simple momentum.\n                Concise Knowledge: If price volatility increases without a proportional increase in volume-weighted consistency, the trend is likely retail-driven and prone to reversal; when high volatility is backed by concentrated volume, it indicates institutional conviction and trend persistence.\n                concise Specification: Calculate the ratio of the daily (High - Low) to the 20-day standard deviation of daily returns, further adjusted by the 20-day average volume to isolate the 'Price-Volume Discrepancy' factor.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "9c3c340d2a7d",
      "parent_trajectory_ids": [
        "6d0e410a5ae0"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.000609669824652,
        "ICIR": 0.0046825494491509,
        "RankIC": 0.0135546604558545,
        "RankICIR": 0.1034714332005731,
        "annualized_return": 0.0144740797813091,
        "information_ratio": 0.1993927350352652,
        "max_drawdown": -0.1577713022552702
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:04:07.471209",
      "updated_at": "2026-01-16T01:04:07.471215"
    },
    "15c711c95540a7f0": {
      "factor_id": "15c711c95540a7f0",
      "factor_name": "Volume_Weighted_Range_Consistency_20D",
      "factor_expression": "RANK(($high - $low) / (TS_STD($return, 20) + 1e-8)) * RANK(TS_PCTCHANGE($volume, 5))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($high - $low) / (TS_STD($return, 20) + 1e-8)) * RANK(TS_PCTCHANGE($volume, 5))\" # Your output factor expression will be filled in here\n    name = \"Volume_Weighted_Range_Consistency_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor evaluates the conviction of price moves by normalizing the price range with volume-weighted volatility. It uses the ratio of the daily range to the 20-day standard deviation of returns, smoothed and cross-sectionally ranked against volume trends.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Intraday Volatility-Volume Entropy' factor, defined as the ratio of high-low price range to the standard deviation of volume-weighted price changes over a rolling window, predicts next-day returns by identifying periods where price movements are unsupported by consistent volume distribution.\n                Concise Observation: The parent strategy focused on overnight gaps and 5-day price efficiency (low-frequency), but failed to capture the intraday 'texture' of price discovery which often leads to higher Information Ratios by filtering out noise-driven price moves.\n                Concise Justification: By comparing the total daily range to the intraday variance of returns, we can distinguish between 'clean' trends (low entropy/high conviction) and 'noisy' mean-reverting fluctuations (high entropy/low conviction), providing a signal orthogonal to simple momentum.\n                Concise Knowledge: If price volatility increases without a proportional increase in volume-weighted consistency, the trend is likely retail-driven and prone to reversal; when high volatility is backed by concentrated volume, it indicates institutional conviction and trend persistence.\n                concise Specification: Calculate the ratio of the daily (High - Low) to the 20-day standard deviation of daily returns, further adjusted by the 20-day average volume to isolate the 'Price-Volume Discrepancy' factor.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "9c3c340d2a7d",
      "parent_trajectory_ids": [
        "6d0e410a5ae0"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.000609669824652,
        "ICIR": 0.0046825494491509,
        "RankIC": 0.0135546604558545,
        "RankICIR": 0.1034714332005731,
        "annualized_return": 0.0144740797813091,
        "information_ratio": 0.1993927350352652,
        "max_drawdown": -0.1577713022552702
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:04:07.488048",
      "updated_at": "2026-01-16T01:04:07.488054"
    },
    "119abc8f14b2ae9a": {
      "factor_id": "119abc8f14b2ae9a",
      "factor_name": "Overnight_Sentiment_Correction_Ratio",
      "factor_expression": "($open - DELAY($close, 1)) / (DELAY($high, 1) - DELAY($low, 1) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"($open - DELAY($close, 1)) / (DELAY($high, 1) - DELAY($low, 1) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Overnight_Sentiment_Correction_Ratio\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures the ratio of the overnight opening gap to the previous day's trading range. It hypothesizes that large gaps relative to the prior day's price movement represent overextended sentiment that is likely to mean-revert. A higher ratio suggests a stronger potential for reversal.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Overnight Sentiment Correction (OSC) factor, defined as the ratio of the opening price gap to the prior day's price range, inversely predicts the subsequent return when the gap is large relative to historical volatility, suggesting that overnight price jumps often overextend due to retail sentiment and face mean-reversion.\n                Concise Observation: The parent strategy LVIQ focused on 20-day trend sustainability, but ignored the high-frequency 'jump' component of returns; market data shows that large overnight gaps without significant volume support often reverse during the first few hours of trading.\n                Concise Justification: Overnight gaps represent information processing during non-trading hours; when these gaps are extreme, they are frequently driven by noise traders or liquidity imbalances at the open, leading to a predictable correction as informed traders provide counter-party liquidity.\n                Concise Knowledge: If a stock opens with a significant gap relative to its typical daily volatility (ATR), the opening price often acts as a liquidity-driven peak or trough that mean-reverts as intraday liquidity stabilizes.\n                concise Specification: The factor is calculated as (Open - Close_prev) / (High_prev - Low_prev + 1e-8), specifically targeting the mean-reversion of the 'Gap' component; the expected relationship is a negative correlation with the next 1-day return, particularly when the denominator (prior range) is small.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "1ef40dd69cb4",
      "parent_trajectory_ids": [
        "6f94ea7c23f5"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0041677708422881,
        "ICIR": 0.0311286374302807,
        "RankIC": 0.0195463200908066,
        "RankICIR": 0.1476171723746717,
        "annualized_return": 0.062027010004924,
        "information_ratio": 0.994845546660707,
        "max_drawdown": -0.0850871512944597
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:06:36.274077",
      "updated_at": "2026-01-16T01:06:36.274083"
    },
    "70dd3590fb78a186": {
      "factor_id": "70dd3590fb78a186",
      "factor_name": "Volatility_Normalized_Gap_Reversion",
      "factor_expression": "RANK(($open - DELAY($close, 1)) / (TS_MEAN($high - $low, 20) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($open - DELAY($close, 1)) / (TS_MEAN($high - $low, 20) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Volatility_Normalized_Gap_Reversion\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor normalizes the overnight price gap by the 20-day average true range (approximated by price range) to identify extreme sentiment-driven jumps. By applying RANK, it identifies stocks with the most significant relative gaps which are expected to face correction from noise-trading levels.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Overnight Sentiment Correction (OSC) factor, defined as the ratio of the opening price gap to the prior day's price range, inversely predicts the subsequent return when the gap is large relative to historical volatility, suggesting that overnight price jumps often overextend due to retail sentiment and face mean-reversion.\n                Concise Observation: The parent strategy LVIQ focused on 20-day trend sustainability, but ignored the high-frequency 'jump' component of returns; market data shows that large overnight gaps without significant volume support often reverse during the first few hours of trading.\n                Concise Justification: Overnight gaps represent information processing during non-trading hours; when these gaps are extreme, they are frequently driven by noise traders or liquidity imbalances at the open, leading to a predictable correction as informed traders provide counter-party liquidity.\n                Concise Knowledge: If a stock opens with a significant gap relative to its typical daily volatility (ATR), the opening price often acts as a liquidity-driven peak or trough that mean-reverts as intraday liquidity stabilizes.\n                concise Specification: The factor is calculated as (Open - Close_prev) / (High_prev - Low_prev + 1e-8), specifically targeting the mean-reversion of the 'Gap' component; the expected relationship is a negative correlation with the next 1-day return, particularly when the denominator (prior range) is small.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "1ef40dd69cb4",
      "parent_trajectory_ids": [
        "6f94ea7c23f5"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0041677708422881,
        "ICIR": 0.0311286374302807,
        "RankIC": 0.0195463200908066,
        "RankICIR": 0.1476171723746717,
        "annualized_return": 0.062027010004924,
        "information_ratio": 0.994845546660707,
        "max_drawdown": -0.0850871512944597
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:06:36.286040",
      "updated_at": "2026-01-16T01:06:36.286046"
    },
    "4b8ec71910973b56": {
      "factor_id": "4b8ec71910973b56",
      "factor_name": "Gap_Magnitude_to_Intraday_Volatility",
      "factor_expression": "(($open / DELAY($close, 1)) - 1) / (TS_STD($return, 10) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"(($open / DELAY($close, 1)) - 1) / (TS_STD($return, 10) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Gap_Magnitude_to_Intraday_Volatility\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor measures the overnight gap relative to the standard deviation of daily returns over the last 10 days. It targets the hypothesis that gaps exceeding typical volatility levels are driven by liquidity imbalances and retail sentiment, leading to intraday mean-reversion.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Overnight Sentiment Correction (OSC) factor, defined as the ratio of the opening price gap to the prior day's price range, inversely predicts the subsequent return when the gap is large relative to historical volatility, suggesting that overnight price jumps often overextend due to retail sentiment and face mean-reversion.\n                Concise Observation: The parent strategy LVIQ focused on 20-day trend sustainability, but ignored the high-frequency 'jump' component of returns; market data shows that large overnight gaps without significant volume support often reverse during the first few hours of trading.\n                Concise Justification: Overnight gaps represent information processing during non-trading hours; when these gaps are extreme, they are frequently driven by noise traders or liquidity imbalances at the open, leading to a predictable correction as informed traders provide counter-party liquidity.\n                Concise Knowledge: If a stock opens with a significant gap relative to its typical daily volatility (ATR), the opening price often acts as a liquidity-driven peak or trough that mean-reverts as intraday liquidity stabilizes.\n                concise Specification: The factor is calculated as (Open - Close_prev) / (High_prev - Low_prev + 1e-8), specifically targeting the mean-reversion of the 'Gap' component; the expected relationship is a negative correlation with the next 1-day return, particularly when the denominator (prior range) is small.\n                ",
      "initial_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "user_initial_direction": "动量",
      "planning_direction": "Intraday-to-Overnight Momentum Spillover: Analyze the predictive power of first-hour trading volume and price direction on the subsequent overnight return gap, testing the hypothesis that institutional execution patterns create a lead-lag effect between intraday strength and next-day opening premiums.",
      "evolution_phase": "mutation",
      "trajectory_id": "1ef40dd69cb4",
      "parent_trajectory_ids": [
        "6f94ea7c23f5"
      ],
      "is_sota": true,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0041677708422881,
        "ICIR": 0.0311286374302807,
        "RankIC": 0.0195463200908066,
        "RankICIR": 0.1476171723746717,
        "annualized_return": 0.062027010004924,
        "information_ratio": 0.994845546660707,
        "max_drawdown": -0.0850871512944597
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:06:36.297864",
      "updated_at": "2026-01-16T01:06:36.297870"
    },
    "6ca75400955c3731": {
      "factor_id": "6ca75400955c3731",
      "factor_name": "Vol_Calibrated_Gap_Efficiency_Reversion",
      "factor_expression": "(($open - DELAY($close, 1)) / (TS_MEAN($high - $low, 20) + 1e-8)) * (1 - ABS(TS_SUM($return, 5)) / (TS_SUM(ABS($return), 5) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"(($open - DELAY($close, 1)) / (TS_MEAN($high - $low, 20) + 1e-8)) * (1 - ABS(TS_SUM($return, 5)) / (TS_SUM(ABS($return), 5) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Vol_Calibrated_Gap_Efficiency_Reversion\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor identifies mean-reversion opportunities by multiplying a volatility-normalized overnight gap with the inverse of the 5-day price efficiency ratio. A large gap relative to historical volatility (ATR) combined with low efficiency (high path dependency/noise) suggests sentiment exhaustion rather than a fundamental breakout.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Volatility-Calibrated Sentiment Resonance' factor, calculated as the interaction between a volatility-normalized overnight gap and the inverse of a 5-day price efficiency ratio, identifies mean-reversion opportunities by distinguishing between emotional overextensions and fundamental breakouts.\n                Concise Observation: Parent strategies show that overnight gaps provide predictive alpha, but their reliability is inconsistent without accounting for the structural quality of the price move and the prevailing volatility regime.\n                Concise Justification: By normalizing the opening gap by the 20-day average true range and weighting it against the efficiency ratio (absolute return divided by sum of absolute returns), we can filter out 'breakaway gaps' that have high fundamental momentum.\n                Concise Knowledge: If an overnight price gap is large relative to both the prior day's range and historical volatility, it likely represents sentiment exhaustion; when this occurs alongside low price efficiency (high path dependency), the probability of mean reversion increases.\n                concise Specification: The factor is defined as (Gap / ATR_20) * (1 - Efficiency_5), where Gap is (Open - Prev_Close), ATR_20 is the 20-day average of (High - Low), and Efficiency_5 is the absolute 5-day return divided by the 5-day sum of absolute daily returns.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "b7c10cfe7c21",
      "parent_trajectory_ids": [
        "0ae7e184a04d",
        "6d0e410a5ae0"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.002492439919,
        "ICIR": 0.0195251087771211,
        "RankIC": 0.0162425417667761,
        "RankICIR": 0.1288556964976664,
        "annualized_return": 0.0109725229065282,
        "information_ratio": 0.1863355731508588,
        "max_drawdown": -0.0855223690644722
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:10:03.326760",
      "updated_at": "2026-01-16T01:10:03.326768"
    },
    "6f3f7a1a11411f10": {
      "factor_id": "6f3f7a1a11411f10",
      "factor_name": "Sentiment_Exhaustion_Rank_Factor",
      "factor_expression": "RANK(($open - DELAY($close, 1)) / (TS_MAX($high, 20) - TS_MIN($low, 20) + 1e-8)) * (1 - ABS(TS_SUM($return, 5)) / (TS_SUM(ABS($return), 5) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(($open - DELAY($close, 1)) / (TS_MAX($high, 20) - TS_MIN($low, 20) + 1e-8)) * (1 - ABS(TS_SUM($return, 5)) / (TS_SUM(ABS($return), 5) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"Sentiment_Exhaustion_Rank_Factor\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A cross-sectional version of the volatility-calibrated sentiment resonance hypothesis. It ranks the overnight gap normalized by the 20-day price range and scales it by the 5-day inefficiency (1 - efficiency ratio). Higher values indicate potential overextensions prone to mean reversion.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Volatility-Calibrated Sentiment Resonance' factor, calculated as the interaction between a volatility-normalized overnight gap and the inverse of a 5-day price efficiency ratio, identifies mean-reversion opportunities by distinguishing between emotional overextensions and fundamental breakouts.\n                Concise Observation: Parent strategies show that overnight gaps provide predictive alpha, but their reliability is inconsistent without accounting for the structural quality of the price move and the prevailing volatility regime.\n                Concise Justification: By normalizing the opening gap by the 20-day average true range and weighting it against the efficiency ratio (absolute return divided by sum of absolute returns), we can filter out 'breakaway gaps' that have high fundamental momentum.\n                Concise Knowledge: If an overnight price gap is large relative to both the prior day's range and historical volatility, it likely represents sentiment exhaustion; when this occurs alongside low price efficiency (high path dependency), the probability of mean reversion increases.\n                concise Specification: The factor is defined as (Gap / ATR_20) * (1 - Efficiency_5), where Gap is (Open - Prev_Close), ATR_20 is the 20-day average of (High - Low), and Efficiency_5 is the absolute 5-day return divided by the 5-day sum of absolute daily returns.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "b7c10cfe7c21",
      "parent_trajectory_ids": [
        "0ae7e184a04d",
        "6d0e410a5ae0"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.002492439919,
        "ICIR": 0.0195251087771211,
        "RankIC": 0.0162425417667761,
        "RankICIR": 0.1288556964976664,
        "annualized_return": 0.0109725229065282,
        "information_ratio": 0.1863355731508588,
        "max_drawdown": -0.0855223690644722
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:10:03.338873",
      "updated_at": "2026-01-16T01:10:03.338880"
    },
    "e4744e88f2060724": {
      "factor_id": "e4744e88f2060724",
      "factor_name": "ZScore_Gap_Efficiency_Interaction",
      "factor_expression": "TS_ZSCORE($open - DELAY($close, 1), 20) * (1 - ABS(TS_SUM($return, 5)) / (TS_SUM(ABS($return), 5) + 1e-8))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_ZSCORE($open - DELAY($close, 1), 20) * (1 - ABS(TS_SUM($return, 5)) / (TS_SUM(ABS($return), 5) + 1e-8))\" # Your output factor expression will be filled in here\n    name = \"ZScore_Gap_Efficiency_Interaction\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor uses the time-series Z-score of the overnight gap to identify extreme sentiment shifts, weighted by the 5-day price inefficiency. By focusing on gaps that are statistically significant relative to the last 20 days, it filters out noise and highlights emotional overextensions.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Volatility-Calibrated Sentiment Resonance' factor, calculated as the interaction between a volatility-normalized overnight gap and the inverse of a 5-day price efficiency ratio, identifies mean-reversion opportunities by distinguishing between emotional overextensions and fundamental breakouts.\n                Concise Observation: Parent strategies show that overnight gaps provide predictive alpha, but their reliability is inconsistent without accounting for the structural quality of the price move and the prevailing volatility regime.\n                Concise Justification: By normalizing the opening gap by the 20-day average true range and weighting it against the efficiency ratio (absolute return divided by sum of absolute returns), we can filter out 'breakaway gaps' that have high fundamental momentum.\n                Concise Knowledge: If an overnight price gap is large relative to both the prior day's range and historical volatility, it likely represents sentiment exhaustion; when this occurs alongside low price efficiency (high path dependency), the probability of mean reversion increases.\n                concise Specification: The factor is defined as (Gap / ATR_20) * (1 - Efficiency_5), where Gap is (Open - Prev_Close), ATR_20 is the 20-day average of (High - Low), and Efficiency_5 is the absolute 5-day return divided by the 5-day sum of absolute daily returns.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "b7c10cfe7c21",
      "parent_trajectory_ids": [
        "0ae7e184a04d",
        "6d0e410a5ae0"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.002492439919,
        "ICIR": 0.0195251087771211,
        "RankIC": 0.0162425417667761,
        "RankICIR": 0.1288556964976664,
        "annualized_return": 0.0109725229065282,
        "information_ratio": 0.1863355731508588,
        "max_drawdown": -0.0855223690644722
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:10:03.350849",
      "updated_at": "2026-01-16T01:10:03.350855"
    },
    "b3ae7ab85c117330": {
      "factor_id": "b3ae7ab85c117330",
      "factor_name": "LCVE_Efficiency_Factor_20D",
      "factor_expression": "(TS_MEAN($return, 20) / (TS_STD($return, 20) + 1e-8)) / ((($high - $low) / ($close + 1e-8)) * TS_STD($return * $volume / (TS_MEAN($volume, 10) + 1e-8), 10) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"(TS_MEAN($return, 20) / (TS_STD($return, 20) + 1e-8)) / ((($high - $low) / ($close + 1e-8)) * TS_STD($return * $volume / (TS_MEAN($volume, 10) + 1e-8), 10) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"LCVE_Efficiency_Factor_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "The Liquidity-Calibrated Volatility Efficiency (LCVE) factor identifies high-quality price trends by scaling the 20-day idiosyncratic Sharpe ratio by a liquidity-adjusted volatility penalty. It penalizes stocks with high intraday ranges and high volume-weighted price dispersion, favoring 'efficient' trends likely driven by institutional accumulation.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Liquidity-Calibrated Volatility Efficiency (LCVE) factor, calculated as the 20-day idiosyncratic return divided by the product of the 10-day high-low price range and the 10-day standard deviation of volume-weighted price changes, identifies high-quality price trends supported by consistent volume distribution.\n                Concise Observation: Parent 1 showed that volume-weighted price entropy captures trend quality (RankIC 0.0136), while Parent 2 demonstrated that idiosyncratic Sharpe ratios normalized by liquidity provide robust signals (RankIC 0.0149).\n                Concise Justification: Combining idiosyncratic performance with a penalty for high intraday volatility and volume-weighted price variance ensures that only 'efficient' price movements—those achieving maximum return with minimum structural noise—are selected as predictive signals.\n                Concise Knowledge: If a high idiosyncratic return is accompanied by low intraday price dispersion and high volume-weighted price consistency, then the trend is likely driven by institutional accumulation rather than speculative noise; when liquidity efficiency is used to scale risk-adjusted returns, it filters out fragile price gaps.\n                concise Specification: The factor is defined as (Mean($return, 20) / Std($return, 20)) / (($high - $low) / $close * Std($return * $volume / Mean($volume, 10), 10)). A window of 20 days is used for the quality signal and 10 days for the volatility-volume efficiency calibration.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "fa509b6c3308",
      "parent_trajectory_ids": [
        "8a9b4e5d566f",
        "6f94ea7c23f5"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0036841285199749,
        "ICIR": 0.026715141302375,
        "RankIC": 0.0206924835741791,
        "RankICIR": 0.1512411704188865,
        "annualized_return": 0.0152771977636292,
        "information_ratio": 0.2410810541661188,
        "max_drawdown": -0.0881812858078658
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:13:08.369465",
      "updated_at": "2026-01-16T01:13:08.369472"
    },
    "7f000ce0e5df387e": {
      "factor_id": "7f000ce0e5df387e",
      "factor_name": "Simplified_LCVE_Rank_20D",
      "factor_expression": "RANK(TS_MEAN($return, 20) / (TS_STD($return, 20) + 1e-8)) - RANK((($high - $low) / ($close + 1e-8)) * TS_STD($return * $volume, 10))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(TS_MEAN($return, 20) / (TS_STD($return, 20) + 1e-8)) - RANK((($high - $low) / ($close + 1e-8)) * TS_STD($return * $volume, 10))\" # Your output factor expression will be filled in here\n    name = \"Simplified_LCVE_Rank_20D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "A cross-sectionally robust version of the LCVE hypothesis. It calculates the ratio of the 20-day return-based Sharpe ratio to the 10-day normalized price range, further adjusted by the volatility of volume-weighted returns. This version uses RANK to ensure the signal is comparable across different market regimes.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Liquidity-Calibrated Volatility Efficiency (LCVE) factor, calculated as the 20-day idiosyncratic return divided by the product of the 10-day high-low price range and the 10-day standard deviation of volume-weighted price changes, identifies high-quality price trends supported by consistent volume distribution.\n                Concise Observation: Parent 1 showed that volume-weighted price entropy captures trend quality (RankIC 0.0136), while Parent 2 demonstrated that idiosyncratic Sharpe ratios normalized by liquidity provide robust signals (RankIC 0.0149).\n                Concise Justification: Combining idiosyncratic performance with a penalty for high intraday volatility and volume-weighted price variance ensures that only 'efficient' price movements—those achieving maximum return with minimum structural noise—are selected as predictive signals.\n                Concise Knowledge: If a high idiosyncratic return is accompanied by low intraday price dispersion and high volume-weighted price consistency, then the trend is likely driven by institutional accumulation rather than speculative noise; when liquidity efficiency is used to scale risk-adjusted returns, it filters out fragile price gaps.\n                concise Specification: The factor is defined as (Mean($return, 20) / Std($return, 20)) / (($high - $low) / $close * Std($return * $volume / Mean($volume, 10), 10)). A window of 20 days is used for the quality signal and 10 days for the volatility-volume efficiency calibration.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "fa509b6c3308",
      "parent_trajectory_ids": [
        "8a9b4e5d566f",
        "6f94ea7c23f5"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0036841285199749,
        "ICIR": 0.026715141302375,
        "RankIC": 0.0206924835741791,
        "RankICIR": 0.1512411704188865,
        "annualized_return": 0.0152771977636292,
        "information_ratio": 0.2410810541661188,
        "max_drawdown": -0.0881812858078658
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:13:08.381729",
      "updated_at": "2026-01-16T01:13:08.381735"
    },
    "d17523869a94561a": {
      "factor_id": "d17523869a94561a",
      "factor_name": "Institutional_Accumulation_Efficiency_10D",
      "factor_expression": "TS_MEAN($return, 20) / (TS_STD(($high - $low) / ($close + 1e-8), 10) * TS_STD($volume / (TS_MEAN($volume, 10) + 1e-8), 10) + 1e-8)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_MEAN($return, 20) / (TS_STD(($high - $low) / ($close + 1e-8), 10) * TS_STD($volume / (TS_MEAN($volume, 10) + 1e-8), 10) + 1e-8)\" # Your output factor expression will be filled in here\n    name = \"Institutional_Accumulation_Efficiency_10D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Focuses on the 'Knowledge' aspect of the LCVE hypothesis: identifying trends with low structural noise. It measures the inverse of the product of price range and volume-weighted price variance, scaled by the 20-day return momentum, to isolate stocks with consistent, low-dispersion price appreciation.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The Liquidity-Calibrated Volatility Efficiency (LCVE) factor, calculated as the 20-day idiosyncratic return divided by the product of the 10-day high-low price range and the 10-day standard deviation of volume-weighted price changes, identifies high-quality price trends supported by consistent volume distribution.\n                Concise Observation: Parent 1 showed that volume-weighted price entropy captures trend quality (RankIC 0.0136), while Parent 2 demonstrated that idiosyncratic Sharpe ratios normalized by liquidity provide robust signals (RankIC 0.0149).\n                Concise Justification: Combining idiosyncratic performance with a penalty for high intraday volatility and volume-weighted price variance ensures that only 'efficient' price movements—those achieving maximum return with minimum structural noise—are selected as predictive signals.\n                Concise Knowledge: If a high idiosyncratic return is accompanied by low intraday price dispersion and high volume-weighted price consistency, then the trend is likely driven by institutional accumulation rather than speculative noise; when liquidity efficiency is used to scale risk-adjusted returns, it filters out fragile price gaps.\n                concise Specification: The factor is defined as (Mean($return, 20) / Std($return, 20)) / (($high - $low) / $close * Std($return * $volume / Mean($volume, 10), 10)). A window of 20 days is used for the quality signal and 10 days for the volatility-volume efficiency calibration.\n                ",
      "initial_direction": "",
      "user_initial_direction": "动量",
      "planning_direction": "",
      "evolution_phase": "crossover",
      "trajectory_id": "fa509b6c3308",
      "parent_trajectory_ids": [
        "8a9b4e5d566f",
        "6f94ea7c23f5"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0036841285199749,
        "ICIR": 0.026715141302375,
        "RankIC": 0.0206924835741791,
        "RankICIR": 0.1512411704188865,
        "annualized_return": 0.0152771977636292,
        "information_ratio": 0.2410810541661188,
        "max_drawdown": -0.0881812858078658
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:13:08.393837",
      "updated_at": "2026-01-16T01:13:08.393843"
    },
    "ced2799870bb06fe": {
      "factor_id": "ced2799870bb06fe",
      "factor_name": "Institutional_Tail_Concentration_5D",
      "factor_expression": "SIGN($close - $open) * ($volume / (TS_MEAN($volume, 5) + 1e-8)) * (($high + $low + $close) / (3 * $open))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"SIGN($close - $open) * ($volume / (TS_MEAN($volume, 5) + 1e-8)) * (($high + $low + $close) / (3 * $open))\" # Your output factor expression will be filled in here\n    name = \"Institutional_Tail_Concentration_5D\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "This factor captures institutional accumulation by measuring the interaction between intraday price trend and volume concentration. It uses the proxy of (High + Low + Close) / 3 vs Open to estimate intraday drift, multiplied by the daily volume, and compares it to its 5-day average to identify 'must-fill' institutional liquidity vacuums.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Late-Session Institutional Accumulation' factor, defined as the ratio of the final hour's volume concentration to the day's total volume weighted by the price trend's convexity, predicts multi-day trend continuation by capturing systematic liquidity vacuums created by institutional execution algorithms.\n                Concise Observation: While overnight gaps capture sentiment shocks, they often lack the volume-based confirmation of institutional conviction; stocks exhibiting high volume-price convexity in the afternoon session tend to sustain momentum better than those with random intraday spikes.\n                Concise Justification: Institutional investors often use VWAP or TWAP algorithms that accelerate toward the market close to minimize tracking error, creating a 'volume tail' that signals informed flow rather than retail noise.\n                Concise Knowledge: If trading volume disproportionately concentrates in the final hour of the session alongside a positive price drift, it indicates institutional 'must-fill' orders; such liquidity demand often persists into subsequent sessions due to execution risk management.\n                concise Specification: The factor is calculated as (Volume_Last_Hour / Volume_Total) multiplied by the sign of the intraday return, further scaled by a 5-day moving average of volume concentration to ensure the signal represents a significant deviation from baseline liquidity patterns.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "7ea21720184f",
      "parent_trajectory_ids": [
        "7d032ee6af69"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0058067924415689,
        "ICIR": 0.0414668950678036,
        "RankIC": 0.0237234419376635,
        "RankICIR": 0.1750361474482218,
        "annualized_return": 0.0330494309271768,
        "information_ratio": 0.4717549732555325,
        "max_drawdown": -0.1328564439790189
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:15:48.289533",
      "updated_at": "2026-01-16T01:15:48.289540"
    },
    "b2e2f5bcc185b88c": {
      "factor_id": "b2e2f5bcc185b88c",
      "factor_name": "Late_Session_Convexity_Proxy",
      "factor_expression": "TS_ZSCORE($return / ((ABS($high - $low) / $close) + 1e-8), 10)",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"TS_ZSCORE($return / ((ABS($high - $low) / $close) + 1e-8), 10)\" # Your output factor expression will be filled in here\n    name = \"Late_Session_Convexity_Proxy\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Captures the convexity of price movement relative to volume intensity. It measures the ratio of the daily return to the volume-weighted price range, normalized by a 10-day Z-score to identify abnormal institutional pressure toward the close.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Late-Session Institutional Accumulation' factor, defined as the ratio of the final hour's volume concentration to the day's total volume weighted by the price trend's convexity, predicts multi-day trend continuation by capturing systematic liquidity vacuums created by institutional execution algorithms.\n                Concise Observation: While overnight gaps capture sentiment shocks, they often lack the volume-based confirmation of institutional conviction; stocks exhibiting high volume-price convexity in the afternoon session tend to sustain momentum better than those with random intraday spikes.\n                Concise Justification: Institutional investors often use VWAP or TWAP algorithms that accelerate toward the market close to minimize tracking error, creating a 'volume tail' that signals informed flow rather than retail noise.\n                Concise Knowledge: If trading volume disproportionately concentrates in the final hour of the session alongside a positive price drift, it indicates institutional 'must-fill' orders; such liquidity demand often persists into subsequent sessions due to execution risk management.\n                concise Specification: The factor is calculated as (Volume_Last_Hour / Volume_Total) multiplied by the sign of the intraday return, further scaled by a 5-day moving average of volume concentration to ensure the signal represents a significant deviation from baseline liquidity patterns.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "7ea21720184f",
      "parent_trajectory_ids": [
        "7d032ee6af69"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0058067924415689,
        "ICIR": 0.0414668950678036,
        "RankIC": 0.0237234419376635,
        "RankICIR": 0.1750361474482218,
        "annualized_return": 0.0330494309271768,
        "information_ratio": 0.4717549732555325,
        "max_drawdown": -0.1328564439790189
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:15:48.302828",
      "updated_at": "2026-01-16T01:15:48.302834"
    },
    "ab3812708d980cee": {
      "factor_id": "ab3812708d980cee",
      "factor_name": "Volume_Momentum_Deviation_Rank",
      "factor_expression": "RANK(TS_RANK(DELTA($volume, 1), 5) * TS_RANK(DELTA($close, 1), 5))",
      "factor_implementation_code": "\nimport pandas as pd\nimport numpy as np\nimport os\nfrom alphaagent.components.coder.factor_coder.expr_parser import parse_expression, parse_symbol\nfrom alphaagent.components.coder.factor_coder.function_lib import *\n\n\ndef calculate_factor(expr: str, name: str):\n    # stock dataframe\n    df = pd.read_hdf('./daily_pv.h5', key='data')\n    \n    expr = parse_symbol(expr, df.columns)\n    expr = parse_expression(expr)\n\n    # replace '$var' by 'df['var'] to extract var's actual values\n    for col in df.columns:\n        expr = expr.replace(col[1:], f\"df[\\'{col}\\']\")\n\n    df[name] = eval(expr)\n    result = df[name].astype(np.float64)\n\n    if os.path.exists('result.h5'):\n        os.remove('result.h5')\n    result.to_hdf('result.h5', key='data')\n\nif __name__ == '__main__':\n    # Input factor expression. Do NOT use the variable format like \"df['$xxx']\" in factor expressions. Instead, you should use \"$xxx\". \n    expr = \"RANK(TS_RANK(DELTA($volume, 1), 5) * TS_RANK(DELTA($close, 1), 5))\" # Your output factor expression will be filled in here\n    name = \"Volume_Momentum_Deviation_Rank\" # Your output factor name will be filled in here\n    calculate_factor(expr, name)",
      "factor_description": "Identifies stocks where volume growth is coupled with positive price momentum, a hallmark of institutional execution algorithms. It ranks the product of volume change and price trend over a short window to isolate systematic accumulation.",
      "experiment_id": "unknown",
      "round_number": 1,
      "hypothesis": "Hypothesis: The 'Late-Session Institutional Accumulation' factor, defined as the ratio of the final hour's volume concentration to the day's total volume weighted by the price trend's convexity, predicts multi-day trend continuation by capturing systematic liquidity vacuums created by institutional execution algorithms.\n                Concise Observation: While overnight gaps capture sentiment shocks, they often lack the volume-based confirmation of institutional conviction; stocks exhibiting high volume-price convexity in the afternoon session tend to sustain momentum better than those with random intraday spikes.\n                Concise Justification: Institutional investors often use VWAP or TWAP algorithms that accelerate toward the market close to minimize tracking error, creating a 'volume tail' that signals informed flow rather than retail noise.\n                Concise Knowledge: If trading volume disproportionately concentrates in the final hour of the session alongside a positive price drift, it indicates institutional 'must-fill' orders; such liquidity demand often persists into subsequent sessions due to execution risk management.\n                concise Specification: The factor is calculated as (Volume_Last_Hour / Volume_Total) multiplied by the sign of the intraday return, further scaled by a 5-day moving average of volume concentration to ensure the signal represents a significant deviation from baseline liquidity patterns.\n                ",
      "initial_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "user_initial_direction": "动量",
      "planning_direction": "Residual Momentum with Volatility Scaling: Extract idiosyncratic returns by regressing stock returns against Fama-French factors, then normalize these residuals by their rolling idiosyncratic volatility to identify high-conviction trend persistence decoupled from market beta and sector rotations.",
      "evolution_phase": "mutation",
      "trajectory_id": "7ea21720184f",
      "parent_trajectory_ids": [
        "7d032ee6af69"
      ],
      "is_sota": false,
      "quality": "high_quality",
      "backtest_metrics": {
        "IC": 0.0058067924415689,
        "ICIR": 0.0414668950678036,
        "RankIC": 0.0237234419376635,
        "RankICIR": 0.1750361474482218,
        "annualized_return": 0.0330494309271768,
        "information_ratio": 0.4717549732555325,
        "max_drawdown": -0.1328564439790189
      },
      "max_correlation_with_alpha158": null,
      "added_at": "2026-01-16T01:15:48.315595",
      "updated_at": "2026-01-16T01:15:48.315601"
    }
  }
}