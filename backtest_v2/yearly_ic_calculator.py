#!/usr/bin/env python3
"""
å¹´åº¦å› å­ IC è®¡ç®—å·¥å…·

åŠŸèƒ½ï¼š
1. ä»å› å­åº“JSONæ–‡ä»¶è¯»å–å› å­åˆ—è¡¨
2. ä¼˜å…ˆçº§åŠ è½½å› å­å€¼ï¼š
   - cache_location.result_h5_pathï¼ˆå› å­åº“æŒ‡å®šçš„ç¼“å­˜ï¼‰
   - MD5 ç¼“å­˜ï¼ˆfactor_cache ç›®å½•ï¼‰
   - å®æ—¶è®¡ç®—ï¼ˆä½¿ç”¨ AlphaAgent è¡¨è¾¾å¼è§£æå™¨ï¼‰
3. è®¡ç®—æŒ‡å®šå¹´åº¦çš„ ICã€Rank ICã€IC IRã€Rank IC IR
4. è¾“å‡º CSV æ–‡ä»¶

ä½¿ç”¨æ–¹å¼:
    python backtest_v2/yearly_ic_calculator.py \
        --factor-json /path/to/factors.json \
        --year 2023 \
        --market csi300 \
        --output /path/to/output.csv
"""

import argparse
import hashlib
import json
import logging
import sys
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
from scipy.stats import spearmanr, pearsonr

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é»˜è®¤ç¼“å­˜ç›®å½•
DEFAULT_CACHE_DIR = Path("/mnt/DATA/quantagent/AlphaAgent/factor_cache")


class YearlyICCalculator:
    """å¹´åº¦ IC è®¡ç®—å™¨"""
    
    def __init__(self, 
                 market: str = "csi300", 
                 provider_uri: str = "/home/tjxy/.qlib/qlib_data/cn_data",
                 cache_dir: Optional[Path] = None):
        self.market = market
        self.provider_uri = provider_uri
        self.cache_dir = cache_dir or DEFAULT_CACHE_DIR
        self._qlib_initialized = False
        self._label_cache = {}  # ç¼“å­˜æ ‡ç­¾æ•°æ®
        self._data_df_cache = {}  # ç¼“å­˜è‚¡ç¥¨æ•°æ®
    
    def _init_qlib(self):
        """åˆå§‹åŒ– Qlib"""
        if self._qlib_initialized:
            return
        
        import qlib
        qlib.init(provider_uri=self.provider_uri, region="cn")
        self._qlib_initialized = True
        logger.info(f"âœ“ Qlib åˆå§‹åŒ–å®Œæˆ: {self.provider_uri}")
    
    def load_factor_library(self, json_path: str) -> Dict:
        """åŠ è½½å› å­åº“"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        factors = data.get('factors', {})
        logger.info(f"âœ“ åŠ è½½å› å­åº“: {len(factors)} ä¸ªå› å­")
        return factors
    
    def _get_cache_key(self, expr: str) -> str:
        """ç”Ÿæˆ MD5 ç¼“å­˜é”®"""
        return hashlib.md5(expr.encode()).hexdigest()
    
    def load_factor_from_cache_location(self, cache_location: Dict, year: int) -> Optional[pd.Series]:
        """
        ä» cache_location å­—æ®µæŒ‡å®šçš„è·¯å¾„åŠ è½½å› å­å€¼
        
        Args:
            cache_location: ç¼“å­˜ä½ç½®ä¿¡æ¯ï¼ŒåŒ…å« result_h5_path
            year: ç›®æ ‡å¹´ä»½
            
        Returns:
            è¿‡æ»¤åçš„å› å­å€¼ Series
        """
        if not cache_location:
            return None
        
        result_h5_path = cache_location.get('result_h5_path', '')
        if not result_h5_path or not Path(result_h5_path).exists():
            return None
        
        try:
            # è¯»å– HDF5 æ–‡ä»¶
            factor_df = pd.read_hdf(result_h5_path, key='data')
            return self._filter_factor_by_year(factor_df, year, result_h5_path)
        except Exception as e:
            logger.debug(f"ä» cache_location åŠ è½½å¤±è´¥ [{result_h5_path}]: {e}")
            return None
    
    def load_factor_from_md5_cache(self, factor_expr: str, year: int) -> Optional[pd.Series]:
        """
        ä» MD5 ç¼“å­˜åŠ è½½å› å­å€¼
        
        Args:
            factor_expr: å› å­è¡¨è¾¾å¼
            year: ç›®æ ‡å¹´ä»½
            
        Returns:
            è¿‡æ»¤åçš„å› å­å€¼ Series
        """
        cache_key = self._get_cache_key(factor_expr)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if not cache_file.exists():
            return None
        
        try:
            result = pd.read_pickle(cache_file)
            return self._filter_factor_by_year(result, year, str(cache_file))
        except Exception as e:
            logger.debug(f"ä» MD5 ç¼“å­˜åŠ è½½å¤±è´¥ [{cache_file}]: {e}")
            return None
    
    def _filter_factor_by_year(self, factor_data: Any, year: int, source: str) -> Optional[pd.Series]:
        """
        å°†å› å­æ•°æ®è¿‡æ»¤åˆ°æŒ‡å®šå¹´ä»½
        
        Args:
            factor_data: åŸå§‹å› å­æ•°æ® (DataFrame æˆ– Series)
            year: ç›®æ ‡å¹´ä»½
            source: æ•°æ®æ¥æºï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            è¿‡æ»¤åçš„å› å­å€¼ Series
        """
        try:
            # å¤„ç† DataFrame æ ¼å¼
            if isinstance(factor_data, pd.DataFrame):
                if len(factor_data.columns) == 1:
                    factor_series = factor_data.iloc[:, 0]
                elif 'factor' in factor_data.columns:
                    factor_series = factor_data['factor']
                else:
                    factor_series = factor_data.iloc[:, 0]
            else:
                factor_series = factor_data
            
            # è¿‡æ»¤åˆ°æŒ‡å®šå¹´ä»½
            start_date = f"{year}-01-01"
            end_date = f"{year}-12-31"
            
            if isinstance(factor_series.index, pd.MultiIndex):
                # MultiIndex: (instrument, datetime) æˆ– (datetime, instrument)
                idx_names = list(factor_series.index.names)
                
                # æ‰¾åˆ° datetime æ‰€åœ¨çš„ level
                datetime_level = None
                for i, name in enumerate(idx_names):
                    if name == 'datetime':
                        datetime_level = i
                        break
                    level_values = factor_series.index.get_level_values(i)
                    if pd.api.types.is_datetime64_any_dtype(level_values):
                        datetime_level = i
                        break
                
                if datetime_level is None:
                    datetime_level = 0
                
                dates = factor_series.index.get_level_values(datetime_level)
                mask = (dates >= pd.Timestamp(start_date)) & (dates <= pd.Timestamp(end_date))
                factor_series = factor_series[mask]
            
            if len(factor_series) == 0:
                return None
            
            return factor_series
            
        except Exception as e:
            logger.debug(f"è¿‡æ»¤å› å­æ•°æ®å¤±è´¥ [{source}]: {e}")
            return None
    
    def calculate_factor_realtime(self, factor_name: str, factor_expr: str, year: int) -> Optional[pd.Series]:
        """
        å®æ—¶è®¡ç®—å› å­å€¼
        
        Args:
            factor_name: å› å­åç§°
            factor_expr: å› å­è¡¨è¾¾å¼
            year: ç›®æ ‡å¹´ä»½
            
        Returns:
            è®¡ç®—å¾—åˆ°çš„å› å­å€¼ Series
        """
        try:
            # è·å–è‚¡ç¥¨æ•°æ®
            data_df = self._get_stock_data(year)
            if data_df is None or data_df.empty:
                return None
            
            # å¯¼å…¥è®¡ç®—å™¨
            import io
            import sys as _sys
            from joblib import parallel_backend
            from alphaagent.components.coder.factor_coder.expr_parser import (
                parse_expression, parse_symbol
            )
            import alphaagent.components.coder.factor_coder.function_lib as func_lib
            
            df = data_df.copy()
            
            # æ·»åŠ  $return åˆ— (å¦‚æœä¸å­˜åœ¨)
            if '$return' not in df.columns:
                df['$return'] = df.groupby('instrument')['$close'].transform(
                    lambda x: x / x.shift(1) - 1
                )
            
            # è§£æè¡¨è¾¾å¼
            expr = parse_symbol(factor_expr, df.columns)
            
            # é™é»˜è§£æ
            old_stdout = _sys.stdout
            _sys.stdout = io.StringIO()
            try:
                expr = parse_expression(expr)
            finally:
                _sys.stdout = old_stdout
            
            # æ›¿æ¢å˜é‡
            for col in df.columns:
                if col.startswith('$'):
                    expr = expr.replace(col[1:], f"df['{col}']")
            
            # æ„å»ºæ‰§è¡Œç¯å¢ƒ
            exec_globals = {'df': df, 'np': np, 'pd': pd}
            for name in dir(func_lib):
                if not name.startswith('_'):
                    obj = getattr(func_lib, name)
                    if callable(obj):
                        exec_globals[name] = obj
            
            # è®¡ç®—
            with parallel_backend('threading', n_jobs=1):
                result = eval(expr, exec_globals)
            
            if isinstance(result, pd.DataFrame):
                result = result.iloc[:, 0]
            
            if isinstance(result, pd.Series):
                result.name = factor_name
                return result.astype(np.float64)
            else:
                return pd.Series(result, index=df.index, name=factor_name).astype(np.float64)
            
        except Exception as e:
            logger.debug(f"å®æ—¶è®¡ç®—å› å­å¤±è´¥ [{factor_name}]: {str(e)[:100]}")
            return None
    
    def _get_stock_data(self, year: int) -> Optional[pd.DataFrame]:
        """è·å–æŒ‡å®šå¹´ä»½çš„è‚¡ç¥¨æ•°æ®"""
        if year in self._data_df_cache:
            return self._data_df_cache[year]
        
        self._init_qlib()
        from qlib.data import D
        
        # æ‰©å±•æ•°æ®èŒƒå›´ä»¥æ”¯æŒéœ€è¦å†å²æ•°æ®çš„å› å­
        start_date = f"{year-1}-01-01"  # å¤šåŠ è½½1å¹´å†å²æ•°æ®
        end_date = f"{year}-12-31"
        
        stock_list = D.instruments(self.market)
        
        fields = ['$open', '$high', '$low', '$close', '$volume', '$vwap']
        df = D.features(
            stock_list,
            fields,
            start_time=start_date,
            end_time=end_date,
            freq='day'
        )
        df.columns = fields
        
        self._data_df_cache[year] = df
        logger.info(f"  åŠ è½½{year}å¹´è‚¡ç¥¨æ•°æ®: {len(df)} è¡Œ")
        
        return df
    
    def get_label_data(self, year: int) -> pd.DataFrame:
        """
        è·å–æŒ‡å®šå¹´ä»½çš„æ ‡ç­¾æ•°æ®ï¼ˆæ”¶ç›Šç‡ï¼‰
        
        Returns:
            DataFrame with MultiIndex (instrument, datetime) and column 'label'
        """
        if year in self._label_cache:
            return self._label_cache[year]
        
        self._init_qlib()
        from qlib.data import D
        
        start_date = f"{year}-01-01"
        end_date = f"{year}-12-31"
        
        stock_list = D.instruments(self.market)
        
        # æ ‡ç­¾: T+2æ”¶ç›Šç‡
        label_expr = "Ref($close, -2) / Ref($close, -1) - 1"
        label_df = D.features(
            stock_list,
            [label_expr],
            start_time=start_date,
            end_time=end_date,
            freq='day'
        )
        label_df.columns = ['label']
        
        self._label_cache[year] = label_df
        logger.info(f"âœ“ åŠ è½½{year}å¹´æ ‡ç­¾æ•°æ®: {len(label_df)} è¡Œ")
        
        return label_df
    
    def load_factor_with_fallback(self, factor_info: Dict, year: int) -> Optional[pd.Series]:
        """
        åŠ è½½å› å­å€¼ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•å¤šç§æ¥æº
        
        ä¼˜å…ˆçº§:
        1. cache_location.result_h5_path
        2. MD5 ç¼“å­˜ (factor_cache ç›®å½•)
        3. å®æ—¶è®¡ç®—
        
        Returns:
            Tuple[factor_series, source_type]
        """
        factor_name = factor_info.get('factor_name', 'unknown')
        factor_expr = factor_info.get('factor_expression', '')
        cache_location = factor_info.get('cache_location')
        
        # 1. å°è¯•ä» cache_location åŠ è½½
        if cache_location:
            result = self.load_factor_from_cache_location(cache_location, year)
            if result is not None and len(result) > 0:
                return result, 'cache_location'
        
        # 2. å°è¯•ä» MD5 ç¼“å­˜åŠ è½½
        if factor_expr:
            result = self.load_factor_from_md5_cache(factor_expr, year)
            if result is not None and len(result) > 0:
                return result, 'md5_cache'
        
        # 3. å®æ—¶è®¡ç®—
        if factor_expr:
            result = self.calculate_factor_realtime(factor_name, factor_expr, year)
            if result is not None and len(result) > 0:
                # è¿‡æ»¤åˆ°ç›®æ ‡å¹´ä»½
                start_date = f"{year}-01-01"
                end_date = f"{year}-12-31"
                
                if isinstance(result.index, pd.MultiIndex):
                    idx_names = list(result.index.names)
                    datetime_level = None
                    for i, name in enumerate(idx_names):
                        if name == 'datetime':
                            datetime_level = i
                            break
                        level_values = result.index.get_level_values(i)
                        if pd.api.types.is_datetime64_any_dtype(level_values):
                            datetime_level = i
                            break
                    
                    if datetime_level is not None:
                        dates = result.index.get_level_values(datetime_level)
                        mask = (dates >= pd.Timestamp(start_date)) & (dates <= pd.Timestamp(end_date))
                        result = result[mask]
                
                if len(result) > 0:
                    return result, 'calculated'
        
        return None, None
    
    def calculate_factor_ic(self, factor_series: pd.Series, label_df: pd.DataFrame) -> Optional[Dict]:
        """
        è®¡ç®—å› å­çš„ IC æŒ‡æ ‡
        
        Returns:
            Dict with: annual_ic, annual_rank_ic, ic_ir, rank_ic_ir
        """
        try:
            # å¯¹é½å› å­å’Œæ ‡ç­¾çš„ç´¢å¼•
            if isinstance(factor_series.index, pd.MultiIndex):
                factor_idx_names = list(factor_series.index.names)
                label_idx_names = list(label_df.index.names)
                
                # å¦‚æœç´¢å¼•é¡ºåºä¸åŒï¼Œè°ƒæ•´å› å­çš„ç´¢å¼•é¡ºåº
                if factor_idx_names != label_idx_names:
                    if set(factor_idx_names) == set(label_idx_names):
                        factor_series = factor_series.swaplevel()
                        factor_series = factor_series.sort_index()
            
            # æ‰¾åˆ°å…±åŒç´¢å¼•
            common_idx = factor_series.index.intersection(label_df.index)
            
            if len(common_idx) < 100:
                logger.debug(f"å…±åŒç´¢å¼•è¿‡å°‘: {len(common_idx)}")
                return None
            
            factor_aligned = factor_series.loc[common_idx]
            label_aligned = label_df.loc[common_idx, 'label']
            
            # è·å–æ‰€æœ‰äº¤æ˜“æ—¥
            if isinstance(factor_aligned.index, pd.MultiIndex):
                idx_names = factor_aligned.index.names
                datetime_level = None
                for name in idx_names:
                    if name == 'datetime':
                        datetime_level = name
                        break
                    level_values = factor_aligned.index.get_level_values(name)
                    if pd.api.types.is_datetime64_any_dtype(level_values):
                        datetime_level = name
                        break
                
                if datetime_level is None:
                    datetime_level = idx_names[0]
                
                dates = factor_aligned.index.get_level_values(datetime_level).unique()
            else:
                dates = factor_aligned.index.unique()
            
            # è®¡ç®—æ¯æ—¥ IC
            daily_ics = []
            daily_rank_ics = []
            
            for date in dates:
                try:
                    # è·å–å½“æ—¥æ•°æ®
                    if isinstance(factor_aligned.index, pd.MultiIndex):
                        f_day = factor_aligned.xs(date, level=datetime_level)
                        l_day = label_aligned.xs(date, level=datetime_level)
                    else:
                        f_day = factor_aligned.loc[date]
                        l_day = label_aligned.loc[date]
                    
                    # å¯¹é½è‚¡ç¥¨
                    if isinstance(f_day, pd.Series) and isinstance(l_day, pd.Series):
                        common_stocks = f_day.index.intersection(l_day.index)
                        f_day = f_day.loc[common_stocks]
                        l_day = l_day.loc[common_stocks]
                    
                    # ç§»é™¤ NaN
                    mask = ~(pd.isna(f_day) | pd.isna(l_day))
                    f_day = f_day[mask]
                    l_day = l_day[mask]
                    
                    if len(f_day) >= 30:
                        # Pearson IC
                        ic, _ = pearsonr(f_day.values, l_day.values)
                        if not np.isnan(ic):
                            daily_ics.append(ic)
                        
                        # Spearman Rank IC
                        rank_ic, _ = spearmanr(f_day.values, l_day.values)
                        if not np.isnan(rank_ic):
                            daily_rank_ics.append(rank_ic)
                
                except Exception:
                    continue
            
            if len(daily_ics) < 20:
                return None
            
            # è®¡ç®—å¹´åº¦ç»Ÿè®¡é‡
            ic_mean = np.mean(daily_ics)
            ic_std = np.std(daily_ics)
            rank_ic_mean = np.mean(daily_rank_ics)
            rank_ic_std = np.std(daily_rank_ics)
            
            return {
                'annual_ic': ic_mean,
                'annual_rank_ic': rank_ic_mean,
                'ic_ir': ic_mean / ic_std if ic_std > 0 else 0,
                'rank_ic_ir': rank_ic_mean / rank_ic_std if rank_ic_std > 0 else 0,
                'n_days': len(daily_ics)
            }
            
        except Exception as e:
            logger.debug(f"ICè®¡ç®—é”™è¯¯: {e}")
            return None
    
    def calculate_all_factors(self, factors: Dict, year: int) -> pd.DataFrame:
        """
        è®¡ç®—æ‰€æœ‰å› å­çš„å¹´åº¦ IC
        
        Args:
            factors: å› å­å­—å…¸
            year: ç›®æ ‡å¹´ä»½
            
        Returns:
            DataFrame with columns: factor_name, annual_ic, annual_rank_ic, ic_ir, rank_ic_ir
        """
        # è·å–æ ‡ç­¾æ•°æ®
        label_df = self.get_label_data(year)
        
        results = []
        total = len(factors)
        success_count = 0
        cache_location_hit = 0
        md5_cache_hit = 0
        calculated_count = 0
        failed_count = 0
        
        for i, (factor_id, factor_info) in enumerate(factors.items()):
            factor_name = factor_info.get('factor_name', factor_id)
            
            # è¿›åº¦æ˜¾ç¤º
            if (i + 1) % 20 == 0 or i == 0:
                logger.info(f"  è¿›åº¦: {i+1}/{total}")
            
            # åŠ è½½å› å­å€¼ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰
            factor_series, source = self.load_factor_with_fallback(factor_info, year)
            
            if factor_series is None or len(factor_series) == 0:
                failed_count += 1
                logger.debug(f"  è·³è¿‡ {factor_name}: æ— æ³•è·å–å› å­å€¼")
                results.append({
                    'factor_name': factor_name,
                    'annual_ic': None,
                    'annual_rank_ic': None,
                    'ic_ir': None,
                    'rank_ic_ir': None
                })
                continue
            
            # ç»Ÿè®¡æ¥æº
            if source == 'cache_location':
                cache_location_hit += 1
            elif source == 'md5_cache':
                md5_cache_hit += 1
            elif source == 'calculated':
                calculated_count += 1
            
            # è®¡ç®— IC
            ic_result = self.calculate_factor_ic(factor_series, label_df)
            
            if ic_result:
                results.append({
                    'factor_name': factor_name,
                    'annual_ic': ic_result['annual_ic'],
                    'annual_rank_ic': ic_result['annual_rank_ic'],
                    'ic_ir': ic_result['ic_ir'],
                    'rank_ic_ir': ic_result['rank_ic_ir']
                })
                success_count += 1
            else:
                results.append({
                    'factor_name': factor_name,
                    'annual_ic': None,
                    'annual_rank_ic': None,
                    'ic_ir': None,
                    'rank_ic_ir': None
                })
        
        logger.info(f"\nâœ“ ICè®¡ç®—å®Œæˆ:")
        logger.info(f"  æˆåŠŸ: {success_count}/{total}")
        logger.info(f"  - cache_location å‘½ä¸­: {cache_location_hit}")
        logger.info(f"  - MD5 ç¼“å­˜å‘½ä¸­: {md5_cache_hit}")
        logger.info(f"  - å®æ—¶è®¡ç®—: {calculated_count}")
        logger.info(f"  - å¤±è´¥: {failed_count}")
        
        return pd.DataFrame(results)
    
    def run(self, factor_json: str, year: int, output_path: str) -> pd.DataFrame:
        """
        ä¸»è¿è¡Œæ–¹æ³•
        
        Args:
            factor_json: å› å­åº“ JSON æ–‡ä»¶è·¯å¾„
            year: ç›®æ ‡å¹´ä»½
            output_path: è¾“å‡º CSV æ–‡ä»¶è·¯å¾„
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š å¹´åº¦ IC è®¡ç®—å·¥å…·")
        logger.info(f"  å› å­åº“: {factor_json}")
        logger.info(f"  å¹´ä»½: {year}")
        logger.info(f"  å¸‚åœº: {self.market}")
        logger.info(f"  ç¼“å­˜ç›®å½•: {self.cache_dir}")
        logger.info(f"{'='*60}\n")
        
        # åŠ è½½å› å­åº“
        factors = self.load_factor_library(factor_json)
        
        # è®¡ç®— IC
        result_df = self.calculate_all_factors(factors, year)
        
        # æŒ‰ Rank IC é™åºæ’åº
        result_df = result_df.sort_values('annual_rank_ic', ascending=False, na_position='last')
        
        # ä¿å­˜ç»“æœ
        result_df.to_csv(output_path, index=False)
        logger.info(f"\nâœ“ ç»“æœå·²ä¿å­˜: {output_path}")
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        valid_df = result_df.dropna(subset=['annual_rank_ic'])
        
        if len(valid_df) > 0:
            print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
            print(f"  æœ‰æ•ˆå› å­æ•°: {len(valid_df)}/{len(result_df)}")
            print(f"  å¹³å‡ Rank IC: {valid_df['annual_rank_ic'].mean():.6f}")
            print(f"  æœ€å¤§ Rank IC: {valid_df['annual_rank_ic'].max():.6f}")
            print(f"  æœ€å° Rank IC: {valid_df['annual_rank_ic'].min():.6f}")
            print(f"  Rank IC > 0: {(valid_df['annual_rank_ic'] > 0).sum()} ä¸ª")
            print(f"  Rank IC < 0: {(valid_df['annual_rank_ic'] < 0).sum()} ä¸ª")
            
            print(f"\nğŸ“ˆ Top 10 å› å­:")
            for _, row in valid_df.head(10).iterrows():
                name = row['factor_name'][:45]
                ric = row['annual_rank_ic']
                print(f"  {name:<45} Rank IC: {ric:.6f}")
            
            print(f"\nğŸ“‰ Bottom 10 å› å­:")
            for _, row in valid_df.tail(10).iterrows():
                name = row['factor_name'][:45]
                ric = row['annual_rank_ic']
                print(f"  {name:<45} Rank IC: {ric:.6f}")
        
        return result_df


def main():
    parser = argparse.ArgumentParser(
        description='å¹´åº¦å› å­ IC è®¡ç®—å·¥å…·ï¼ˆæ”¯æŒå¤šç§ç¼“å­˜æ¥æº + å®æ—¶è®¡ç®—ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è®¡ç®— 2023 å¹´ CSI300 ä¸Šçš„ IC
  python yearly_ic_calculator.py \\
      --factor-json /path/to/factors.json \\
      --year 2023 \\
      --market csi300 \\
      --output /path/to/output.csv
        """
    )
    
    parser.add_argument(
        '-j', '--factor-json',
        type=str,
        required=True,
        help='å› å­åº“ JSON æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '-y', '--year',
        type=int,
        required=True,
        help='ç›®æ ‡å¹´ä»½ (e.g., 2023)'
    )
    
    parser.add_argument(
        '-m', '--market',
        type=str,
        default='csi300',
        help='è‚¡ç¥¨æ±  (é»˜è®¤: csi300)'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=str,
        default=None,
        help='è¾“å‡º CSV æ–‡ä»¶è·¯å¾„ (é»˜è®¤: å› å­åº“åŒç›®å½•ä¸‹çš„ {market}_{year}_ic_metrics.csv)'
    )
    
    parser.add_argument(
        '--provider-uri',
        type=str,
        default='/home/tjxy/.qlib/qlib_data/cn_data',
        help='Qlib æ•°æ®ç›®å½•'
    )
    
    parser.add_argument(
        '--cache-dir',
        type=str,
        default=str(DEFAULT_CACHE_DIR),
        help=f'MD5 ç¼“å­˜ç›®å½• (é»˜è®¤: {DEFAULT_CACHE_DIR})'
    )
    
    args = parser.parse_args()
    
    # é»˜è®¤è¾“å‡ºè·¯å¾„
    if args.output is None:
        factor_dir = Path(args.factor_json).parent
        args.output = str(factor_dir / f"{args.market}_{args.year}_ic_metrics.csv")
    
    # åˆ›å»ºè®¡ç®—å™¨å¹¶è¿è¡Œ
    calculator = YearlyICCalculator(
        market=args.market,
        provider_uri=args.provider_uri,
        cache_dir=Path(args.cache_dir)
    )
    
    calculator.run(
        factor_json=args.factor_json,
        year=args.year,
        output_path=args.output
    )


if __name__ == '__main__':
    main()
