#!/usr/bin/env python3
"""
è‡ªå®šä¹‰å› å­è®¡ç®—å™¨ - ç›´æ¥ä½¿ç”¨ AlphaAgent çš„è¡¨è¾¾å¼è§£æå™¨
æ”¯æŒæ‰€æœ‰å› å­æŒ–æ˜æ—¶ä½¿ç”¨çš„è¡¨è¾¾å¼è¯­æ³•

åŠŸèƒ½:
1. è§£æå› å­è¡¨è¾¾å¼ (ä½¿ç”¨ expr_parser)
2. è®¡ç®—å› å­å€¼ (ä½¿ç”¨ function_lib)
3. ç”Ÿæˆä¸ Qlib DataLoader å…¼å®¹çš„æ•°æ®æ ¼å¼
4. æ”¯æŒä»ç¼“å­˜åŠ è½½é¢„è®¡ç®—çš„å› å­å€¼
"""

import hashlib
import json
import logging
import os
import sys
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning, module='pandas')
warnings.filterwarnings('ignore', category=UserWarning, module='alphaagent')

# é…ç½® joblib ä½¿ç”¨çº¿ç¨‹åç«¯è€Œä¸æ˜¯è¿›ç¨‹åç«¯ï¼Œé¿å…å­è¿›ç¨‹å¯¼å…¥ LLM æ¨¡å—
os.environ.setdefault('JOBLIB_START_METHOD', 'loky')

logger = logging.getLogger(__name__)

# é»˜è®¤ç¼“å­˜ç›®å½•
DEFAULT_CACHE_DIR = Path("/mnt/DATA/quantagent/AlphaAgent/factor_cache")


class CustomFactorCalculator:
    """
    è‡ªå®šä¹‰å› å­è®¡ç®—å™¨
    ç›´æ¥ä½¿ç”¨ AlphaAgent çš„è¡¨è¾¾å¼è§£æå™¨å’Œå‡½æ•°åº“
    æ”¯æŒä»ç¼“å­˜åŠ è½½é¢„è®¡ç®—çš„å› å­å€¼
    æ”¯æŒè‡ªåŠ¨ä»ä¸»ç¨‹åºæ—¥å¿—ä¸­æå–ç¼“å­˜
    """
    
    def __init__(self, data_df: pd.DataFrame, cache_dir: Optional[Path] = None, auto_extract_cache: bool = True):
        """
        åˆå§‹åŒ–å› å­è®¡ç®—å™¨
        
        Args:
            data_df: è‚¡ç¥¨æ•°æ® DataFrameï¼Œéœ€è¦æœ‰ MultiIndex (datetime, instrument)
                    åˆ—åŒ…å«: $open, $high, $low, $close, $volume, $vwap
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ (å¯é€‰)
            auto_extract_cache: æ˜¯å¦è‡ªåŠ¨ä»ä¸»ç¨‹åºæ—¥å¿—ä¸­æå–ç¼“å­˜ (é»˜è®¤ True)
        """
        self.data_df = data_df
        self.cache_dir = cache_dir or DEFAULT_CACHE_DIR
        self.auto_extract_cache = auto_extract_cache
        self._cache_extracted = False  # æ ‡è®°æ˜¯å¦å·²æ‰§è¡Œè¿‡è‡ªåŠ¨æå–
        self._prepare_data()
        
    def _prepare_data(self):
        """å‡†å¤‡æ•°æ®ï¼Œæ·»åŠ å¸¸ç”¨è¡ç”Ÿåˆ—"""
        df = self.data_df.copy()
        
        # æ·»åŠ  $return åˆ— (å¦‚æœä¸å­˜åœ¨)
        if '$return' not in df.columns:
            df['$return'] = df.groupby('instrument')['$close'].transform(
                lambda x: x / x.shift(1) - 1
            )
        
        self.data_df = df
        logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ: {len(df)} è¡Œ, åˆ—: {list(df.columns)}")
    
    def _get_cache_key(self, expr: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”® (ä½¿ç”¨è¡¨è¾¾å¼çš„ MD5 å“ˆå¸Œ)"""
        return hashlib.md5(expr.encode()).hexdigest()
    
    def _load_from_cache(self, expr: str) -> Optional[pd.Series]:
        """
        ä»ç¼“å­˜åŠ è½½å› å­å€¼
        
        Args:
            expr: å› å­è¡¨è¾¾å¼
            
        Returns:
            Optional[pd.Series]: ç¼“å­˜çš„å› å­å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        cache_key = self._get_cache_key(expr)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            try:
                result = pd.read_pickle(cache_file)
                # å¤„ç†å¯èƒ½çš„ DataFrame æ ¼å¼ (ä¸»ç¨‹åºä¿å­˜çš„æ˜¯ DataFrame)
                if isinstance(result, pd.DataFrame):
                    if len(result.columns) == 1:
                        result = result.iloc[:, 0]
                    elif 'factor' in result.columns:
                        result = result['factor']
                    else:
                        # å–ç¬¬ä¸€åˆ—
                        result = result.iloc[:, 0]
                
                # å¤„ç†ç´¢å¼•é¡ºåºä¸ä¸€è‡´çš„é—®é¢˜
                # ç¼“å­˜å¯èƒ½æ˜¯ (datetime, instrument)ï¼Œè€Œå›æµ‹æ•°æ®æ˜¯ (instrument, datetime)
                if isinstance(result.index, pd.MultiIndex):
                    cache_idx_names = list(result.index.names)
                    data_idx_names = list(self.data_df.index.names)
                    
                    # å¦‚æœç´¢å¼•åç§°é¡ºåºä¸åŒï¼Œè°ƒæ•´é¡ºåº
                    if cache_idx_names != data_idx_names and set(cache_idx_names) == set(data_idx_names):
                        # äº¤æ¢ç´¢å¼•çº§åˆ«ä»¥åŒ¹é…ç›®æ ‡æ•°æ®
                        result = result.swaplevel()
                        result = result.sort_index()
                
                return result
            except Exception as e:
                logger.debug(f"ç¼“å­˜åŠ è½½å¤±è´¥ [{cache_key}]: {e}")
                return None
        return None
    
    def _save_to_cache(self, expr: str, result: pd.Series):
        """
        ä¿å­˜å› å­å€¼åˆ°ç¼“å­˜
        
        Args:
            expr: å› å­è¡¨è¾¾å¼
            result: è®¡ç®—çš„å› å­å€¼
        """
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            cache_key = self._get_cache_key(expr)
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            result.to_pickle(cache_file)
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _auto_extract_cache_from_logs(self):
        """
        è‡ªåŠ¨ä»ä¸»ç¨‹åºæ—¥å¿—ä¸­æå–ç¼“å­˜
        åªåœ¨é¦–æ¬¡éœ€è¦æ—¶æ‰§è¡Œä¸€æ¬¡
        """
        if self._cache_extracted:
            return
        
        self._cache_extracted = True
        
        try:
            # åŠ¨æ€å¯¼å…¥ç¼“å­˜æå–å™¨
            from tools.factor_cache_extractor import extract_factors_to_cache
            
            logger.info("ğŸ”„ è‡ªåŠ¨æå–ä¸»ç¨‹åºç¼“å­˜...")
            new_count = extract_factors_to_cache(
                output_dir=self.cache_dir,
                verbose=False
            )
            if new_count > 0:
                logger.info(f"   âœ“ æ–°æå– {new_count} ä¸ªå› å­åˆ°ç¼“å­˜")
        except ImportError:
            logger.debug("ç¼“å­˜æå–å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨æå–")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨æå–ç¼“å­˜å¤±è´¥: {e}")
        
    def calculate_factor(self, factor_name: str, factor_expression: str) -> Optional[pd.Series]:
        """
        è®¡ç®—å•ä¸ªå› å­
        
        Args:
            factor_name: å› å­åç§°
            factor_expression: å› å­è¡¨è¾¾å¼
            
        Returns:
            pd.Series: å› å­å€¼ (MultiIndex: datetime, instrument)
        """
        try:
            # å¯¼å…¥è¡¨è¾¾å¼è§£æå™¨ï¼ˆé™é»˜å¯¼å…¥ï¼Œé¿å…ä¸å¿…è¦çš„æ—¥å¿—ï¼‰
            import io
            import sys as _sys
            from contextlib import redirect_stdout
            
            # é…ç½® joblib ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼ï¼Œé¿å…å­è¿›ç¨‹å¯¼å…¥é—®é¢˜
            from joblib import parallel_backend
            
            from alphaagent.components.coder.factor_coder.expr_parser import (
                parse_expression, parse_symbol
            )
            # å¯¼å…¥å‡½æ•°åº“
            import alphaagent.components.coder.factor_coder.function_lib as func_lib
            
            # å¤åˆ¶æ•°æ®
            df = self.data_df.copy()
            
            # è§£æè¡¨è¾¾å¼ï¼ˆæŠ‘åˆ¶ parse_expression çš„æ‰“å°è¾“å‡ºï¼‰
            expr = parse_symbol(factor_expression, df.columns)
            
            # é™é»˜è§£æï¼ˆæŠ‘åˆ¶ print è¾“å‡ºï¼‰
            old_stdout = _sys.stdout
            _sys.stdout = io.StringIO()
            try:
                expr = parse_expression(expr)
            finally:
                _sys.stdout = old_stdout
            
            # æ›¿æ¢å˜é‡ä¸º DataFrame åˆ—å¼•ç”¨
            for col in df.columns:
                if col.startswith('$'):
                    expr = expr.replace(col[1:], f"df['{col}']")
            
            # æ„å»ºæ‰§è¡Œç¯å¢ƒ
            exec_globals = {
                'df': df,
                'np': np,
                'pd': pd,
            }
            
            # æ·»åŠ æ‰€æœ‰å‡½æ•°åº“ä¸­çš„å‡½æ•°
            for name in dir(func_lib):
                if not name.startswith('_'):
                    obj = getattr(func_lib, name)
                    if callable(obj):
                        exec_globals[name] = obj
            
            # ä½¿ç”¨çº¿ç¨‹åç«¯è¿›è¡Œè®¡ç®—ï¼Œé¿å…å­è¿›ç¨‹å¯¼å…¥ LLM æ¨¡å—
            with parallel_backend('threading', n_jobs=1):
                # è®¡ç®—å› å­å€¼
                result = eval(expr, exec_globals)
            
            if isinstance(result, pd.DataFrame):
                result = result.iloc[:, 0]
            
            if isinstance(result, pd.Series):
                result.name = factor_name
                # ç¡®ä¿ç»“æœä¸åŸå§‹æ•°æ®æœ‰ç›¸åŒçš„ç´¢å¼•
                if not result.index.equals(df.index):
                    result = result.reindex(df.index)
                return result.astype(np.float64)
            else:
                # å¦‚æœç»“æœæ˜¯æ ‡é‡æˆ–æ•°ç»„ï¼Œè½¬æ¢ä¸º Series
                return pd.Series(result, index=df.index, name=factor_name).astype(np.float64)
                
        except Exception as e:
            logger.warning(f"å› å­è®¡ç®—å¤±è´¥ [{factor_name}]: {str(e)[:200]}")
            return None
    
    def calculate_factors_from_json(self, json_path: str, 
                                   max_factors: Optional[int] = None) -> pd.DataFrame:
        """
        ä» JSON æ–‡ä»¶æ‰¹é‡è®¡ç®—å› å­
        
        Args:
            json_path: å› å­ JSON æ–‡ä»¶è·¯å¾„
            max_factors: æœ€å¤§å› å­æ•°é‡é™åˆ¶
            
        Returns:
            pd.DataFrame: è®¡ç®—å¾—åˆ°çš„å› å­å€¼ DataFrame
        """
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        factors = data.get('factors', {})
        
        results = {}
        success_count = 0
        fail_count = 0
        
        factor_items = list(factors.items())
        if max_factors:
            factor_items = factor_items[:max_factors]
        
        total = len(factor_items)
        logger.info(f"å¼€å§‹è®¡ç®— {total} ä¸ªå› å­...")
        
        for i, (factor_id, factor_info) in enumerate(factor_items):
            factor_name = factor_info.get('factor_name', factor_id)
            factor_expr = factor_info.get('factor_expression', '')
            
            if not factor_expr:
                fail_count += 1
                continue
            
            if (i + 1) % 10 == 0 or i == 0:
                logger.info(f"  è¿›åº¦: {i+1}/{total}")
            
            result = self.calculate_factor(factor_name, factor_expr)
            
            if result is not None:
                results[factor_name] = result
                success_count += 1
            else:
                fail_count += 1
        
        logger.info(f"å› å­è®¡ç®—å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        
        if results:
            return pd.DataFrame(results)
        return pd.DataFrame()
    
    def calculate_factors_batch(self, factors: List[Dict], use_cache: bool = True) -> pd.DataFrame:
        """
        æ‰¹é‡è®¡ç®—å› å­
        
        Args:
            factors: å› å­åˆ—è¡¨ï¼Œæ¯ä¸ªå› å­æ˜¯ dictï¼ŒåŒ…å« factor_name å’Œ factor_expression
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (é»˜è®¤ True)
            
        Returns:
            pd.DataFrame: è®¡ç®—å¾—åˆ°çš„å› å­å€¼
        """
        # è‡ªåŠ¨ä»ä¸»ç¨‹åºæ—¥å¿—ä¸­æå–ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ä¸”å°šæœªæ‰§è¡Œï¼‰
        if use_cache and self.auto_extract_cache:
            self._auto_extract_cache_from_logs()
        
        results = {}
        success_count = 0
        fail_count = 0
        cache_hit_count = 0
        total = len(factors)
        
        for i, factor_info in enumerate(factors):
            factor_name = factor_info.get('factor_name', 'unknown')
            factor_expr = factor_info.get('factor_expression', '')
            
            if not factor_expr:
                fail_count += 1
                continue
            
            logger.info(f"  è®¡ç®—å› å­ [{i+1}/{total}]: {factor_name}")
            
            result = None
            
            # 1. ä¼˜å…ˆæ£€æŸ¥ç¼“å­˜
            if use_cache:
                result = self._load_from_cache(factor_expr)
                if result is not None:
                    cache_hit_count += 1
                    # ç¡®ä¿ç´¢å¼•å¯¹é½
                    if not result.index.equals(self.data_df.index):
                        try:
                            # å°è¯•å¯¹é½ç´¢å¼• - ç¼“å­˜å¯èƒ½åŒ…å«æ›´å¤šè‚¡ç¥¨/æ—¥æœŸ
                            common_idx = result.index.intersection(self.data_df.index)
                            if len(common_idx) > len(self.data_df.index) * 0.5:  # è‡³å°‘50%åŒ¹é…
                                result = result.reindex(self.data_df.index)
                                logger.debug(f"    ç´¢å¼•å¯¹é½: å…±åŒç´¢å¼• {len(common_idx)}, ç›®æ ‡ {len(self.data_df.index)}")
                            else:
                                logger.warning(f"    âš  ç¼“å­˜ç´¢å¼•åŒ¹é…ç‡è¿‡ä½ ({len(common_idx)}/{len(self.data_df.index)}), é‡æ–°è®¡ç®—")
                                result = None
                        except Exception as e:
                            logger.warning(f"    âš  ç´¢å¼•å¯¹é½å¤±è´¥: {e}, é‡æ–°è®¡ç®—")
                            result = None
                    
                    if result is not None and len(result) > 0 and not result.isna().all():
                        valid_count = (~result.isna()).sum()
                        results[factor_name] = result
                        success_count += 1
                        logger.info(f"    âœ“ ä»ç¼“å­˜åŠ è½½ (æœ‰æ•ˆæ•°æ®: {valid_count}/{len(result)})")
                        continue
            
            # 2. ç¼“å­˜æœªå‘½ä¸­ï¼Œè¿›è¡Œè®¡ç®—
            result = self.calculate_factor(factor_name, factor_expr)
            
            if result is not None and len(result) > 0:
                # ç¡®ä¿ç»“æœæ˜¯æœ‰æ•ˆçš„ Series
                if not result.isna().all():
                    results[factor_name] = result
                    success_count += 1
                    logger.info(f"    âœ“ è®¡ç®—æˆåŠŸ (æœ‰æ•ˆæ•°æ®: {(~result.isna()).sum()}/{len(result)})")
                    # ä¿å­˜åˆ°ç¼“å­˜
                    if use_cache:
                        self._save_to_cache(factor_expr, result)
                else:
                    fail_count += 1
                    logger.warning(f"    âœ— å› å­ {factor_name} å…¨ä¸º NaN")
            else:
                fail_count += 1
                logger.warning(f"    âœ— å› å­ {factor_name} è®¡ç®—å¤±è´¥æˆ–ä¸ºç©º")
        
        logger.info(f"  å› å­è®¡ç®—å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}, ç¼“å­˜å‘½ä¸­ {cache_hit_count}")
        
        if results:
            # åˆ›å»º DataFrameï¼Œä½¿ç”¨åŸå§‹æ•°æ®çš„ç´¢å¼•
            result_df = pd.DataFrame(results, index=self.data_df.index)
            
            # éªŒè¯ DataFrame
            logger.info(f"  ç»“æœ DataFrame: {result_df.shape}, ç´¢å¼•ç±»å‹: {type(result_df.index).__name__}")
            
            return result_df
        
        return pd.DataFrame()


class CustomFactorDataLoader:
    """
    è‡ªå®šä¹‰å› å­æ•°æ®åŠ è½½å™¨
    å°†è®¡ç®—å¥½çš„å› å­å€¼è½¬æ¢ä¸º Qlib å¯ç”¨çš„æ ¼å¼
    """
    
    def __init__(self, factor_df: pd.DataFrame, label_expr: str = "Ref($close, -2) / Ref($close, -1) - 1"):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            factor_df: å› å­å€¼ DataFrame (MultiIndex: datetime, instrument)
            label_expr: æ ‡ç­¾è¡¨è¾¾å¼
        """
        self.factor_df = factor_df
        self.label_expr = label_expr
        
    def to_qlib_format(self, data_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        è½¬æ¢ä¸º Qlib æ•°æ®æ ¼å¼
        
        Args:
            data_df: åŸå§‹ä»·æ ¼æ•°æ®
            
        Returns:
            Tuple[features_df, labels_df]
        """
        # è®¡ç®—æ ‡ç­¾
        from alphaagent.components.coder.factor_coder.expr_parser import (
            parse_expression, parse_symbol
        )
        import alphaagent.components.coder.factor_coder.function_lib as func_lib
        
        df = data_df.copy()
        
        # è§£ææ ‡ç­¾è¡¨è¾¾å¼
        expr = parse_symbol(self.label_expr, df.columns)
        expr = parse_expression(expr)
        
        for col in df.columns:
            if col.startswith('$'):
                expr = expr.replace(col[1:], f"df['{col}']")
        
        exec_globals = {'df': df, 'np': np, 'pd': pd}
        for name in dir(func_lib):
            if not name.startswith('_'):
                obj = getattr(func_lib, name)
                if callable(obj):
                    exec_globals[name] = obj
        
        label = eval(expr, exec_globals)
        if isinstance(label, pd.DataFrame):
            label = label.iloc[:, 0]
        
        labels_df = pd.DataFrame({'LABEL0': label})
        
        return self.factor_df, labels_df


def get_qlib_stock_data(config: Dict) -> pd.DataFrame:
    """
    ä» Qlib è·å–è‚¡ç¥¨æ•°æ®
    
    Args:
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å« data é…ç½®
        
    Returns:
        pd.DataFrame: è‚¡ç¥¨æ•°æ®
    """
    import qlib
    from qlib.data import D
    
    data_config = config.get('data', {})
    
    provider_uri = data_config.get('provider_uri', '/home/tjxy/.qlib/qlib_data/cn_data')
    
    # åˆå§‹åŒ– Qlib (å¦‚æœå°šæœªåˆå§‹åŒ–)
    try:
        qlib.init(provider_uri=provider_uri, region='cn')
    except Exception:
        pass  # å·²ç»åˆå§‹åŒ–
    
    start_time = data_config.get('start_time', '2016-01-01')
    end_time = data_config.get('end_time', '2025-12-31')
    market = data_config.get('market', 'csi300')
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    stock_list = D.instruments(market)
    
    # è·å–æ•°æ®
    fields = ['$open', '$high', '$low', '$close', '$volume', '$vwap']
    df = D.features(
        stock_list,
        fields,
        start_time=start_time,
        end_time=end_time,
        freq='day'
    )
    
    df.columns = fields
    
    logger.info(f"âœ“ åŠ è½½è‚¡ç¥¨æ•°æ®: {len(df)} è¡Œ")
    
    return df


if __name__ == '__main__':
    """æµ‹è¯•å› å­è®¡ç®—"""
    import yaml
    
    logging.basicConfig(level=logging.INFO)
    
    # åŠ è½½é…ç½®
    config_path = Path(__file__).parent / 'config.yaml'
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è·å–æ•°æ®
    print("è·å–è‚¡ç¥¨æ•°æ®...")
    data_df = get_qlib_stock_data(config)
    
    # åˆ›å»ºè®¡ç®—å™¨
    calculator = CustomFactorCalculator(data_df)
    
    # æµ‹è¯•å•ä¸ªå› å­
    test_expr = "RANK(-1 * TS_PCTCHANGE($close, 10))"
    print(f"\næµ‹è¯•è¡¨è¾¾å¼: {test_expr}")
    
    result = calculator.calculate_factor("test_factor", test_expr)
    if result is not None:
        print(f"è®¡ç®—æˆåŠŸ! ç»“æœå½¢çŠ¶: {result.shape}")
        print(result.head())
    else:
        print("è®¡ç®—å¤±è´¥!")

