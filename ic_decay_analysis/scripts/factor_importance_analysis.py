#!/usr/bin/env python3
"""
å› å­é‡è¦æ€§åˆ†æ

åŠŸèƒ½ï¼š
1. è®­ç»ƒ LightGBM æ¨¡å‹å¹¶æå–å› å­é‡è¦æ€§
2. åˆ†æä¸åŒå¹´ä»½å› å­æƒé‡çš„å˜åŒ–
3. è¯†åˆ«ä¸»å¯¼å› å­åŠå…¶åœ¨2023å¹´çš„è¡¨ç°
4. å¯¹æ¯” AA å’Œ QA å› å­åº“çš„é‡è¦æ€§åˆ†å¸ƒç‰¹å¾
"""

import json
import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FactorImportanceAnalyzer:
    """å› å­é‡è¦æ€§åˆ†æå™¨"""
    
    def __init__(self):
        self.factor_libraries = {}
        self.importance_results = {}
        self.qlib_initialized = False
    
    def _init_qlib(self):
        """åˆå§‹åŒ– Qlib"""
        if self.qlib_initialized:
            return
        
        import qlib
        qlib.init(provider_uri="/home/tjxy/.qlib/qlib_data/cn_data", region="cn")
        self.qlib_initialized = True
        logger.info("âœ“ Qlib åˆå§‹åŒ–å®Œæˆ")
    
    def load_factor_library(self, name: str, path: str):
        """åŠ è½½å› å­åº“"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        factors = data.get('factors', {})
        self.factor_libraries[name] = {
            'path': path,
            'factors': factors,
            'metadata': data.get('metadata', {})
        }
        
        logger.info(f"âœ“ åŠ è½½å› å­åº“ {name}: {len(factors)} ä¸ªå› å­")
    
    def _prepare_factor_data(self, lib_name: str, year: int) -> Optional[Tuple[pd.DataFrame, pd.Series]]:
        """
        å‡†å¤‡å› å­æ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒ
        
        è¿”å›: (ç‰¹å¾DataFrame, æ ‡ç­¾Series)
        """
        self._init_qlib()
        
        from qlib.data import D
        from backtest_v2.custom_factor_calculator import get_qlib_stock_data
        
        lib_data = self.factor_libraries[lib_name]
        factors = lib_data['factors']
        
        # è®­ç»ƒæ—¶é—´èŒƒå›´ï¼šä½¿ç”¨æµ‹è¯•å¹´ä»½å‰ä¸¤å¹´ä½œä¸ºè®­ç»ƒé›†
        train_start = f"{year-3}-01-01"
        train_end = f"{year-1}-12-31"
        
        print(f"  è®­ç»ƒæ•°æ®èŒƒå›´: {train_start} ~ {train_end}")
        
        try:
            # è·å–è‚¡ç¥¨åˆ—è¡¨
            stock_list = D.instruments("csi300")
            
            # æ”¶é›†æ‰€æœ‰å› å­æ•°æ®
            all_factor_dfs = []
            factor_names = []
            
            for factor_id, factor_info in factors.items():
                factor_name = factor_info.get('factor_name', factor_id)
                cache_loc = factor_info.get('cache_location')
                
                factor_df = None
                
                # å°è¯•ä»ç¼“å­˜åŠ è½½
                if cache_loc:
                    result_h5_path = cache_loc.get('result_h5_path')
                    if result_h5_path and Path(result_h5_path).exists():
                        try:
                            factor_df = pd.read_hdf(result_h5_path, key='data')
                            
                            # è¿‡æ»¤æ—¶é—´èŒƒå›´
                            if isinstance(factor_df.index, pd.MultiIndex):
                                dates = factor_df.index.get_level_values('datetime')
                            else:
                                dates = factor_df.index
                            
                            mask = (dates >= pd.Timestamp(train_start)) & (dates <= pd.Timestamp(train_end))
                            factor_df = factor_df[mask]
                            
                            if isinstance(factor_df, pd.Series):
                                factor_df = factor_df.to_frame(name=factor_name)
                            else:
                                factor_df.columns = [factor_name]
                            
                        except Exception as e:
                            logger.debug(f"  ç¼“å­˜åŠ è½½å¤±è´¥ {factor_name}: {e}")
                
                if factor_df is not None and len(factor_df) > 0:
                    all_factor_dfs.append(factor_df)
                    factor_names.append(factor_name)
            
            if len(all_factor_dfs) == 0:
                logger.warning(f"  æ²¡æœ‰å¯ç”¨çš„å› å­æ•°æ®")
                return None
            
            # åˆå¹¶æ‰€æœ‰å› å­
            features_df = pd.concat(all_factor_dfs, axis=1)
            features_df = features_df.loc[:, ~features_df.columns.duplicated()]
            
            print(f"  åŠ è½½ {len(features_df.columns)} ä¸ªå› å­, {len(features_df)} è¡Œæ•°æ®")
            
            # è·å–æ ‡ç­¾
            label_expr = "Ref($close, -2) / Ref($close, -1) - 1"
            label_df = D.features(
                stock_list,
                [label_expr],
                start_time=train_start,
                end_time=train_end,
                freq='day'
            )
            label_df.columns = ['label']
            
            # å¯¹é½æ•°æ®
            common_idx = features_df.index.intersection(label_df.index)
            features_df = features_df.loc[common_idx]
            label_series = label_df.loc[common_idx, 'label']
            
            # æ•°æ®é¢„å¤„ç†
            features_df = features_df.fillna(0)
            features_df = features_df.replace([np.inf, -np.inf], 0)
            
            # ç§»é™¤æ ‡ç­¾ä¸º NaN çš„è¡Œ
            valid_mask = ~label_series.isna()
            features_df = features_df[valid_mask]
            label_series = label_series[valid_mask]
            
            print(f"  é¢„å¤„ç†å: {len(features_df)} è¡Œæ•°æ®")
            
            return features_df, label_series
            
        except Exception as e:
            logger.error(f"  å‡†å¤‡æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def train_and_get_importance(self, lib_name: str, year: int) -> Optional[Dict]:
        """
        è®­ç»ƒ LightGBM æ¨¡å‹å¹¶è·å–å› å­é‡è¦æ€§
        """
        import lightgbm as lgb
        
        print(f"\nè®­ç»ƒ {lib_name} - {year} å¹´æ¨¡å‹...")
        
        data = self._prepare_factor_data(lib_name, year)
        if data is None:
            return None
        
        features_df, label_series = data
        
        # æ¨¡å‹å‚æ•°
        params = {
            'objective': 'regression',
            'metric': 'mse',
            'learning_rate': 0.1,
            'max_depth': 8,
            'num_leaves': 210,
            'colsample_bytree': 0.8,
            'subsample': 0.8,
            'lambda_l1': 200,
            'lambda_l2': 500,
            'min_child_samples': 100,
            'verbose': -1,
            'seed': 42
        }
        
        # åˆ›å»ºæ•°æ®é›†
        train_data = lgb.Dataset(features_df, label=label_series)
        
        # è®­ç»ƒæ¨¡å‹
        model = lgb.train(
            params,
            train_data,
            num_boost_round=200,
            callbacks=[lgb.log_evaluation(period=0)]  # ç¦ç”¨æ—¥å¿—
        )
        
        # è·å–å› å­é‡è¦æ€§
        importance_gain = model.feature_importance(importance_type='gain')
        importance_split = model.feature_importance(importance_type='split')
        
        feature_names = features_df.columns.tolist()
        
        # æ•´ç†ç»“æœ
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance_gain': importance_gain,
            'importance_split': importance_split
        })
        
        # å½’ä¸€åŒ–é‡è¦æ€§
        importance_df['importance_gain_norm'] = importance_df['importance_gain'] / importance_df['importance_gain'].sum()
        importance_df['importance_split_norm'] = importance_df['importance_split'] / importance_df['importance_split'].sum()
        
        # æŒ‰é‡è¦æ€§æ’åº
        importance_df = importance_df.sort_values('importance_gain', ascending=False)
        
        return {
            'importance_df': importance_df,
            'n_features': len(feature_names),
            'total_gain': importance_df['importance_gain'].sum(),
            'top_10_features': importance_df.head(10)['feature'].tolist(),
            'top_10_gain_pct': importance_df.head(10)['importance_gain_norm'].sum()
        }
    
    def analyze_importance_by_year(self, years: List[int] = [2021, 2022, 2023, 2024, 2025]):
        """åˆ†æå„å¹´ä»½çš„å› å­é‡è¦æ€§"""
        results = {}
        
        for lib_name in self.factor_libraries.keys():
            print(f"\n{'='*70}")
            print(f"ğŸ“Š åˆ†æå› å­åº“: {lib_name}")
            print(f"{'='*70}")
            
            lib_results = {}
            
            for year in years:
                importance = self.train_and_get_importance(lib_name, year)
                if importance:
                    lib_results[year] = importance
                    
                    print(f"\n  {year}å¹´ Top 10 å› å­ (å æ€»é‡è¦æ€§ {importance['top_10_gain_pct']*100:.1f}%):")
                    for i, name in enumerate(importance['top_10_features'][:5]):
                        print(f"    {i+1}. {name}")
            
            results[lib_name] = lib_results
        
        self.importance_results = results
        return results
    
    def analyze_dominant_factors(self) -> Dict:
        """
        åˆ†æä¸»å¯¼å› å­
        
        è¯†åˆ«åœ¨å¤šä¸ªå¹´ä»½éƒ½æ’åé å‰çš„å› å­
        """
        dominant_factors = {}
        
        for lib_name, lib_results in self.importance_results.items():
            # ç»Ÿè®¡æ¯ä¸ªå› å­åœ¨å„å¹´ä»½çš„æ’å
            factor_ranks = {}
            
            for year, year_data in lib_results.items():
                imp_df = year_data['importance_df']
                
                for rank, (_, row) in enumerate(imp_df.iterrows()):
                    feature = row['feature']
                    if feature not in factor_ranks:
                        factor_ranks[feature] = {}
                    factor_ranks[feature][year] = {
                        'rank': rank + 1,
                        'importance_gain': row['importance_gain'],
                        'importance_pct': row['importance_gain_norm']
                    }
            
            # è®¡ç®—å¹³å‡æ’åå’Œç¨³å®šæ€§
            factor_stats = []
            for feature, yearly_data in factor_ranks.items():
                ranks = [v['rank'] for v in yearly_data.values()]
                gains = [v['importance_pct'] for v in yearly_data.values()]
                
                factor_stats.append({
                    'feature': feature,
                    'avg_rank': np.mean(ranks),
                    'min_rank': min(ranks),
                    'max_rank': max(ranks),
                    'rank_std': np.std(ranks),
                    'avg_importance_pct': np.mean(gains),
                    'years_in_top_20': sum(1 for r in ranks if r <= 20),
                    'yearly_data': yearly_data
                })
            
            # æŒ‰å¹³å‡æ’åæ’åº
            factor_stats.sort(key=lambda x: x['avg_rank'])
            
            # å–å‡ºç¨³å®šçš„é«˜é‡è¦æ€§å› å­
            dominant = [f for f in factor_stats if f['years_in_top_20'] >= 3]
            
            dominant_factors[lib_name] = {
                'all_factors': factor_stats,
                'dominant_factors': dominant[:20],
                'n_dominant': len(dominant)
            }
        
        return dominant_factors
    
    def compare_importance_shift(self) -> Dict:
        """
        å¯¹æ¯”å› å­é‡è¦æ€§åœ¨2022â†’2023çš„å˜åŒ–
        """
        shifts = {}
        
        for lib_name, lib_results in self.importance_results.items():
            if 2022 not in lib_results or 2023 not in lib_results:
                continue
            
            imp_2022 = lib_results[2022]['importance_df'].set_index('feature')
            imp_2023 = lib_results[2023]['importance_df'].set_index('feature')
            
            # åˆå¹¶æ¯”è¾ƒ
            common_features = set(imp_2022.index) & set(imp_2023.index)
            
            comparison = []
            for feature in common_features:
                gain_2022 = imp_2022.loc[feature, 'importance_gain_norm']
                gain_2023 = imp_2023.loc[feature, 'importance_gain_norm']
                
                rank_2022 = imp_2022.index.get_loc(feature) + 1 if feature in imp_2022.index else None
                rank_2023 = imp_2023.index.get_loc(feature) + 1 if feature in imp_2023.index else None
                
                change = (gain_2023 - gain_2022) / gain_2022 * 100 if gain_2022 > 0 else 0
                
                comparison.append({
                    'feature': feature,
                    'gain_2022': gain_2022,
                    'gain_2023': gain_2023,
                    'gain_change_pct': change,
                    'rank_2022': rank_2022,
                    'rank_2023': rank_2023
                })
            
            # æŒ‰é‡è¦æ€§å˜åŒ–æ’åºï¼ˆè¯†åˆ«ä¸Šå‡å’Œä¸‹é™æœ€å¤šçš„å› å­ï¼‰
            comparison.sort(key=lambda x: x['gain_change_pct'])
            
            shifts[lib_name] = {
                'declining': comparison[:10],  # é‡è¦æ€§ä¸‹é™æœ€å¤š
                'rising': comparison[-10:][::-1],  # é‡è¦æ€§ä¸Šå‡æœ€å¤š
                'all': comparison
            }
        
        return shifts
    
    def save_results(self, output_dir: str = None):
        """ä¿å­˜åˆ†æç»“æœ"""
        if output_dir is None:
            output_dir = Path(__file__).parent.parent / "results"
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å¹´åº¦é‡è¦æ€§
        yearly_importance = {}
        for lib_name, lib_results in self.importance_results.items():
            yearly_importance[lib_name] = {}
            for year, year_data in lib_results.items():
                yearly_importance[lib_name][year] = {
                    'top_20': year_data['importance_df'].head(20).to_dict('records'),
                    'n_features': year_data['n_features'],
                    'top_10_gain_pct': year_data['top_10_gain_pct']
                }
        
        with open(output_dir / "factor_importance_by_year.json", 'w', encoding='utf-8') as f:
            json.dump(yearly_importance, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸»å¯¼å› å­
        dominant = self.analyze_dominant_factors()
        dominant_simplified = {}
        for lib_name, data in dominant.items():
            dominant_simplified[lib_name] = {
                'dominant_factors': [
                    {k: v for k, v in f.items() if k != 'yearly_data'}
                    for f in data['dominant_factors']
                ],
                'n_dominant': data['n_dominant']
            }
        
        with open(output_dir / "dominant_factors.json", 'w', encoding='utf-8') as f:
            json.dump(dominant_simplified, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é‡è¦æ€§å˜åŒ–
        shifts = self.compare_importance_shift()
        with open(output_dir / "importance_shift_2022_2023.json", 'w', encoding='utf-8') as f:
            json.dump(shifts, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def print_analysis_report(self):
        """æ‰“å°åˆ†ææŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("ğŸ“ˆ å› å­é‡è¦æ€§åˆ†ææŠ¥å‘Š")
        print(f"{'='*80}")
        
        # ä¸»å¯¼å› å­
        dominant = self.analyze_dominant_factors()
        
        for lib_name, data in dominant.items():
            print(f"\nã€{lib_name} å› å­åº“ - ç¨³å®šé«˜é‡è¦æ€§å› å­ Top 10ã€‘")
            print(f"{'Factor Name':<50} {'Avg Rank':<10} {'Avg Imp%':<10} {'Years Top20':<12}")
            print("-" * 82)
            
            for f in data['dominant_factors'][:10]:
                name = f['feature'][:48]
                avg_rank = f'{f["avg_rank"]:.1f}'
                avg_imp = f'{f["avg_importance_pct"]*100:.2f}%'
                years_top = str(f['years_in_top_20'])
                print(f"{name:<50} {avg_rank:<10} {avg_imp:<10} {years_top:<12}")
        
        # é‡è¦æ€§å˜åŒ–
        shifts = self.compare_importance_shift()
        
        for lib_name, data in shifts.items():
            print(f"\nã€{lib_name} å› å­åº“ - 2022â†’2023 é‡è¦æ€§ä¸‹é™æœ€å¤šçš„å› å­ã€‘")
            print(f"{'Factor Name':<50} {'2022 Imp%':<12} {'2023 Imp%':<12} {'Change':<10}")
            print("-" * 84)
            
            for f in data['declining'][:5]:
                name = f['feature'][:48]
                imp_2022 = f'{f["gain_2022"]*100:.2f}%'
                imp_2023 = f'{f["gain_2023"]*100:.2f}%'
                change = f'{f["gain_change_pct"]:.1f}%'
                print(f"{name:<50} {imp_2022:<12} {imp_2023:<12} {change:<10}")
            
            print(f"\nã€{lib_name} å› å­åº“ - 2022â†’2023 é‡è¦æ€§ä¸Šå‡æœ€å¤šçš„å› å­ã€‘")
            for f in data['rising'][:5]:
                name = f['feature'][:48]
                imp_2022 = f'{f["gain_2022"]*100:.2f}%'
                imp_2023 = f'{f["gain_2023"]*100:.2f}%'
                change = f'+{f["gain_change_pct"]:.1f}%'
                print(f"{name:<50} {imp_2022:<12} {imp_2023:<12} {change:<10}")
        
        # å…³é”®å‘ç°
        print(f"\n{'='*80}")
        print("ğŸ” å…³é”®å‘ç°")
        print(f"{'='*80}")
        
        if 'AA' in dominant and 'QA' in dominant:
            aa_dominant = dominant['AA']['n_dominant']
            qa_dominant = dominant['QA']['n_dominant']
            
            print(f"\n  AA å› å­åº“ç¨³å®šé«˜é‡è¦æ€§å› å­æ•°: {aa_dominant}")
            print(f"  QA å› å­åº“ç¨³å®šé«˜é‡è¦æ€§å› å­æ•°: {qa_dominant}")
            
            if aa_dominant < qa_dominant:
                print(f"\n  âš ï¸  AA å› å­åº“çš„å› å­é‡è¦æ€§æ›´ä¸ç¨³å®šï¼Œå¯èƒ½å¯¼è‡´åœ¨å¸‚åœºå˜åŒ–æ—¶è¡¨ç°ä¸‹é™")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='å› å­é‡è¦æ€§åˆ†æ')
    parser.add_argument('--years', '-y', nargs='+', type=int,
                       default=[2021, 2022, 2023, 2024, 2025],
                       help='è¦åˆ†æçš„å¹´ä»½')
    
    args = parser.parse_args()
    
    analyzer = FactorImportanceAnalyzer()
    
    # åŠ è½½å› å­åº“
    analyzer.load_factor_library(
        "AA",
        "/home/tjxy/quantagent/AlphaAgent/factor_library/AA_top80_RankIC_AA_gpt_123_csi300.json"
    )
    analyzer.load_factor_library(
        "QA",
        "/home/tjxy/quantagent/AlphaAgent/factor_library/hj/RANKIC_desc_150_QA_round11_best_gpt_123_csi300.json"
    )
    
    # åˆ†æ
    analyzer.analyze_importance_by_year(years=args.years)
    
    # ä¿å­˜ç»“æœ
    analyzer.save_results()
    
    # æ‰“å°æŠ¥å‘Š
    analyzer.print_analysis_report()


if __name__ == "__main__":
    main()

