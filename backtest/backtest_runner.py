#!/usr/bin/env python3
"""
å›æµ‹æ‰§è¡Œå™¨ - ä½¿ç”¨ Qlib è¿›è¡Œå®Œæ•´å›æµ‹

åŠŸèƒ½:
1. åŠ è½½å› å­ï¼ˆå®˜æ–¹/è‡ªå®šä¹‰ï¼‰
2. è®¡ç®—è‡ªå®šä¹‰å› å­å€¼ (ä½¿ç”¨ QuantaAlpha è¡¨è¾¾å¼è§£æå™¨)
3. è®­ç»ƒæ¨¡å‹
4. æ‰§è¡Œå›æµ‹
5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡

æ”¯æŒä¸¤ç§æ¨¡å¼:
- å®˜æ–¹å› å­æ¨¡å¼: ä½¿ç”¨ Qlib å†…ç½®çš„ DataLoader
- è‡ªå®šä¹‰å› å­æ¨¡å¼: ä½¿ç”¨ expr_parser + function_lib è®¡ç®—å› å­å€¼
"""

import json
import logging
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

import numpy as np
import pandas as pd
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

logger = logging.getLogger(__name__)


class BacktestRunner:
    """å›æµ‹æ‰§è¡Œå™¨"""
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–å›æµ‹æ‰§è¡Œå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self._qlib_initialized = False
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"âœ“ åŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
        return config
    
    def _init_qlib(self):
        """åˆå§‹åŒ– Qlib"""
        if self._qlib_initialized:
            return
            
        import qlib
        
        provider_uri = self.config['data']['provider_uri']
        qlib.init(provider_uri=provider_uri, region='cn')
        self._qlib_initialized = True
        logger.info(f"âœ“ Qlib åˆå§‹åŒ–å®Œæˆ: {provider_uri}")
    
    def _apply_test_period(self, test_period: str):
        """
        åº”ç”¨æµ‹è¯•æ—¶é—´æ®µé…ç½®
        
        Args:
            test_period: æ—¶é—´æ®µæ ‡è¯† (default/2021/2022/2023/2024/2025/2022-2023/2024-2025)
        """
        test_periods = self.config.get('test_periods', {})
        
        if test_period not in test_periods:
            logger.warning(f"æœªæ‰¾åˆ°æµ‹è¯•æ—¶é—´æ®µé…ç½®: {test_period}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return
        
        period_config = test_periods[test_period]
        
        # æ›´æ–° dataset segments ä¸­çš„ test
        if 'test' in period_config:
            self.config['dataset']['segments']['test'] = period_config['test']
            logger.info(f"æ›´æ–°æµ‹è¯•é›†æ—¶é—´: {period_config['test']}")
        
        # æ›´æ–° backtest é…ç½®
        if 'backtest_start' in period_config:
            self.config['backtest']['backtest']['start_time'] = period_config['backtest_start']
        if 'backtest_end' in period_config:
            self.config['backtest']['backtest']['end_time'] = period_config['backtest_end']
        
        logger.info(f"åº”ç”¨æµ‹è¯•æ—¶é—´æ®µ: {period_config.get('name', test_period)}")
    
    def run(self, 
            factor_source: Optional[str] = None,
            factor_json: Optional[List[str]] = None,
            experiment_name: Optional[str] = None,
            output_name: Optional[str] = None,
            test_period: str = 'default',
            ic_only: bool = False) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´å›æµ‹æµç¨‹
        
        Args:
            factor_source: å› å­æºç±»å‹ (è¦†ç›–é…ç½®æ–‡ä»¶)
            factor_json: è‡ªå®šä¹‰å› å­ JSON æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (è¦†ç›–é…ç½®æ–‡ä»¶)
            experiment_name: å®éªŒåç§° (è¦†ç›–é…ç½®æ–‡ä»¶)
            output_name: è¾“å‡ºæ–‡ä»¶åå‰ç¼€ (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å› å­åº“æ–‡ä»¶å)
            test_period: æµ‹è¯•æ—¶é—´æ®µ (default/2021/2022/2023/2024/2025/2022-2023/2024-2025)
            ic_only: æ˜¯å¦ä»…è®¡ç®— IC æŒ‡æ ‡ï¼Œè·³è¿‡ç­–ç•¥ç»„åˆå›æµ‹
            
        Returns:
            Dict: å›æµ‹ç»“æœæŒ‡æ ‡
        """
        start_time_total = time.time()
        
        # åˆå§‹åŒ– Qlib
        self._init_qlib()
        
        # æ›´æ–°é…ç½®
        if factor_source:
            self.config['factor_source']['type'] = factor_source
        if factor_json:
            self.config['factor_source']['custom']['json_files'] = factor_json
        
        # åº”ç”¨æµ‹è¯•æ—¶é—´æ®µé…ç½®
        self._apply_test_period(test_period)
        
        # è‡ªåŠ¨ä»å› å­åº“æ–‡ä»¶åç”Ÿæˆè¾“å‡ºåç§°
        if output_name is None and factor_json:
            # å–ç¬¬ä¸€ä¸ªå› å­åº“æ–‡ä»¶åï¼ˆå»æ‰æ‰©å±•åï¼‰
            output_name = Path(factor_json[0]).stem
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ—¶é—´æ®µï¼Œåœ¨è¾“å‡ºåç§°ä¸­æ·»åŠ æ ‡è¯†
        if test_period != 'default' and output_name:
            output_name = f"{output_name}_{test_period}"
        
        exp_name = experiment_name or output_name or self.config['experiment']['name']
        rec_name = self.config['experiment']['recorder']
        
        # è·å–æ—¶é—´æ®µåç§°ç”¨äºæ˜¾ç¤º
        period_name = self.config.get('test_periods', {}).get(test_period, {}).get('name', test_period)
        
        print(f"\n{'='*70}")
        print(f"ğŸš€ å¼€å§‹å›æµ‹: {exp_name}")
        if factor_json:
            print(f"ğŸ“ å› å­åº“: {factor_json[0]}")
        print(f"ğŸ“… æµ‹è¯•æ—¶é—´æ®µ: {period_name}")
        if ic_only:
            print(f"âš¡ æ¨¡å¼: ä»…è®¡ç®— IC æŒ‡æ ‡ï¼ˆè·³è¿‡ç­–ç•¥å›æµ‹ï¼‰")
        print(f"{'='*70}\n")
        
        # 1. åŠ è½½å› å­
        print("ğŸ“Š ç¬¬ä¸€æ­¥ï¼šåŠ è½½å› å­...")
        factor_expressions, custom_factors = self._load_factors()
        print(f"  âœ“ Qlib å…¼å®¹å› å­: {len(factor_expressions)} ä¸ª")
        print(f"  âœ“ éœ€è¦è®¡ç®—çš„è‡ªå®šä¹‰å› å­: {len(custom_factors)} ä¸ª")
        
        # 2. è®¡ç®—è‡ªå®šä¹‰å› å­ï¼ˆå¦‚æœæœ‰ï¼‰
        computed_factors = None
        if custom_factors:
            print("\nğŸ”§ ç¬¬äºŒæ­¥ï¼šè®¡ç®—è‡ªå®šä¹‰å› å­...")
            computed_factors = self._compute_custom_factors(custom_factors)
            if computed_factors is not None and not computed_factors.empty:
                print(f"  âœ“ æˆåŠŸè®¡ç®— {len(computed_factors.columns)} ä¸ªå› å­")
        
        # 3. åˆ›å»ºæ•°æ®é›†
        print("\nğŸ“ˆ ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæ•°æ®é›†...")
        dataset = self._create_dataset(factor_expressions, computed_factors)
        
        # 4. è®­ç»ƒæ¨¡å‹å¹¶å›æµ‹
        if ic_only:
            print("\nğŸ¤– ç¬¬å››æ­¥ï¼šè®­ç»ƒæ¨¡å‹å¹¶è®¡ç®— IC æŒ‡æ ‡ï¼ˆè·³è¿‡ç­–ç•¥å›æµ‹ï¼‰...")
        else:
            print("\nğŸ¤– ç¬¬å››æ­¥ï¼šè®­ç»ƒæ¨¡å‹å¹¶æ‰§è¡Œå›æµ‹...")
        metrics = self._train_and_backtest(dataset, exp_name, rec_name, ic_only=ic_only)
        
        # 5. è¾“å‡ºç»“æœ
        total_time = time.time() - start_time_total
        self._print_results(metrics, total_time, ic_only=ic_only)
        
        # 6. ä¿å­˜ç»“æœ
        self._save_results(metrics, exp_name, factor_source or self.config['factor_source']['type'], 
                          len(factor_expressions) + len(custom_factors), total_time,
                          output_name=output_name, test_period=test_period, ic_only=ic_only)
        
        return metrics
    
    def _load_factors(self) -> Tuple[Dict[str, str], List[Dict]]:
        """åŠ è½½å› å­"""
        from .factor_loader import FactorLoader
        
        loader = FactorLoader(self.config)
        return loader.load_factors()
    
    def _compute_custom_factors(self, factors: List[Dict]) -> Optional[pd.DataFrame]:
        """
        è®¡ç®—è‡ªå®šä¹‰å› å­
        ä½¿ç”¨ QuantaAlpha çš„ expr_parser å’Œ function_lib
        æ”¯æŒä»ç¼“å­˜åŠ è½½é¢„è®¡ç®—çš„å› å­å€¼
        """
        from .custom_factor_calculator import CustomFactorCalculator, get_qlib_stock_data
        from pathlib import Path
        
        # è·å–æ•°æ®
        print("  è·å–è‚¡ç¥¨æ•°æ®...")
        data_df = get_qlib_stock_data(self.config)
        
        if data_df is None or data_df.empty:
            logger.error("æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")
            return None
        
        logger.info(f"  âœ“ åŠ è½½è‚¡ç¥¨æ•°æ®: {len(data_df)} æ¡è®°å½•")
        
        # è·å–ç¼“å­˜é…ç½®
        llm_config = self.config.get('llm', {})
        cache_dir = llm_config.get('cache_dir')
        if cache_dir:
            cache_dir = Path(cache_dir)
        
        # æ˜¯å¦è‡ªåŠ¨ä»ä¸»ç¨‹åºæ—¥å¿—æå–ç¼“å­˜
        auto_extract = llm_config.get('auto_extract_cache', True)

        # è·å–å¹¶è¡Œè®¡ç®—é…ç½®
        factor_calc_config = self.config.get('factor_calculation', {})
        n_jobs = factor_calc_config.get('n_jobs', 1)
        
        # åˆ›å»ºè®¡ç®—å™¨ (ä¼ é€’ç¼“å­˜ç›®å½•å’Œè‡ªåŠ¨æå–é…ç½®)
        calculator = CustomFactorCalculator(data_df, cache_dir=cache_dir, auto_extract_cache=auto_extract)
        
        # è®¡ç®—å› å­ (ä¼šä¼˜å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œç¼“å­˜ä¸å­˜åœ¨ä¼šè‡ªåŠ¨æå–)
        result_df = calculator.calculate_factors_batch(factors, use_cache=True, n_jobs=n_jobs)
        
        # éªŒè¯ç»“æœ
        if result_df is None:
            logger.error("å› å­è®¡ç®—è¿”å› None")
            return None
        
        if not isinstance(result_df, pd.DataFrame):
            logger.error(f"å› å­è®¡ç®—è¿”å›ç±»å‹é”™è¯¯: {type(result_df)}")
            return None
        
        if result_df.empty:
            logger.error("å› å­è®¡ç®—ç»“æœä¸ºç©º DataFrame")
            return None
        
        # ç¡®ä¿ç´¢å¼•æ­£ç¡®
        if not isinstance(result_df.index, pd.MultiIndex):
            logger.warning("å› å­æ•°æ®ç´¢å¼•ä¸æ˜¯ MultiIndexï¼Œå°è¯•ä¿®å¤...")
            # å°è¯•ä½¿ç”¨åŸå§‹æ•°æ®çš„ç´¢å¼•
            if isinstance(data_df.index, pd.MultiIndex):
                result_df.index = data_df.index
        
        logger.info(f"  âœ“ å› å­è®¡ç®—å®Œæˆ: {len(result_df.columns)} ä¸ªå› å­, {len(result_df)} è¡Œæ•°æ®")
        
        return result_df
    
    def _create_dataset(self, 
                       factor_expressions: Dict[str, str],
                       computed_factors: Optional[pd.DataFrame] = None):
        """
        åˆ›å»º Qlib æ•°æ®é›†
        
        æ”¯æŒä¸¤ç§æ¨¡å¼:
        1. çº¯ Qlib å› å­æ¨¡å¼: ä½¿ç”¨ QlibDataLoader
        2. è‡ªå®šä¹‰å› å­æ¨¡å¼: ä½¿ç”¨é¢„è®¡ç®—çš„å› å­å€¼ + StaticDataLoader
        """
        from qlib.data.dataset import DatasetH
        from qlib.data.dataset.handler import DataHandlerLP
        
        data_config = self.config['data']
        dataset_config = self.config['dataset']
        
        # æ£€æŸ¥ computed_factors çš„æœ‰æ•ˆæ€§
        has_computed_factors = False
        if computed_factors is not None:
            if isinstance(computed_factors, pd.DataFrame):
                # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                if len(computed_factors) > 0 and len(computed_factors.columns) > 0:
                    has_computed_factors = True
                    logger.info(f"  æ£€æµ‹åˆ°é¢„è®¡ç®—å› å­: {len(computed_factors.columns)} ä¸ªå› å­, {len(computed_factors)} è¡Œæ•°æ®")
                else:
                    logger.warning(f"  é¢„è®¡ç®—å› å­ DataFrame ä¸ºç©º: {computed_factors.shape}")
            else:
                logger.warning(f"  é¢„è®¡ç®—å› å­ç±»å‹ä¸æ­£ç¡®: {type(computed_factors)}")
        
        # å¦‚æœæœ‰è®¡ç®—å¥½çš„è‡ªå®šä¹‰å› å­ï¼Œä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰å› å­æ¨¡å¼
        if has_computed_factors:
            print("  ä½¿ç”¨è‡ªå®šä¹‰å› å­æ¨¡å¼ (é¢„è®¡ç®—å› å­å€¼)...")
            return self._create_dataset_with_computed_factors(
                factor_expressions, computed_factors
            )
        
        # çº¯ Qlib å› å­æ¨¡å¼
        expressions = list(factor_expressions.values())
        names = list(factor_expressions.keys())
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å› å­
        if not expressions:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å› å­è¡¨è¾¾å¼ã€‚å¦‚æœä½¿ç”¨è‡ªå®šä¹‰å› å­ï¼Œè¯·ç¡®ä¿å› å­è®¡ç®—æˆåŠŸã€‚")
        
        handler_config = {
            'start_time': data_config['start_time'],
            'end_time': data_config['end_time'],
            'instruments': data_config['market'],
            'data_loader': {
                'class': 'QlibDataLoader',
                'module_path': 'qlib.contrib.data.loader',
                'kwargs': {
                    'config': {
                        'feature': (expressions, names),
                        'label': ([dataset_config['label']], ['LABEL0'])
                    }
                }
            },
            'learn_processors': dataset_config['learn_processors'],
            'infer_processors': dataset_config['infer_processors']
        }
        
        dataset = DatasetH(
            handler=DataHandlerLP(**handler_config),
            segments=dataset_config['segments']
        )
        
        print(f"  è®­ç»ƒé›†: {dataset_config['segments']['train']}")
        print(f"  éªŒè¯é›†: {dataset_config['segments']['valid']}")
        print(f"  æµ‹è¯•é›†: {dataset_config['segments']['test']}")
        print(f"  å› å­æ•°é‡: {len(expressions)}")
        
        return dataset
    
    def _create_dataset_with_computed_factors(self,
                                              factor_expressions: Dict[str, str],
                                              computed_factors: pd.DataFrame):
        """
        ä½¿ç”¨é¢„è®¡ç®—çš„å› å­å€¼åˆ›å»ºæ•°æ®é›†
        
        è¿™ç§æ¨¡å¼ä¸‹:
        1. å…ˆè®¡ç®—æ ‡ç­¾
        2. å°†å› å­å€¼å’Œæ ‡ç­¾åˆå¹¶
        3. ä½¿ç”¨è‡ªå®šä¹‰ DataHandler åŠ è½½æ•°æ®
        """
        from qlib.data.dataset import DatasetH
        from qlib.data.dataset.handler import DataHandler
        from qlib.data import D
        
        data_config = self.config['data']
        dataset_config = self.config['dataset']
        
        print(f"  è®¡ç®—å› å­æ•°é‡: {len(computed_factors.columns)}")
        
        # è®¡ç®—æ ‡ç­¾
        print("  è®¡ç®—æ ‡ç­¾...")
        label_expr = dataset_config['label']
        label_df = self._compute_label(label_expr)
        
        # åˆå¹¶ Qlib å…¼å®¹å› å­ (å¦‚æœæœ‰)
        all_feature_dfs = [computed_factors]
        
        if factor_expressions:
            print(f"  åŠ è½½ {len(factor_expressions)} ä¸ª Qlib å…¼å®¹å› å­...")
            qlib_factors = self._load_qlib_factors(factor_expressions)
            if qlib_factors is not None and not qlib_factors.empty:
                all_feature_dfs.append(qlib_factors)
        
        # åˆå¹¶æ‰€æœ‰å› å­
        features_df = pd.concat(all_feature_dfs, axis=1)
        
        # å»é™¤é‡å¤åˆ—
        features_df = features_df.loc[:, ~features_df.columns.duplicated()]
        
        print(f"  æ€»å› å­æ•°é‡: {len(features_df.columns)}")
        
        # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
        # ç¡®ä¿ç´¢å¼•å¯¹é½
        common_index = features_df.index.intersection(label_df.index)
        features_df = features_df.loc[common_index]
        label_df = label_df.loc[common_index]
        
        print(f"  æ•°æ®è¡Œæ•°: {len(features_df)}")
        
        # ç›´æ¥ä½¿ç”¨ DataHandler æ„å»ºæ•°æ®é›†
        # åˆå¹¶ feature å’Œ label
        combined_df = pd.concat([features_df, label_df], axis=1)
        
        # åº”ç”¨é¢„å¤„ç†
        from qlib.data.dataset.processor import Fillna, ProcessInf, CSRankNorm, DropnaLabel
        
        print("  åº”ç”¨æ•°æ®é¢„å¤„ç†...")
        
        # åˆ†ç¦» feature å’Œ label åˆ—
        feature_cols = list(features_df.columns)
        label_cols = list(label_df.columns)
        
        # å¤„ç† feature
        combined_df[feature_cols] = combined_df[feature_cols].fillna(0)
        combined_df[feature_cols] = combined_df[feature_cols].replace([np.inf, -np.inf], 0)
        
        # å¯¹ feature åš CSRankNorm
        for col in feature_cols:
            combined_df[col] = combined_df.groupby(level='datetime')[col].transform(
                lambda x: (x.rank(pct=True) - 0.5) if len(x) > 1 else 0
            )
        
        # å¤„ç† label - åˆ é™¤ label ä¸º NaN çš„è¡Œ
        combined_df = combined_df.dropna(subset=label_cols)
        
        # å¯¹ label åš CSRankNorm  
        for col in label_cols:
            combined_df[col] = combined_df.groupby(level='datetime')[col].transform(
                lambda x: (x.rank(pct=True) - 0.5) if len(x) > 1 else 0
            )
        
        print(f"  é¢„å¤„ç†åæ•°æ®è¡Œæ•°: {len(combined_df)}")
        
        # ä½¿ç”¨å¤šçº§åˆ—ç´¢å¼•æ ‡è¯† feature å’Œ label (Qlib æ ‡å‡†æ ¼å¼)
        # é‡æ„ DataFrame åˆ—ä¸º MultiIndex: (col_set, col_name)
        feature_tuples = [('feature', col) for col in feature_cols]
        label_tuples = [('label', col) for col in label_cols]
        
        combined_df_multi = combined_df.copy()
        combined_df_multi.columns = pd.MultiIndex.from_tuples(
            feature_tuples + label_tuples
        )
        
        # æ„å»ºè‡ªå®šä¹‰ DataHandler
        class PrecomputedDataHandler(DataHandler):
            """ä½¿ç”¨é¢„è®¡ç®—æ•°æ®çš„ DataHandler"""
            
            def __init__(self, data_df, segments):
                self._data = data_df
                self._segments = segments
            
            @property
            def data_loader(self):
                return None
            
            @property
            def instruments(self):
                return list(self._data.index.get_level_values('instrument').unique())
            
            def fetch(self, selector=None, level='datetime', col_set='feature', 
                     data_key=None, squeeze=False, proc_func=None):
                """è·å–æ•°æ®"""
                # æ ¹æ® col_set é€‰æ‹©åˆ—
                if col_set in ('feature', 'label'):
                    result = self._data[col_set].copy()
                elif col_set == '__all' or col_set is None:
                    result = self._data.copy()
                else:
                    # col_set å¯èƒ½æ˜¯åˆ—ååˆ—è¡¨
                    if isinstance(col_set, (list, tuple)):
                        result = self._data[list(col_set)].copy()
                    else:
                        result = self._data.copy()
                
                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                # selector å¯èƒ½æ˜¯ tuple, list, æˆ– slice æ ¼å¼
                if selector is not None:
                    start, end = None, None
                    
                    # å¤„ç† tuple æˆ– list æ ¼å¼: (start, end) æˆ– [start, end]
                    if isinstance(selector, (tuple, list)) and len(selector) == 2:
                        start, end = selector[0], selector[1]
                    # å¤„ç† slice æ ¼å¼
                    elif isinstance(selector, slice):
                        start, end = selector.start, selector.stop
                    
                    # æ‰§è¡Œæ—¥æœŸè¿‡æ»¤
                    if start is not None and end is not None:
                        dates = result.index.get_level_values('datetime')
                        mask = (dates >= pd.Timestamp(start)) & (dates <= pd.Timestamp(end))
                        result = result.loc[mask]
                
                if squeeze and result.shape[1] == 1:
                    result = result.iloc[:, 0]
                
                return result
            
            def get_cols(self, col_set='feature'):
                """è·å–åˆ—å"""
                if col_set in self._data.columns.get_level_values(0):
                    return list(self._data[col_set].columns)
                return list(self._data.columns.get_level_values(1))
            
            def setup_data(self, **kwargs):
                pass
            
            def config(self, **kwargs):
                pass
        
        # åˆ›å»º handler
        handler = PrecomputedDataHandler(combined_df_multi, dataset_config['segments'])
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = DatasetH(
            handler=handler,
            segments=dataset_config['segments']
        )
        
        print(f"  è®­ç»ƒé›†: {dataset_config['segments']['train']}")
        print(f"  éªŒè¯é›†: {dataset_config['segments']['valid']}")
        print(f"  æµ‹è¯•é›†: {dataset_config['segments']['test']}")
        
        return dataset
    
    def _compute_label(self, label_expr: str) -> pd.DataFrame:
        """
        è®¡ç®—æ ‡ç­¾
        
        ä½¿ç”¨ Qlib åŸç”Ÿæ–¹å¼è®¡ç®—æ ‡ç­¾ï¼ˆå› ä¸ºæ ‡ç­¾éœ€è¦å‘å‰çœ‹ï¼‰
        """
        from qlib.data import D
        
        data_config = self.config['data']
        
        print(f"  æ ‡ç­¾è¡¨è¾¾å¼: {label_expr}")
        
        stock_list = D.instruments(data_config['market'])
        
        # ä½¿ç”¨ Qlib è®¡ç®—æ ‡ç­¾
        label_df = D.features(
            stock_list,
            [label_expr],
            start_time=data_config['start_time'],
            end_time=data_config['end_time'],
            freq='day'
        )
        
        label_df.columns = ['LABEL0']
        
        print(f"  æ ‡ç­¾æ•°æ®è¡Œæ•°: {len(label_df)}")
        
        return label_df
    
    def _load_qlib_factors(self, factor_expressions: Dict[str, str]) -> Optional[pd.DataFrame]:
        """åŠ è½½ Qlib å…¼å®¹çš„å› å­"""
        from qlib.data import D
        
        data_config = self.config['data']
        
        try:
            stock_list = D.instruments(data_config['market'])
            
            expressions = list(factor_expressions.values())
            names = list(factor_expressions.keys())
            
            df = D.features(
                stock_list,
                expressions,
                start_time=data_config['start_time'],
                end_time=data_config['end_time'],
                freq='day'
            )
            
            df.columns = names
            return df
        except Exception as e:
            logger.warning(f"åŠ è½½ Qlib å› å­å¤±è´¥: {e}")
            return None
    
    def _train_and_backtest(self, dataset, exp_name: str, rec_name: str, ic_only: bool = False) -> Dict:
        """è®­ç»ƒæ¨¡å‹å¹¶æ‰§è¡Œå›æµ‹
        
        Args:
            dataset: Qlib æ•°æ®é›†
            exp_name: å®éªŒåç§°
            rec_name: è®°å½•å™¨åç§°
            ic_only: æ˜¯å¦ä»…è®¡ç®— IC æŒ‡æ ‡ï¼Œè·³è¿‡ç­–ç•¥ç»„åˆå›æµ‹
        """
        from qlib.contrib.model.gbdt import LGBModel
        from qlib.data import D
        from qlib.workflow import R
        from qlib.workflow.record_temp import SignalRecord, SigAnaRecord
        from qlib.backtest import backtest as qlib_backtest
        from qlib.contrib.evaluate import risk_analysis
        
        model_config = self.config['model']
        backtest_config = self.config['backtest']['backtest']
        strategy_config = self.config['backtest']['strategy']
        
        metrics = {}
        
        with R.start(experiment_name=exp_name, recorder_name=rec_name):
            # è®­ç»ƒæ¨¡å‹
            print("  è®­ç»ƒ LightGBM æ¨¡å‹...")
            train_start = time.time()
            
            if model_config['type'] == 'lgb':
                model = LGBModel(**model_config['params'])
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_config['type']}")
            
            model.fit(dataset)
            print(f"  âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ (è€—æ—¶: {time.time()-train_start:.2f}ç§’)")
            
            # ç”Ÿæˆé¢„æµ‹
            print("  ç”Ÿæˆé¢„æµ‹...")
            pred = model.predict(dataset)
            print(f"  âœ“ é¢„æµ‹æ•°æ®å½¢çŠ¶: {pred.shape}")
            
            # ä¿å­˜é¢„æµ‹
            sr = SignalRecord(recorder=R.get_recorder(), model=model, dataset=dataset)
            sr.generate()
            
            # è®¡ç®— IC æŒ‡æ ‡
            print("  è®¡ç®— IC æŒ‡æ ‡...")
            try:
                sar = SigAnaRecord(recorder=R.get_recorder(), ana_long_short=False, ann_scaler=252)
                sar.generate()
                
                recorder = R.get_recorder()
                try:
                    ic_series = recorder.load_object("sig_analysis/ic.pkl")
                    ric_series = recorder.load_object("sig_analysis/ric.pkl")
                    
                    if isinstance(ic_series, pd.Series) and len(ic_series) > 0:
                        metrics['IC'] = float(ic_series.mean())
                        metrics['ICIR'] = float(ic_series.mean() / ic_series.std()) if ic_series.std() > 0 else 0.0
                    
                    if isinstance(ric_series, pd.Series) and len(ric_series) > 0:
                        metrics['Rank IC'] = float(ric_series.mean())
                        metrics['Rank ICIR'] = float(ric_series.mean() / ric_series.std()) if ric_series.std() > 0 else 0.0
                    
                    print(f"  âœ“ IC={metrics.get('IC', 0):.6f}, ICIR={metrics.get('ICIR', 0):.6f}")
                    print(f"  âœ“ Rank IC={metrics.get('Rank IC', 0):.6f}, Rank ICIR={metrics.get('Rank ICIR', 0):.6f}")
                except Exception as e:
                    logger.warning(f"æ— æ³•è¯»å– IC ç»“æœ: {e}")
            except Exception as e:
                logger.warning(f"IC åˆ†æå¤±è´¥: {e}")
            
            # å¦‚æœæ˜¯ ic_only æ¨¡å¼ï¼Œè·³è¿‡ç­–ç•¥ç»„åˆå›æµ‹
            if ic_only:
                print("  â© è·³è¿‡ç­–ç•¥ç»„åˆå›æµ‹ (--ic-only æ¨¡å¼)")
                return metrics
            
            # æ‰§è¡Œç»„åˆå›æµ‹
            print("  æ‰§è¡Œç»„åˆå›æµ‹...")
            try:
                bt_start = time.time()
                
                market = self.config['data']['market']
                instruments = D.instruments(market)
                stock_list = D.list_instruments(
                    instruments,
                    start_time=backtest_config['start_time'],
                    end_time=backtest_config['end_time'],
                    as_list=True
                )
                print(f"  âœ“ è‚¡ç¥¨æ•°é‡: {len(stock_list)}")
                
                if len(stock_list) < 10:
                    logger.warning(f"âš ï¸  è­¦å‘Š: è‚¡ç¥¨æ± è¿‡å° ({len(stock_list)} åªè‚¡ç¥¨)ï¼Œå›æµ‹ç»“æœå¯èƒ½ä¸å¯ä¿¡ï¼")
                
                # è¿‡æ»¤ä»·æ ¼å¼‚å¸¸çš„è‚¡ç¥¨ä¿¡å·
                print("  æ£€æŸ¥å¹¶è¿‡æ»¤ä»·æ ¼å¼‚å¸¸æ•°æ®...")
                try:
                    price_data = D.features(
                        stock_list,
                        ['$close'],
                        start_time=backtest_config['start_time'],
                        end_time=backtest_config['end_time'],
                        freq='day'
                    )
                    invalid_mask = (price_data['$close'] == 0) | (price_data['$close'].isna())
                    invalid_count = invalid_mask.sum()
                    
                    if invalid_count > 0:
                        print(f"  âš ï¸ å‘ç° {invalid_count} æ¡ä»·æ ¼ä¸º0/NaNçš„è®°å½•")
                        if isinstance(pred, pd.Series):
                            invalid_indices = invalid_mask[invalid_mask].index
                            invalid_set = set()
                            for idx in invalid_indices:
                                instrument, datetime = idx
                                invalid_set.add((datetime, instrument))
                            
                            filtered_count = 0
                            for idx in pred.index:
                                if idx in invalid_set:
                                    pred.loc[idx] = np.nan
                                    filtered_count += 1
                            
                            if filtered_count > 0:
                                print(f"  âœ“ å·²å°† {filtered_count} æ¡ä»·æ ¼å¼‚å¸¸çš„é¢„æµ‹ä¿¡å·è®¾ä¸ºNaN")
                except Exception as filter_err:
                    logger.warning(f"ä»·æ ¼è¿‡æ»¤å¤±è´¥: {filter_err}")
                
                portfolio_metric_dict, indicator_dict = qlib_backtest(
                    executor={
                        "class": "SimulatorExecutor",
                        "module_path": "qlib.backtest.executor",
                        "kwargs": {
                            "time_per_step": "day",
                            "generate_portfolio_metrics": True,
                            "verbose": False,
                            "indicator_config": {"show_indicator": False}
                        }
                    },
                    strategy={
                        "class": strategy_config['class'],
                        "module_path": strategy_config['module_path'],
                        "kwargs": {
                            "signal": pred,
                            "topk": strategy_config['kwargs']['topk'],
                            "n_drop": strategy_config['kwargs']['n_drop']
                        }
                    },
                    start_time=backtest_config['start_time'],
                    end_time=backtest_config['end_time'],
                    account=backtest_config['account'],
                    benchmark=backtest_config['benchmark'],
                    exchange_kwargs={
                        "codes": stock_list,
                        **backtest_config['exchange_kwargs']
                    }
                )
                
                print(f"  âœ“ ç»„åˆå›æµ‹å®Œæˆ (è€—æ—¶: {time.time()-bt_start:.2f}ç§’)")
                
                # æå–ç»„åˆæŒ‡æ ‡
                if portfolio_metric_dict and "1day" in portfolio_metric_dict:
                    report_df, positions_df = portfolio_metric_dict["1day"]
                    
                    if isinstance(report_df, pd.DataFrame) and 'return' in report_df.columns:
                        portfolio_return = report_df['return'].replace([np.inf, -np.inf], np.nan).fillna(0)
                        bench_return = report_df['bench'].replace([np.inf, -np.inf], np.nan).fillna(0) if 'bench' in report_df.columns else 0
                        cost = report_df['cost'].replace([np.inf, -np.inf], np.nan).fillna(0) if 'cost' in report_df.columns else 0
                        
                        excess_return_with_cost = portfolio_return - bench_return - cost
                        excess_return_with_cost = excess_return_with_cost.dropna()
                        
                        if len(excess_return_with_cost) > 0:
                            analysis = risk_analysis(excess_return_with_cost)
                            
                            if isinstance(analysis, pd.DataFrame):
                                analysis = analysis['risk'] if 'risk' in analysis.columns else analysis.iloc[:, 0]
                            
                            ann_ret = float(analysis.get('annualized_return', 0))
                            info_ratio = float(analysis.get('information_ratio', 0))
                            max_dd = float(analysis.get('max_drawdown', 0))
                            
                            if not np.isnan(ann_ret) and not np.isinf(ann_ret):
                                metrics['annualized_return'] = ann_ret
                            if not np.isnan(info_ratio) and not np.isinf(info_ratio):
                                metrics['information_ratio'] = info_ratio
                            if not np.isnan(max_dd) and not np.isinf(max_dd):
                                metrics['max_drawdown'] = max_dd
                            
                            if max_dd != 0 and not np.isnan(ann_ret) and not np.isinf(ann_ret):
                                calmar = ann_ret / abs(max_dd)
                                if not np.isnan(calmar) and not np.isinf(calmar):
                                    metrics['calmar_ratio'] = calmar
                            
                            print(f"  âœ“ æå–äº†ç»„åˆç­–ç•¥æŒ‡æ ‡")
                            
            except Exception as e:
                logger.warning(f"ç»„åˆå›æµ‹å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        return metrics
    
    def _print_results(self, metrics: Dict, total_time: float, ic_only: bool = False):
        """æ‰“å°ç»“æœ"""
        print(f"\n{'='*70}")
        print("ğŸ“ˆ å›æµ‹ç»“æœ:")
        print(f"{'='*70}")
        
        print("\nã€IC æŒ‡æ ‡ã€‘")
        print(f"  IC:               {metrics.get('IC', 'N/A'):.6f}" if isinstance(metrics.get('IC'), float) else f"  IC:               {metrics.get('IC', 'N/A')}")
        print(f"  ICIR:             {metrics.get('ICIR', 'N/A'):.6f}" if isinstance(metrics.get('ICIR'), float) else f"  ICIR:             {metrics.get('ICIR', 'N/A')}")
        print(f"  Rank IC:          {metrics.get('Rank IC', 'N/A'):.6f}" if isinstance(metrics.get('Rank IC'), float) else f"  Rank IC:          {metrics.get('Rank IC', 'N/A')}")
        print(f"  Rank ICIR:        {metrics.get('Rank ICIR', 'N/A'):.6f}" if isinstance(metrics.get('Rank ICIR'), float) else f"  Rank ICIR:        {metrics.get('Rank ICIR', 'N/A')}")
        
        if ic_only:
            print("\nã€ç­–ç•¥æŒ‡æ ‡ã€‘")
            print("  â© å·²è·³è¿‡ (--ic-only æ¨¡å¼)")
        else:
            print("\nã€ç­–ç•¥æŒ‡æ ‡ã€‘")
            print(f"  å¹´åŒ–æ”¶ç›Š:         {metrics.get('annualized_return', 'N/A'):.4f}" if isinstance(metrics.get('annualized_return'), float) else f"  å¹´åŒ–æ”¶ç›Š:         {metrics.get('annualized_return', 'N/A')}")
            print(f"  ä¿¡æ¯æ¯”ç‡:         {metrics.get('information_ratio', 'N/A'):.4f}" if isinstance(metrics.get('information_ratio'), float) else f"  ä¿¡æ¯æ¯”ç‡:         {metrics.get('information_ratio', 'N/A')}")
            print(f"  æœ€å¤§å›æ’¤:         {metrics.get('max_drawdown', 'N/A'):.4f}" if isinstance(metrics.get('max_drawdown'), float) else f"  æœ€å¤§å›æ’¤:         {metrics.get('max_drawdown', 'N/A')}")
            print(f"  å¡å°”ç›æ¯”ç‡:       {metrics.get('calmar_ratio', 'N/A'):.4f}" if isinstance(metrics.get('calmar_ratio'), float) else f"  å¡å°”ç›æ¯”ç‡:       {metrics.get('calmar_ratio', 'N/A')}")
        
        print(f"\nâ±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"{'='*70}\n")
    
    def _save_results(self, metrics: Dict, exp_name: str, 
                     factor_source: str, num_factors: int, elapsed: float,
                     output_name: Optional[str] = None,
                     test_period: str = 'default',
                     ic_only: bool = False):
        """ä¿å­˜ç»“æœ"""
        output_dir = Path(self.config['experiment'].get('output_dir', './backtest_results'))
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨è‡ªå®šä¹‰è¾“å‡ºåç§°æˆ–é…ç½®ä¸­çš„é»˜è®¤åç§°
        if output_name:
            output_file = f"{output_name}_backtest_metrics.json"
        else:
            output_file = self.config['experiment']['output_metrics_file']
        output_path = output_dir / output_file
        
        # è·å–æ—¶é—´æ®µåç§°
        period_name = self.config.get('test_periods', {}).get(test_period, {}).get('name', test_period)
        
        result_data = {
            "experiment_name": exp_name,
            "factor_source": factor_source,
            "num_factors": num_factors,
            "test_period": test_period,
            "test_period_name": period_name,
            "ic_only": ic_only,
            "metrics": metrics,
            "config": {
                "data_range": f"{self.config['data']['start_time']} ~ {self.config['data']['end_time']}",
                "test_range": f"{self.config['dataset']['segments']['test'][0]} ~ {self.config['dataset']['segments']['test'][1]}",
                "backtest_range": f"{self.config['backtest']['backtest']['start_time']} ~ {self.config['backtest']['backtest']['end_time']}",
                "market": self.config['data']['market'],
                "benchmark": self.config['backtest']['backtest']['benchmark']
            },
            "elapsed_seconds": elapsed
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}\n")
        
        # åŒæ—¶è¿½åŠ åˆ°æ±‡æ€»æ–‡ä»¶
        summary_file = output_dir / "batch_summary.json"
        summary_data = []
        if summary_file.exists():
            try:
                with open(summary_file, 'r', encoding='utf-8') as f:
                    summary_data = json.load(f)
            except:
                summary_data = []
        
        # æ·»åŠ å½“å‰ç»“æœåˆ°æ±‡æ€»
        ann_ret = metrics.get('annualized_return')
        mdd = metrics.get('max_drawdown')
        calmar_ratio = None
        if ann_ret is not None and mdd is not None and mdd != 0:
            calmar_ratio = ann_ret / abs(mdd)
        
        summary_entry = {
            "name": output_name or exp_name,
            "test_period": test_period,
            "test_period_name": period_name,
            "ic_only": ic_only,
            "num_factors": num_factors,
            "IC": metrics.get('IC'),
            "ICIR": metrics.get('ICIR'),
            "Rank_IC": metrics.get('Rank IC'),
            "Rank_ICIR": metrics.get('Rank ICIR'),
            "annualized_return": ann_ret if not ic_only else None,
            "information_ratio": metrics.get('information_ratio') if not ic_only else None,
            "max_drawdown": mdd if not ic_only else None,
            "calmar_ratio": calmar_ratio if not ic_only else None,
            "elapsed_seconds": elapsed
        }
        summary_data.append(summary_entry)
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ å·²è¿½åŠ åˆ°æ±‡æ€»: {summary_file}")
