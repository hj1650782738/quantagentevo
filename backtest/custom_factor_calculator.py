#!/usr/bin/env python3
"""
è‡ªå®šä¹‰å› å­è®¡ç®—å™¨ - ç›´æ¥ä½¿ç”¨ QuantaAlpha çš„è¡¨è¾¾å¼è§£æå™¨
æ”¯æŒæ‰€æœ‰å› å­æŒ–æ˜æ—¶ä½¿ç”¨çš„è¡¨è¾¾å¼è¯­æ³•

åŠŸèƒ½:
1. è§£æå› å­è¡¨è¾¾å¼ (ä½¿ç”¨ expr_parser)
2. è®¡ç®—å› å­å€¼ (ä½¿ç”¨ function_lib)
3. ç”Ÿæˆä¸ Qlib DataLoader å…¼å®¹çš„æ•°æ®æ ¼å¼
4. æ”¯æŒä»ç¼“å­˜åŠ è½½é¢„è®¡ç®—çš„å› å­å€¼
"""

import hashlib
import json
import logging
import os
import sys
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# æŠ‘åˆ¶ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning, module='pandas')
warnings.filterwarnings('ignore', category=UserWarning, module='quantaalpha')

# é…ç½® joblib ä½¿ç”¨çº¿ç¨‹åç«¯è€Œä¸æ˜¯è¿›ç¨‹åç«¯ï¼Œé¿å…å­è¿›ç¨‹å¯¼å…¥ LLM æ¨¡å—
os.environ.setdefault('JOBLIB_START_METHOD', 'loky')

logger = logging.getLogger(__name__)

# é»˜è®¤ç¼“å­˜ç›®å½•
DEFAULT_CACHE_DIR = Path("/mnt/DATA/quantagent/QuantaAlpha/factor_cache")


class CustomFactorCalculator:
    """
    è‡ªå®šä¹‰å› å­è®¡ç®—å™¨
    ç›´æ¥ä½¿ç”¨ QuantaAlpha çš„è¡¨è¾¾å¼è§£æå™¨å’Œå‡½æ•°åº“
    æ”¯æŒä»ç¼“å­˜åŠ è½½é¢„è®¡ç®—çš„å› å­å€¼
    æ”¯æŒè‡ªåŠ¨ä»ä¸»ç¨‹åºæ—¥å¿—ä¸­æå–ç¼“å­˜
    """
    
    def __init__(self, data_df: pd.DataFrame, cache_dir: Optional[Path] = None, auto_extract_cache: bool = True):
        """
        åˆå§‹åŒ–å› å­è®¡ç®—å™¨
        
        Args:
            data_df: è‚¡ç¥¨æ•°æ® DataFrameï¼Œéœ€è¦æœ‰ MultiIndex (datetime, instrument)
                    åˆ—åŒ…å«: $open, $high, $low, $close, $volume, $vwap
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ (å¯é€‰)
            auto_extract_cache: æ˜¯å¦è‡ªåŠ¨ä»ä¸»ç¨‹åºæ—¥å¿—ä¸­æå–ç¼“å­˜ (é»˜è®¤ True)
        """
        self.data_df = data_df
        self.cache_dir = cache_dir or DEFAULT_CACHE_DIR
        self.auto_extract_cache = auto_extract_cache
        self._cache_extracted = False  # æ ‡è®°æ˜¯å¦å·²æ‰§è¡Œè¿‡è‡ªåŠ¨æå–
        self._prepare_data()
        
    def _prepare_data(self):
        """å‡†å¤‡æ•°æ®ï¼Œæ·»åŠ å¸¸ç”¨è¡ç”Ÿåˆ—"""
        df = self.data_df.copy()
        
        # æ·»åŠ  $return åˆ— (å¦‚æœä¸å­˜åœ¨)
        if '$return' not in df.columns:
            df['$return'] = df.groupby('instrument')['$close'].transform(
                lambda x: x / x.shift(1) - 1
            )
        
        self.data_df = df
        logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ: {len(df)} è¡Œ, åˆ—: {list(df.columns)}")
    
    def _get_cache_key(self, expr: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”® (ä½¿ç”¨è¡¨è¾¾å¼çš„ MD5 å“ˆå¸Œ)"""
        return hashlib.md5(expr.encode()).hexdigest()
    
    def _load_from_cache(self, expr: str) -> Optional[pd.Series]:
        """
        ä»ç¼“å­˜åŠ è½½å› å­å€¼
        
        Args:
            expr: å› å­è¡¨è¾¾å¼
            
        Returns:
            Optional[pd.Series]: ç¼“å­˜çš„å› å­å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        cache_key = self._get_cache_key(expr)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            try:
                result = pd.read_pickle(cache_file)
                return self._process_cached_result(result, cache_key)
            except Exception as e:
                logger.debug(f"ç¼“å­˜åŠ è½½å¤±è´¥ [{cache_key}]: {e}")
                return None
        return None
    
    def _load_from_cache_location(self, cache_location: Dict) -> Optional[pd.Series]:
        """
        ä» cache_location å­—æ®µæŒ‡å®šçš„è·¯å¾„åŠ è½½å› å­å€¼
        
        Args:
            cache_location: ç¼“å­˜ä½ç½®ä¿¡æ¯ï¼ŒåŒ…å« result_h5_path
            
        Returns:
            Optional[pd.Series]: ç¼“å­˜çš„å› å­å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        if not cache_location:
            return None
        
        result_h5_path = cache_location.get('result_h5_path', '')
        if not result_h5_path:
            return None
        
        h5_file = Path(result_h5_path)
        if not h5_file.exists():
            logger.debug(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {result_h5_path}")
            return None
        
        try:
            # è¯»å– HDF5 æ–‡ä»¶
            result = pd.read_hdf(str(h5_file))
            return self._process_cached_result(result, result_h5_path)
        except Exception as e:
            logger.debug(f"ä» cache_location åŠ è½½å¤±è´¥ [{result_h5_path}]: {e}")
            return None
    
    def _process_cached_result(self, result: Any, source: str) -> Optional[pd.Series]:
        """
        å¤„ç†ç¼“å­˜ç»“æœï¼Œç»Ÿä¸€æ ¼å¼
        
        Args:
            result: ä»ç¼“å­˜åŠ è½½çš„åŸå§‹æ•°æ®
            source: æ•°æ®æ¥æºï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            Optional[pd.Series]: å¤„ç†åçš„å› å­å€¼
        """
        try:
            # å¤„ç†å¯èƒ½çš„ DataFrame æ ¼å¼
            if isinstance(result, pd.DataFrame):
                if len(result.columns) == 1:
                    result = result.iloc[:, 0]
                elif 'factor' in result.columns:
                    result = result['factor']
                else:
                    # å–ç¬¬ä¸€åˆ—
                    result = result.iloc[:, 0]
            
            # å¤„ç†ç´¢å¼•é¡ºåºä¸ä¸€è‡´çš„é—®é¢˜
            # ç¼“å­˜å¯èƒ½æ˜¯ (datetime, instrument)ï¼Œè€Œå›æµ‹æ•°æ®æ˜¯ (instrument, datetime)
            if isinstance(result.index, pd.MultiIndex):
                cache_idx_names = list(result.index.names)
                data_idx_names = list(self.data_df.index.names)
                
                # å¦‚æœç´¢å¼•åç§°é¡ºåºä¸åŒï¼Œè°ƒæ•´é¡ºåº
                if cache_idx_names != data_idx_names and set(cache_idx_names) == set(data_idx_names):
                    # äº¤æ¢ç´¢å¼•çº§åˆ«ä»¥åŒ¹é…ç›®æ ‡æ•°æ®
                    result = result.swaplevel()
                    result = result.sort_index()
            
            return result
        except Exception as e:
            logger.debug(f"å¤„ç†ç¼“å­˜ç»“æœå¤±è´¥ [{source}]: {e}")
            return None
    
    def _save_to_cache(self, expr: str, result: pd.Series):
        """
        ä¿å­˜å› å­å€¼åˆ°ç¼“å­˜
        
        Args:
            expr: å› å­è¡¨è¾¾å¼
            result: è®¡ç®—çš„å› å­å€¼
        """
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            cache_key = self._get_cache_key(expr)
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            result.to_pickle(cache_file)
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _auto_extract_cache_from_logs(self):
        """
        è‡ªåŠ¨ä»ä¸»ç¨‹åºæ—¥å¿—ä¸­æå–ç¼“å­˜
        åªåœ¨é¦–æ¬¡éœ€è¦æ—¶æ‰§è¡Œä¸€æ¬¡
        """
        if self._cache_extracted:
            return
        
        self._cache_extracted = True
        
        try:
            # åŠ¨æ€å¯¼å…¥ç¼“å­˜æå–å™¨
            from tools.factor_cache_extractor import extract_factors_to_cache
            
            logger.info("ğŸ”„ è‡ªåŠ¨æå–ä¸»ç¨‹åºç¼“å­˜...")
            new_count = extract_factors_to_cache(
                output_dir=self.cache_dir,
                verbose=False
            )
            if new_count > 0:
                logger.info(f"   âœ“ æ–°æå– {new_count} ä¸ªå› å­åˆ°ç¼“å­˜")
        except ImportError:
            logger.debug("ç¼“å­˜æå–å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨æå–")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨æå–ç¼“å­˜å¤±è´¥: {e}")
        
    def calculate_factor(self, factor_name: str, factor_expression: str) -> Optional[pd.Series]:
        """
        è®¡ç®—å•ä¸ªå› å­
        
        Args:
            factor_name: å› å­åç§°
            factor_expression: å› å­è¡¨è¾¾å¼
            
        Returns:
            pd.Series: å› å­å€¼ (MultiIndex: datetime, instrument)
        """
        try:
            # å¯¼å…¥è¡¨è¾¾å¼è§£æå™¨ï¼ˆé™é»˜å¯¼å…¥ï¼Œé¿å…ä¸å¿…è¦çš„æ—¥å¿—ï¼‰
            import io
            import sys as _sys
            from contextlib import redirect_stdout
            
            # é…ç½® joblib ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼ï¼Œé¿å…å­è¿›ç¨‹å¯¼å…¥é—®é¢˜
            from joblib import parallel_backend
            
            from quantaalpha.components.coder.factor_coder.expr_parser import (
                parse_expression, parse_symbol
            )
            # å¯¼å…¥å‡½æ•°åº“
            import quantaalpha.components.coder.factor_coder.function_lib as func_lib
            
            # å¤åˆ¶æ•°æ®
            df = self.data_df.copy()
            
            # è§£æè¡¨è¾¾å¼ï¼ˆæŠ‘åˆ¶ parse_expression çš„æ‰“å°è¾“å‡ºï¼‰
            expr = parse_symbol(factor_expression, df.columns)
            
            # é™é»˜è§£æï¼ˆæŠ‘åˆ¶ print è¾“å‡ºï¼‰
            old_stdout = _sys.stdout
            _sys.stdout = io.StringIO()
            try:
                expr = parse_expression(expr)
            finally:
                _sys.stdout = old_stdout
            
            # æ›¿æ¢å˜é‡ä¸º DataFrame åˆ—å¼•ç”¨
            for col in df.columns:
                if col.startswith('$'):
                    expr = expr.replace(col[1:], f"df['{col}']")
            
            # æ„å»ºæ‰§è¡Œç¯å¢ƒ
            exec_globals = {
                'df': df,
                'np': np,
                'pd': pd,
            }
            
            # æ·»åŠ æ‰€æœ‰å‡½æ•°åº“ä¸­çš„å‡½æ•°
            for name in dir(func_lib):
                if not name.startswith('_'):
                    obj = getattr(func_lib, name)
                    if callable(obj):
                        exec_globals[name] = obj
            
            # ä½¿ç”¨çº¿ç¨‹åç«¯è¿›è¡Œè®¡ç®—ï¼Œé¿å…å­è¿›ç¨‹å¯¼å…¥ LLM æ¨¡å—
            # æ³¨æ„ï¼šåœ¨lokyå¤šè¿›ç¨‹åç«¯ä¸­ï¼Œè¿™é‡ŒåµŒå¥—ä½¿ç”¨ threading æ˜¯å®‰å…¨çš„
            # from joblib import parallel_backend
            # with parallel_backend('threading', n_jobs=1):
            #     # è®¡ç®—å› å­å€¼
            #     result = eval(expr, exec_globals)
            result = eval(expr, exec_globals)
            
            if isinstance(result, pd.DataFrame):
                result = result.iloc[:, 0]
            
            if isinstance(result, pd.Series):
                result.name = factor_name
                # ç¡®ä¿ç»“æœä¸åŸå§‹æ•°æ®æœ‰ç›¸åŒçš„ç´¢å¼•
                if not result.index.equals(df.index):
                    result = result.reindex(df.index)
                return result.astype(np.float64)
            else:
                # å¦‚æœç»“æœæ˜¯æ ‡é‡æˆ–æ•°ç»„ï¼Œè½¬æ¢ä¸º Series
                return pd.Series(result, index=df.index, name=factor_name).astype(np.float64)
                
        except Exception as e:
            logger.warning(f"å› å­è®¡ç®—å¤±è´¥ [{factor_name}]: {str(e)[:200]}")
            return None
    
    def calculate_factors_from_json(self, json_path: str, 
                                   max_factors: Optional[int] = None) -> pd.DataFrame:
        """
        ä» JSON æ–‡ä»¶æ‰¹é‡è®¡ç®—å› å­
        
        Args:
            json_path: å› å­ JSON æ–‡ä»¶è·¯å¾„
            max_factors: æœ€å¤§å› å­æ•°é‡é™åˆ¶
            
        Returns:
            pd.DataFrame: è®¡ç®—å¾—åˆ°çš„å› å­å€¼ DataFrame
        """
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        factors = data.get('factors', {})
        
        results = {}
        success_count = 0
        fail_count = 0
        
        factor_items = list(factors.items())
        if max_factors:
            factor_items = factor_items[:max_factors]
        
        total = len(factor_items)
        logger.info(f"å¼€å§‹è®¡ç®— {total} ä¸ªå› å­...")
        
        for i, (factor_id, factor_info) in enumerate(factor_items):
            factor_name = factor_info.get('factor_name', factor_id)
            factor_expr = factor_info.get('factor_expression', '')
            
            if not factor_expr:
                fail_count += 1
                continue
            
            if (i + 1) % 10 == 0 or i == 0:
                logger.info(f"  è¿›åº¦: {i+1}/{total}")
            
            result = self.calculate_factor(factor_name, factor_expr)
            
            if result is not None:
                results[factor_name] = result
                success_count += 1
            else:
                fail_count += 1
        
        logger.info(f"å› å­è®¡ç®—å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        
        if results:
            return pd.DataFrame(results)
        return pd.DataFrame()
    
    def calculate_factors_batch(self, factors: List[Dict], use_cache: bool = True, n_jobs: int = 1) -> pd.DataFrame:
        """
        æ‰¹é‡è®¡ç®—å› å­ (å…¨å¹¶è¡Œä¼˜åŒ–ç‰ˆ)
        """
        if use_cache and self.auto_extract_cache:
            self._auto_extract_cache_from_logs()
        
        logger.info(f"å¼€å§‹å¤„ç† {len(factors)} ä¸ªå› å­ (å¹¶è¡Œåº¦: {n_jobs})...")
        
        from joblib import Parallel, delayed
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ (ç¼“å­˜æ£€æŸ¥ + è®¡ç®—)
        # ä½¿ç”¨ loky åç«¯ (å¤šè¿›ç¨‹) ä»¥é¿å… Python GIL å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆ
        # æ³¨æ„: éœ€è¦å°†å†…éƒ¨å‡½æ•°æ”¹ä¸ºç±»æ–¹æ³•ä»¥ä¾¿ pickling
        parallel_results = Parallel(n_jobs=n_jobs, backend='loky')(
            delayed(self._process_single_factor)(info, use_cache) for info in factors
        )
        
        results = {}
        success_count = 0
        fail_count = 0
        stats = {"cache_location": 0, "md5_cache": 0, "computed": 0, "failed": 0, "empty_expr": 0}
        
        for name, expr, result, source in parallel_results:
            stats[source] = stats.get(source, 0) + 1
            
            if result is not None and len(result) > 0:
                if not result.isna().all():
                    results[name] = result
                    success_count += 1
                    # å¦‚æœæ˜¯æ–°è®¡ç®—çš„ï¼Œä¿å­˜åˆ°ç¼“å­˜
                    if source == "computed" and use_cache:
                        self._save_to_cache(expr, result)
                else:
                    fail_count += 1
                    logger.warning(f"    âœ— å› å­ {name} å…¨ä¸º NaN")
            elif source != "empty_expr":
                fail_count += 1
                if source == "computed":
                    logger.warning(f"    âœ— å› å­ {name} è®¡ç®—å¤±è´¥")
        
        logger.info(f"å› å­å¤„ç†å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        logger.info(f"  ç»Ÿè®¡: {stats}")
        
        if results:
            result_df = pd.DataFrame(results, index=self.data_df.index)
            logger.info(f"  ç»“æœ DataFrame: {result_df.shape}")
            return result_df
        
        return pd.DataFrame()

    def _process_single_factor(self, factor_info, use_cache):
        """å¤„ç†å•ä¸ªå› å­ (æ”¯æŒå¹¶è¡Œè°ƒç”¨)"""
        import logging
        import sys
        
        # é…ç½®ç®€å•çš„ loggerï¼Œè¾“å‡ºåˆ° stderr
        logging.basicConfig(level=logging.INFO, stream=sys.stderr, format='%(asctime)s - %(message)s')
        logger = logging.getLogger(f"worker_{os.getpid()}")
        
        factor_name = factor_info.get('factor_name', 'unknown')
        factor_expr = factor_info.get('factor_expression', '')
        cache_location = factor_info.get('cache_location')
        
        if not factor_expr:
            return factor_name, factor_expr, None, "empty_expr"
        
        # 1. ä¼˜å…ˆæ£€æŸ¥ cache_location
        if use_cache and cache_location:
            try:
                result = self._load_from_cache_location(cache_location)
                if result is not None:
                    result = self._validate_and_align_result(result, factor_name)
                    if result is not None:
                        return factor_name, factor_expr, result, "cache_location"
            except Exception:
                pass
        
        # 2. æ£€æŸ¥ MD5 ç¼“å­˜
        if use_cache:
            try:
                result = self._load_from_cache(factor_expr)
                if result is not None:
                    result = self._validate_and_align_result(result, factor_name)
                    if result is not None:
                        return factor_name, factor_expr, result, "md5_cache"
            except Exception:
                pass
        
        # 3. è®¡ç®—å› å­
        logger.info(f"    è®¡ç®—: {factor_name}")
        try:
            # é‡æ–°å¯¼å…¥å¿…è¦çš„æ¨¡å—ä»¥ç¡®ä¿åœ¨å­è¿›ç¨‹ä¸­å¯ç”¨
            import numpy as np
            import pandas as pd
            import quantaalpha.components.coder.factor_coder.function_lib as func_lib
            
            # ä½¿ç”¨ eval è®¡ç®—ï¼Œä¸ä½¿ç”¨ self.calculate_factor å› ä¸ºå®ƒåŒ…å«äº†å¯èƒ½æ— æ³•åºåˆ—åŒ–çš„å¯¹è±¡
            
            # å¤åˆ¶æ•°æ®
            df = self.data_df.copy()
            
            # å¯¼å…¥è¡¨è¾¾å¼è§£æå™¨
            from quantaalpha.components.coder.factor_coder.expr_parser import (
                parse_expression, parse_symbol
            )
            import io
            import sys as _sys
            
            # è§£æè¡¨è¾¾å¼
            expr = parse_symbol(factor_expr, df.columns)
            
            # é™é»˜è§£æ
            old_stdout = _sys.stdout
            _sys.stdout = io.StringIO()
            try:
                expr = parse_expression(expr)
            finally:
                _sys.stdout = old_stdout
            
            # æ›¿æ¢å˜é‡ä¸º DataFrame åˆ—å¼•ç”¨
            for col in df.columns:
                if col.startswith('$'):
                    expr = expr.replace(col[1:], f"df['{col}']")
            
            # æ„å»ºæ‰§è¡Œç¯å¢ƒ
            exec_globals = {
                'df': df,
                'np': np,
                'pd': pd,
            }
            
            # æ·»åŠ æ‰€æœ‰å‡½æ•°åº“ä¸­çš„å‡½æ•°
            for name in dir(func_lib):
                if not name.startswith('_'):
                    obj = getattr(func_lib, name)
                    if callable(obj):
                        exec_globals[name] = obj
            
            result = eval(expr, exec_globals)
            
            if isinstance(result, pd.DataFrame):
                result = result.iloc[:, 0]
            
            if isinstance(result, pd.Series):
                result.name = factor_name
                # ç¡®ä¿ç»“æœä¸åŸå§‹æ•°æ®æœ‰ç›¸åŒçš„ç´¢å¼•
                if not result.index.equals(df.index):
                    result = result.reindex(df.index)
                result = result.astype(np.float64)
            else:
                # å¦‚æœç»“æœæ˜¯æ ‡é‡æˆ–æ•°ç»„ï¼Œè½¬æ¢ä¸º Series
                result = pd.Series(result, index=df.index, name=factor_name).astype(np.float64)
            
            return factor_name, factor_expr, result, "computed"
        except Exception as e:
            # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
            import traceback
            error_msg = f"{str(e)}\n{traceback.format_exc()}"
            # logger åœ¨å­è¿›ç¨‹ä¸­å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œæ‰“å°åˆ°æ ‡å‡†é”™è¯¯
            import sys
            print(f"Error calculating {factor_name}: {error_msg}", file=sys.stderr)
            return factor_name, factor_expr, None, "failed"
    
    def _validate_and_align_result(self, result: pd.Series, factor_name: str) -> Optional[pd.Series]:
        """
        éªŒè¯å¹¶å¯¹é½ç¼“å­˜ç»“æœçš„ç´¢å¼•
        
        Args:
            result: ç¼“å­˜åŠ è½½çš„å› å­å€¼
            factor_name: å› å­åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            Optional[pd.Series]: å¯¹é½åçš„ç»“æœï¼Œå¦‚æœéªŒè¯å¤±è´¥åˆ™è¿”å› None
        """
        if result is None:
            return None
        
        # ç¡®ä¿ç´¢å¼•å¯¹é½
        if not result.index.equals(self.data_df.index):
            try:
                # å°è¯•å¯¹é½ç´¢å¼• - ç¼“å­˜å¯èƒ½åŒ…å«æ›´å¤šè‚¡ç¥¨/æ—¥æœŸ
                common_idx = result.index.intersection(self.data_df.index)
                if len(common_idx) > len(self.data_df.index) * 0.5:  # è‡³å°‘50%åŒ¹é…
                    result = result.reindex(self.data_df.index)
                    logger.debug(f"    ç´¢å¼•å¯¹é½: å…±åŒç´¢å¼• {len(common_idx)}, ç›®æ ‡ {len(self.data_df.index)}")
                else:
                    logger.warning(f"    âš  ç¼“å­˜ç´¢å¼•åŒ¹é…ç‡è¿‡ä½ ({len(common_idx)}/{len(self.data_df.index)}), å°†é‡æ–°è®¡ç®—")
                    return None
            except Exception as e:
                logger.warning(f"    âš  ç´¢å¼•å¯¹é½å¤±è´¥: {e}, å°†é‡æ–°è®¡ç®—")
                return None
        
        # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
        if result is None or len(result) == 0 or result.isna().all():
            return None
        
        return result


class CustomFactorDataLoader:
    """
    è‡ªå®šä¹‰å› å­æ•°æ®åŠ è½½å™¨
    å°†è®¡ç®—å¥½çš„å› å­å€¼è½¬æ¢ä¸º Qlib å¯ç”¨çš„æ ¼å¼
    """
    
    def __init__(self, factor_df: pd.DataFrame, label_expr: str = "Ref($close, -2) / Ref($close, -1) - 1"):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            factor_df: å› å­å€¼ DataFrame (MultiIndex: datetime, instrument)
            label_expr: æ ‡ç­¾è¡¨è¾¾å¼
        """
        self.factor_df = factor_df
        self.label_expr = label_expr
        
    def to_qlib_format(self, data_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        è½¬æ¢ä¸º Qlib æ•°æ®æ ¼å¼
        
        Args:
            data_df: åŸå§‹ä»·æ ¼æ•°æ®
            
        Returns:
            Tuple[features_df, labels_df]
        """
        # è®¡ç®—æ ‡ç­¾
        from quantaalpha.components.coder.factor_coder.expr_parser import (
            parse_expression, parse_symbol
        )
        import quantaalpha.components.coder.factor_coder.function_lib as func_lib
        
        df = data_df.copy()
        
        # è§£ææ ‡ç­¾è¡¨è¾¾å¼
        expr = parse_symbol(self.label_expr, df.columns)
        expr = parse_expression(expr)
        
        for col in df.columns:
            if col.startswith('$'):
                expr = expr.replace(col[1:], f"df['{col}']")
        
        exec_globals = {'df': df, 'np': np, 'pd': pd}
        for name in dir(func_lib):
            if not name.startswith('_'):
                obj = getattr(func_lib, name)
                if callable(obj):
                    exec_globals[name] = obj
        
        label = eval(expr, exec_globals)
        if isinstance(label, pd.DataFrame):
            label = label.iloc[:, 0]
        
        labels_df = pd.DataFrame({'LABEL0': label})
        
        return self.factor_df, labels_df


def get_qlib_stock_data(config: Dict) -> pd.DataFrame:
    """
    ä» Qlib è·å–è‚¡ç¥¨æ•°æ®
    
    Args:
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å« data é…ç½®
        
    Returns:
        pd.DataFrame: è‚¡ç¥¨æ•°æ®
    """
    import qlib
    from qlib.data import D
    
    data_config = config.get('data', {})
    
    provider_uri = data_config.get('provider_uri', '/home/tjxy/.qlib/qlib_data/cn_data')
    
    # åˆå§‹åŒ– Qlib (å¦‚æœå°šæœªåˆå§‹åŒ–)
    try:
        qlib.init(provider_uri=provider_uri, region='cn')
    except Exception:
        pass  # å·²ç»åˆå§‹åŒ–
    
    start_time = data_config.get('start_time', '2016-01-01')
    end_time = data_config.get('end_time', '2025-12-31')
    market = data_config.get('market', 'csi300')
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    stock_list = D.instruments(market)
    
    # è·å–æ•°æ®
    fields = ['$open', '$high', '$low', '$close', '$volume', '$vwap']
    df = D.features(
        stock_list,
        fields,
        start_time=start_time,
        end_time=end_time,
        freq='day'
    )
    
    df.columns = fields
    
    logger.info(f"âœ“ åŠ è½½è‚¡ç¥¨æ•°æ®: {len(df)} è¡Œ")
    
    return df


if __name__ == '__main__':
    """æµ‹è¯•å› å­è®¡ç®—"""
    import yaml
    
    logging.basicConfig(level=logging.INFO)
    
    # åŠ è½½é…ç½®
    config_path = Path(__file__).parent / 'config.yaml'
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è·å–æ•°æ®
    print("è·å–è‚¡ç¥¨æ•°æ®...")
    data_df = get_qlib_stock_data(config)
    
    # åˆ›å»ºè®¡ç®—å™¨
    calculator = CustomFactorCalculator(data_df)
    
    # æµ‹è¯•å•ä¸ªå› å­
    test_expr = "RANK(-1 * TS_PCTCHANGE($close, 10))"
    print(f"\næµ‹è¯•è¡¨è¾¾å¼: {test_expr}")
    
    result = calculator.calculate_factor("test_factor", test_expr)
    if result is not None:
        print(f"è®¡ç®—æˆåŠŸ! ç»“æœå½¢çŠ¶: {result.shape}")
        print(result.head())
    else:
        print("è®¡ç®—å¤±è´¥!")

