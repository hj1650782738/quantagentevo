#!/usr/bin/env python3
"""
å› å­å†—ä½™åº¦å¯è§†åŒ–å·¥å…·

åŸºäº AST ç»“æ„è®¡ç®—å› å­ä¹‹é—´çš„ç›¸ä¼¼åº¦/è·ç¦»ï¼Œç„¶åä½¿ç”¨é™ç»´ç®—æ³•ï¼ˆMDS/t-SNEï¼‰
å°†å› å­æ˜ å°„åˆ° 2D ç©ºé—´ï¼Œç”Ÿæˆæ•£ç‚¹å›¾æ¥å±•ç¤ºå› å­çš„å†—ä½™ç¨‹åº¦ã€‚

è¶Šèšé›†çš„ç‚¹è¡¨ç¤ºå› å­ä¹‹é—´å†—ä½™åº¦è¶Šé«˜ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    python tools/visualize_factor_redundancy.py factor_ast_output.json --output redundancy_plot.html
"""

import json
import sys
import os
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from alphaagent.components.coder.factor_coder.factor_ast import (
    parse_expression, Node, VarNode, NumberNode, FunctionNode, 
    BinaryOpNode, ConditionalNode, UnaryOpNode,
    find_largest_common_subtree, count_nodes
)


def get_subtree_size(node: Node) -> int:
    """è®¡ç®—å­æ ‘å¤§å°"""
    if isinstance(node, (NumberNode, VarNode)):
        return 1
    elif isinstance(node, FunctionNode):
        return 1 + sum(get_subtree_size(arg) for arg in node.args)
    elif isinstance(node, BinaryOpNode):
        return 1 + get_subtree_size(node.left) + get_subtree_size(node.right)
    elif isinstance(node, ConditionalNode):
        return 1 + get_subtree_size(node.condition) + \
               get_subtree_size(node.true_expr) + \
               get_subtree_size(node.false_expr)
    elif isinstance(node, UnaryOpNode):
        return 1 + get_subtree_size(node.operand)
    return 0


def calculate_similarity(expr1: str, expr2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå› å­è¡¨è¾¾å¼çš„ç›¸ä¼¼åº¦
    
    ç›¸ä¼¼åº¦ = æœ€å¤§å…¬å…±å­æ ‘å¤§å° / min(æ ‘1å¤§å°, æ ‘2å¤§å°)
    
    è¿”å›å€¼åœ¨ [0, 1] ä¹‹é—´ï¼Œ1 è¡¨ç¤ºå®Œå…¨ç›¸åŒ
    """
    try:
        tree1 = parse_expression(expr1)
        tree2 = parse_expression(expr2)
        
        size1 = get_subtree_size(tree1)
        size2 = get_subtree_size(tree2)
        
        if size1 == 0 or size2 == 0:
            return 0.0
        
        match = find_largest_common_subtree(tree1, tree2)
        
        if match is None:
            return 0.0
        
        # ä½¿ç”¨ Jaccard-like ç›¸ä¼¼åº¦
        min_size = min(size1, size2)
        similarity = match.size / min_size
        
        return min(similarity, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1
        
    except Exception as e:
        return 0.0


def calculate_distance(similarity: float) -> float:
    """å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºè·ç¦»"""
    return 1.0 - similarity


def build_distance_matrix(factors: List[Tuple[str, str, str]], 
                          verbose: bool = True) -> Tuple[np.ndarray, List[str], List[str]]:
    """
    æ„å»ºå› å­ä¹‹é—´çš„è·ç¦»çŸ©é˜µ
    
    Args:
        factors: [(factor_id, factor_name, factor_expression), ...]
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
        
    Returns:
        (è·ç¦»çŸ©é˜µ, å› å­IDåˆ—è¡¨, å› å­åç§°åˆ—è¡¨)
    """
    n = len(factors)
    distance_matrix = np.zeros((n, n))
    factor_ids = [f[0] for f in factors]
    factor_names = [f[1] for f in factors]
    expressions = [f[2] for f in factors]
    
    total_pairs = n * (n - 1) // 2
    computed = 0
    
    if verbose:
        print(f"ğŸ“Š è®¡ç®— {n} ä¸ªå› å­ä¹‹é—´çš„è·ç¦»çŸ©é˜µ ({total_pairs} å¯¹)...")
    
    for i in range(n):
        for j in range(i + 1, n):
            similarity = calculate_similarity(expressions[i], expressions[j])
            distance = calculate_distance(similarity)
            distance_matrix[i, j] = distance
            distance_matrix[j, i] = distance
            
            computed += 1
            if verbose and computed % 500 == 0:
                print(f"  è¿›åº¦: {computed}/{total_pairs} ({100*computed/total_pairs:.1f}%)")
    
    if verbose:
        print(f"âœ… è·ç¦»çŸ©é˜µè®¡ç®—å®Œæˆï¼")
    
    return distance_matrix, factor_ids, factor_names


def reduce_to_2d(distance_matrix: np.ndarray, method: str = 'mds') -> np.ndarray:
    """
    å°†è·ç¦»çŸ©é˜µé™ç»´åˆ° 2D
    
    Args:
        distance_matrix: è·ç¦»çŸ©é˜µ
        method: 'mds' æˆ– 'tsne'
        
    Returns:
        2D åæ ‡æ•°ç»„ (n, 2)
    """
    print(f"ğŸ”„ ä½¿ç”¨ {method.upper()} è¿›è¡Œé™ç»´...")
    
    if method == 'mds':
        from sklearn.manifold import MDS
        mds = MDS(n_components=2, dissimilarity='precomputed', 
                  random_state=42, n_init=4, max_iter=300)
        coords = mds.fit_transform(distance_matrix)
    elif method == 'tsne':
        from sklearn.manifold import TSNE
        # t-SNE éœ€è¦å…ˆè½¬æ¢ä¸ºç›¸ä¼¼åº¦çŸ©é˜µæˆ–ä½¿ç”¨ metric='precomputed'
        tsne = TSNE(n_components=2, metric='precomputed', 
                    random_state=42, perplexity=min(30, len(distance_matrix)-1))
        coords = tsne.fit_transform(distance_matrix)
    else:
        raise ValueError(f"Unknown method: {method}")
    
    print(f"âœ… é™ç»´å®Œæˆï¼")
    return coords


def create_interactive_plot(coords: np.ndarray, 
                           factor_ids: List[str],
                           factor_names: List[str],
                           factor_expressions: List[str],
                           statistics: Optional[List[Dict]] = None,
                           cluster_labels: Optional[List[int]] = None,
                           output_path: str = 'redundancy_plot.html'):
    """
    åˆ›å»ºäº¤äº’å¼æ•£ç‚¹å›¾ (ä½¿ç”¨ Plotly)
    """
    try:
        import plotly.graph_objects as go
        import plotly.express as px
    except ImportError:
        print("âš ï¸ éœ€è¦å®‰è£… plotly: pip install plotly")
        return None
    
    # å‡†å¤‡ hover æ–‡æœ¬
    hover_texts = []
    for i, (fid, fname, expr) in enumerate(zip(factor_ids, factor_names, factor_expressions)):
        text = f"<b>{fname}</b><br>"
        text += f"ID: {fid[:16]}...<br>"
        text += f"è¡¨è¾¾å¼: {expr[:80]}..."
        if statistics and statistics[i]:
            stats = statistics[i]
            text += f"<br>èŠ‚ç‚¹æ•°: {stats.get('total_nodes', 'N/A')}"
            text += f"<br>ASTæ·±åº¦: {stats.get('tree_depth', 'N/A')}"
            text += f"<br>å‡½æ•°æ•°: {stats.get('function_count', 'N/A')}"
            text += f"<br>å˜é‡æ•°: {stats.get('variable_count', 'N/A')}"
        if cluster_labels is not None:
            text += f"<br>èšç±»: {cluster_labels[i]}"
        hover_texts.append(text)
    
    # ä½¿ç”¨èŠ‚ç‚¹æ•°ä½œä¸ºç‚¹å¤§å°ï¼ˆå¦‚æœæœ‰ç»Ÿè®¡ä¿¡æ¯ï¼‰
    if statistics:
        sizes = [s.get('total_nodes', 10) if s else 10 for s in statistics]
        # å½’ä¸€åŒ–å¤§å°
        min_s, max_s = min(sizes), max(sizes)
        if max_s > min_s:
            sizes = [8 + 20 * (s - min_s) / (max_s - min_s) for s in sizes]
        else:
            sizes = [12] * len(sizes)
    else:
        sizes = [12] * len(coords)
    
    # ä½¿ç”¨èšç±»æ ‡ç­¾æˆ–ASTæ·±åº¦ä½œä¸ºé¢œè‰²
    if cluster_labels is not None:
        colors = cluster_labels
        colorscale = 'Rainbow'
        colorbar_title = 'èšç±»ç¼–å·'
    elif statistics:
        colors = [s.get('tree_depth', 5) if s else 5 for s in statistics]
        colorscale = 'Viridis'
        colorbar_title = 'AST æ·±åº¦'
    else:
        colors = coords[:, 0]
        colorscale = 'Viridis'
        colorbar_title = 'ä½ç½®'
    
    # åˆ›å»ºæ•£ç‚¹å›¾
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(
        x=coords[:, 0],
        y=coords[:, 1],
        mode='markers',
        marker=dict(
            size=sizes,
            color=colors,
            colorscale=colorscale,
            opacity=0.75,
            line=dict(width=1, color='white'),
            colorbar=dict(title=colorbar_title)
        ),
        text=hover_texts,
        hoverinfo='text',
        name='å› å­'
    ))
    
    fig.update_layout(
        title=dict(
            text='å› å­å†—ä½™åº¦æ•£ç‚¹å›¾<br><sup>è·ç¦»è¶Šè¿‘ = ASTç»“æ„è¶Šç›¸ä¼¼ = å†—ä½™åº¦è¶Šé«˜ | ç‚¹å¤§å° = èŠ‚ç‚¹æ•° | é¢œè‰² = èšç±»</sup>',
            font=dict(size=18)
        ),
        xaxis_title='MDS ç»´åº¦ 1 (ä¿æŒè·ç¦»å…³ç³»çš„æŠ•å½±åæ ‡)',
        yaxis_title='MDS ç»´åº¦ 2 (ä¿æŒè·ç¦»å…³ç³»çš„æŠ•å½±åæ ‡)',
        template='plotly_dark',
        width=1200,
        height=800,
        hovermode='closest',
        annotations=[
            dict(
                text="ğŸ’¡ æç¤ºï¼šç»´åº¦1/2 æ˜¯å°†é«˜ç»´è·ç¦»çŸ©é˜µé™ç»´åˆ°2Dçš„æŠ•å½±åæ ‡ï¼Œ<br>æœ¬èº«æ— å…·ä½“ç‰©ç†å«ä¹‰ï¼Œä½†ä¿æŒäº†å› å­é—´çš„ç›¸å¯¹è·ç¦»å…³ç³»",
                xref="paper", yref="paper",
                x=0.01, y=-0.08,
                showarrow=False,
                font=dict(size=11, color='gray')
            )
        ]
    )
    
    # ä¿å­˜ä¸º HTML
    fig.write_html(output_path)
    print(f"ğŸ“Š äº¤äº’å¼å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    return fig


def create_matplotlib_plot(coords: np.ndarray,
                          factor_names: List[str],
                          statistics: Optional[List[Dict]] = None,
                          output_path: str = 'redundancy_plot.png',
                          show_labels: bool = False):
    """
    åˆ›å»ºé™æ€æ•£ç‚¹å›¾ (ä½¿ç”¨ Matplotlib)
    """
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
    
    fig, ax = plt.subplots(figsize=(14, 10))
    
    # ä½¿ç”¨èŠ‚ç‚¹æ•°ä½œä¸ºç‚¹å¤§å°
    if statistics:
        sizes = [s.get('total_nodes', 30) * 3 if s else 30 for s in statistics]
    else:
        sizes = [50] * len(coords)
    
    # ä½¿ç”¨æ·±åº¦ä½œä¸ºé¢œè‰²
    if statistics:
        colors = [s.get('tree_depth', 5) if s else 5 for s in statistics]
    else:
        colors = coords[:, 0]
    
    scatter = ax.scatter(coords[:, 0], coords[:, 1], 
                        c=colors, s=sizes, 
                        alpha=0.6, cmap='viridis',
                        edgecolors='white', linewidths=0.5)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('AST æ·±åº¦', fontsize=12)
    
    # å¯é€‰ï¼šæ·»åŠ æ ‡ç­¾
    if show_labels and len(factor_names) <= 50:
        for i, name in enumerate(factor_names):
            ax.annotate(name[:15], (coords[i, 0], coords[i, 1]),
                       fontsize=6, alpha=0.7)
    
    ax.set_xlabel('ç»´åº¦ 1', fontsize=12)
    ax.set_ylabel('ç»´åº¦ 2', fontsize=12)
    ax.set_title('å› å­å†—ä½™åº¦æ•£ç‚¹å›¾\nï¼ˆè·ç¦»è¶Šè¿‘è¡¨ç¤ºå†—ä½™åº¦è¶Šé«˜ï¼Œç‚¹å¤§å°=èŠ‚ç‚¹æ•°ï¼Œé¢œè‰²=æ·±åº¦ï¼‰', 
                 fontsize=14, fontweight='bold')
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3)
    ax.set_facecolor('#f8f9fa')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š é™æ€å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    return fig


def load_factor_ast_data(input_path: str) -> Tuple[List[Tuple[str, str, str]], List[Dict]]:
    """
    ä» AST æå–ç»“æœæ–‡ä»¶åŠ è½½å› å­æ•°æ®
    
    Returns:
        (factors_list, statistics_list)
        factors_list: [(factor_id, factor_name, factor_expression), ...]
        statistics_list: [stats_dict, ...]
    """
    print(f"ğŸ“– åŠ è½½å› å­ AST æ•°æ®: {input_path}")
    
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    factors = []
    statistics = []
    
    # æ£€æŸ¥æ˜¯ AST-only æ ¼å¼è¿˜æ˜¯å®Œæ•´å› å­åº“æ ¼å¼
    if 'factor_asts' in data:
        # AST-only æ ¼å¼
        factor_asts = data['factor_asts']
        for factor_id, factor_data in factor_asts.items():
            if not factor_data.get('parse_success', False):
                continue
            
            factors.append((
                factor_id,
                factor_data.get('factor_name', factor_id),
                factor_data.get('factor_expression', '')
            ))
            statistics.append(factor_data.get('statistics', {}))
    
    elif 'factors' in data:
        # å®Œæ•´å› å­åº“æ ¼å¼ï¼ˆå¸¦æˆ–ä¸å¸¦ ASTï¼‰
        for factor_id, factor_data in data['factors'].items():
            expr = factor_data.get('factor_expression', '')
            if not expr:
                continue
            
            factors.append((
                factor_id,
                factor_data.get('factor_name', factor_id),
                expr
            ))
            
            # å¦‚æœæœ‰ AST ç»Ÿè®¡ä¿¡æ¯
            if 'factor_ast' in factor_data:
                statistics.append(factor_data['factor_ast'].get('statistics', {}))
            else:
                statistics.append({})
    
    print(f"âœ… åŠ è½½äº† {len(factors)} ä¸ªæœ‰æ•ˆå› å­")
    return factors, statistics


def save_distance_matrix(distance_matrix: np.ndarray,
                        factor_ids: List[str],
                        factor_names: List[str],
                        output_path: str,
                        format: str = 'json') -> str:
    """
    ä¿å­˜å› å­è·ç¦»çŸ©é˜µ
    
    Args:
        distance_matrix: è·ç¦»çŸ©é˜µ (n x n)
        factor_ids: å› å­IDåˆ—è¡¨
        factor_names: å› å­åç§°åˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
        format: è¾“å‡ºæ ¼å¼ ('json', 'csv', 'both')
        
    Returns:
        å®é™…ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    n = len(factor_ids)
    
    if format in ('json', 'both'):
        # JSON æ ¼å¼ï¼šå®Œæ•´çš„ç»“æ„åŒ–æ•°æ®
        json_path = output_path if output_path.endswith('.json') else output_path + '.json'
        
        # æ„å»ºè¯¦ç»†çš„è·ç¦»æ•°æ®
        matrix_data = {
            "metadata": {
                "total_factors": n,
                "total_pairs": n * (n - 1) // 2,
                "distance_metric": "1 - (LCS_size / min_tree_size)",
                "description": "è·ç¦»è¶Šå°è¡¨ç¤ºå› å­ASTç»“æ„è¶Šç›¸ä¼¼ï¼ˆå†—ä½™åº¦è¶Šé«˜ï¼‰"
            },
            "factors": [
                {"id": fid, "name": fname, "index": i}
                for i, (fid, fname) in enumerate(zip(factor_ids, factor_names))
            ],
            "distance_matrix": distance_matrix.tolist(),
            "pairwise_distances": []
        }
        
        # æ·»åŠ é…å¯¹è·ç¦»åˆ—è¡¨ï¼ˆæ–¹ä¾¿æŸ¥è¯¢ï¼‰
        for i in range(n):
            for j in range(i + 1, n):
                matrix_data["pairwise_distances"].append({
                    "factor1_id": factor_ids[i],
                    "factor1_name": factor_names[i],
                    "factor2_id": factor_ids[j],
                    "factor2_name": factor_names[j],
                    "distance": float(distance_matrix[i, j]),
                    "similarity": float(1 - distance_matrix[i, j])
                })
        
        # æŒ‰è·ç¦»æ’åºï¼ˆæœ€ç›¸ä¼¼çš„åœ¨å‰é¢ï¼‰
        matrix_data["pairwise_distances"].sort(key=lambda x: x["distance"])
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(matrix_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š è·ç¦»çŸ©é˜µ (JSON) å·²ä¿å­˜åˆ°: {json_path}")
    
    if format in ('csv', 'both'):
        # CSV æ ¼å¼ï¼šçŸ©é˜µè¡¨æ ¼å½¢å¼
        csv_path = output_path if output_path.endswith('.csv') else output_path.replace('.json', '') + '.csv'
        
        import csv
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # è¡¨å¤´ï¼šç©ºæ ¼ + å› å­åç§°
            header = [''] + [f"{fname[:30]}" for fname in factor_names]
            writer.writerow(header)
            
            # æ¯è¡Œï¼šå› å­åç§° + è·ç¦»å€¼
            for i, fname in enumerate(factor_names):
                row = [fname[:30]] + [f"{distance_matrix[i, j]:.4f}" for j in range(n)]
                writer.writerow(row)
        
        print(f"ğŸ“Š è·ç¦»çŸ©é˜µ (CSV) å·²ä¿å­˜åˆ°: {csv_path}")
        
        # é¢å¤–è¾“å‡ºé…å¯¹åˆ—è¡¨ CSV
        pairs_csv_path = csv_path.replace('.csv', '_pairs.csv')
        with open(pairs_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['factor1_id', 'factor1_name', 'factor2_id', 'factor2_name', 'distance', 'similarity'])
            
            pairs = []
            for i in range(n):
                for j in range(i + 1, n):
                    pairs.append((
                        factor_ids[i], factor_names[i],
                        factor_ids[j], factor_names[j],
                        distance_matrix[i, j], 1 - distance_matrix[i, j]
                    ))
            
            # æŒ‰è·ç¦»æ’åº
            pairs.sort(key=lambda x: x[4])
            for pair in pairs:
                writer.writerow([pair[0], pair[1], pair[2], pair[3], f"{pair[4]:.4f}", f"{pair[5]:.4f}"])
        
        print(f"ğŸ“Š é…å¯¹è·ç¦»åˆ—è¡¨ (CSV) å·²ä¿å­˜åˆ°: {pairs_csv_path}")
    
    return output_path


def analyze_clusters(coords: np.ndarray, factor_names: List[str], 
                    n_clusters: int = 5) -> Dict[str, Any]:
    """
    å¯¹é™ç»´åçš„åæ ‡è¿›è¡Œèšç±»åˆ†æ
    """
    from sklearn.cluster import KMeans
    
    print(f"ğŸ” è¿›è¡Œ {n_clusters} èšç±»åˆ†æ...")
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(coords)
    
    # ç»Ÿè®¡æ¯ä¸ªç°‡çš„ä¿¡æ¯
    clusters = {}
    for i in range(n_clusters):
        cluster_indices = np.where(labels == i)[0]
        cluster_factors = [factor_names[j] for j in cluster_indices]
        
        # è®¡ç®—ç°‡å†…å¹³å‡è·ç¦»ï¼ˆç´§å¯†åº¦ï¼‰
        cluster_coords = coords[cluster_indices]
        center = cluster_coords.mean(axis=0)
        avg_dist = np.mean(np.sqrt(np.sum((cluster_coords - center) ** 2, axis=1)))
        
        clusters[f"cluster_{i}"] = {
            "size": len(cluster_factors),
            "factors": cluster_factors[:10],  # åªæ˜¾ç¤ºå‰10ä¸ª
            "compactness": float(avg_dist),
            "center": center.tolist()
        }
    
    print(f"âœ… èšç±»å®Œæˆï¼")
    
    # æ‰“å°èšç±»æ‘˜è¦
    print("\nğŸ“Š èšç±»æ‘˜è¦:")
    for cluster_name, info in sorted(clusters.items(), key=lambda x: -x[1]['size']):
        print(f"  {cluster_name}: {info['size']} ä¸ªå› å­, ç´§å¯†åº¦={info['compactness']:.3f}")
        print(f"    ä»£è¡¨å› å­: {', '.join(info['factors'][:3])}...")
    
    return {"labels": labels.tolist(), "clusters": clusters}


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å› å­å†—ä½™åº¦å¯è§†åŒ–å·¥å…·')
    parser.add_argument('input', help='å› å­ AST JSON æ–‡ä»¶è·¯å¾„ï¼ˆæˆ–å› å­åº“ JSONï¼‰')
    parser.add_argument('--output', '-o', default='redundancy_plot.html',
                       help='è¾“å‡ºå›¾ç‰‡è·¯å¾„ (é»˜è®¤: redundancy_plot.html)')
    parser.add_argument('--method', '-m', choices=['mds', 'tsne'], default='mds',
                       help='é™ç»´æ–¹æ³• (é»˜è®¤: mds)')
    parser.add_argument('--max-factors', type=int, default=200,
                       help='æœ€å¤§å¤„ç†å› å­æ•° (é»˜è®¤: 200ï¼Œè¿‡å¤šä¼šå¾ˆæ…¢)')
    parser.add_argument('--clusters', '-c', type=int, default=5,
                       help='èšç±»æ•° (é»˜è®¤: 5)')
    parser.add_argument('--static', action='store_true',
                       help='ç”Ÿæˆé™æ€ PNG å›¾ç‰‡è€Œéäº¤äº’å¼ HTML')
    
    # è·ç¦»çŸ©é˜µè¾“å‡ºé€‰é¡¹
    parser.add_argument('--output-matrix', type=str, default=None,
                       help='è¾“å‡ºè·ç¦»çŸ©é˜µçš„è·¯å¾„ (ä¸æŒ‡å®šåˆ™ä¸è¾“å‡ºçŸ©é˜µ)')
    parser.add_argument('--matrix-format', choices=['json', 'csv', 'both'], default='json',
                       help='è·ç¦»çŸ©é˜µè¾“å‡ºæ ¼å¼ (é»˜è®¤: json)')
    parser.add_argument('--matrix-only', action='store_true',
                       help='ä»…è¾“å‡ºè·ç¦»çŸ©é˜µï¼Œä¸ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)
    
    # åŠ è½½æ•°æ®
    factors, statistics = load_factor_ast_data(args.input)
    
    # é™åˆ¶å› å­æ•°é‡
    if len(factors) > args.max_factors:
        print(f"âš ï¸ å› å­æ•°é‡ ({len(factors)}) è¶…è¿‡é™åˆ¶ï¼Œéšæœºé‡‡æ · {args.max_factors} ä¸ª")
        np.random.seed(42)
        indices = np.random.choice(len(factors), args.max_factors, replace=False)
        factors = [factors[i] for i in indices]
        statistics = [statistics[i] for i in indices]
    
    if len(factors) < 3:
        print("âŒ å› å­æ•°é‡å¤ªå°‘ï¼Œè‡³å°‘éœ€è¦ 3 ä¸ª")
        sys.exit(1)
    
    # è®¡ç®—è·ç¦»çŸ©é˜µ
    distance_matrix, factor_ids, factor_names = build_distance_matrix(factors)
    
    # è¾“å‡ºè·ç¦»çŸ©é˜µï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
    if args.output_matrix:
        save_distance_matrix(
            distance_matrix, factor_ids, factor_names,
            args.output_matrix, args.matrix_format
        )
    
    # å¦‚æœä»…è¾“å‡ºçŸ©é˜µï¼Œåˆ°æ­¤ç»“æŸ
    if args.matrix_only:
        if not args.output_matrix:
            # é»˜è®¤è¾“å‡ºè·¯å¾„
            default_matrix_path = args.input.replace('.json', '_distance_matrix.json')
            save_distance_matrix(
                distance_matrix, factor_ids, factor_names,
                default_matrix_path, args.matrix_format
            )
        print("\nâœ… è·ç¦»çŸ©é˜µè¾“å‡ºå®Œæˆï¼")
        return
    
    # é™ç»´
    coords = reduce_to_2d(distance_matrix, method=args.method)
    
    # èšç±»åˆ†æ
    cluster_result = analyze_clusters(coords, factor_names, n_clusters=args.clusters)
    
    # ç”Ÿæˆå›¾è¡¨
    expressions = [f[2] for f in factors]
    
    if args.static:
        output_path = args.output.replace('.html', '.png')
        create_matplotlib_plot(coords, factor_names, statistics, output_path)
    else:
        create_interactive_plot(coords, factor_ids, factor_names, expressions, 
                               statistics, cluster_result['labels'], args.output)
    
    # ä¿å­˜èšç±»ç»“æœ
    cluster_output = args.output.replace('.html', '_clusters.json').replace('.png', '_clusters.json')
    with open(cluster_output, 'w', encoding='utf-8') as f:
        json.dump(cluster_result, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“Š èšç±»ç»“æœå·²ä¿å­˜åˆ°: {cluster_output}")
    
    print("\nâœ… å®Œæˆï¼")


if __name__ == '__main__':
    main()

