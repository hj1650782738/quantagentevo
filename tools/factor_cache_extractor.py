#!/usr/bin/env python3
"""
å› å­ç¼“å­˜æå–å™¨

ä»ä¸»ç¨‹åºçš„æ—¥å¿—å’Œå·¥ä½œç©ºé—´ä¸­æå–å·²è®¡ç®—çš„å› å­æ•°æ®ï¼Œ
è½¬æ¢ä¸º backtest_v2 å¯ç›´æ¥ä½¿ç”¨çš„ç¼“å­˜æ ¼å¼ã€‚

åŠŸèƒ½:
1. ä»æ—¥å¿—ä¸­è§£æå› å­ ID å’Œå·¥ä½œç©ºé—´ UUID çš„å¯¹åº”å…³ç³»
2. æŠŠ result.h5 æ–‡ä»¶é‡å‘½åå¹¶ä¿å­˜åˆ°ç¼“å­˜ç›®å½•
3. ç”Ÿæˆå› å­æ˜ å°„ç´¢å¼•æ–‡ä»¶

ä½¿ç”¨æ–¹å¼:
    python tools/factor_cache_extractor.py --log-dir /path/to/log --output-dir /mnt/DATA/quantagent/AlphaAgent/factor_cache
    
    # æŒ‡å®šå®éªŒ ID
    python tools/factor_cache_extractor.py --exp-id 2026-01-16_17-24-17-907337 --output-dir /mnt/DATA/quantagent/AlphaAgent/factor_cache
"""

import argparse
import hashlib
import json
import pickle
import shutil
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd

# é»˜è®¤è·¯å¾„é…ç½® - æ•°æ®å…¨éƒ¨å­˜å‚¨åˆ° /mnt/DATA/quantagent
# æ—¥å¿—ç›®å½•åˆ—è¡¨ï¼ˆæ–°è·¯å¾„ä¼˜å…ˆï¼Œæ—§è·¯å¾„å…¼å®¹å†å²æ•°æ®ï¼‰
DEFAULT_LOG_DIRS = [
    "/mnt/DATA/quantagent/AlphaAgent/log",  # æ–°è·¯å¾„
    "/home/tjxy/quantagent/AlphaAgent/log",  # æ—§è·¯å¾„ï¼ˆå…¼å®¹å†å²æ•°æ®ï¼‰
]
DEFAULT_LOG_DIR = DEFAULT_LOG_DIRS[0]  # ä¸»æ—¥å¿—ç›®å½•
# å·¥ä½œç©ºé—´åŸºç¡€ç›®å½•ï¼ˆç”¨äºåŠ¨æ€å‘ç°æ‰€æœ‰ workspace ç›®å½•ï¼‰
WORKSPACE_BASE_DIRS = [
    "/mnt/DATA/quantagent/AlphaAgent",  # æ–°è·¯å¾„åŸºç¡€ç›®å½•
    "/home/tjxy/quantagent/AlphaAgent/git_ignore_folder",  # æ—§è·¯å¾„åŸºç¡€ç›®å½•
]
DEFAULT_OUTPUT_DIR = "/mnt/DATA/quantagent/AlphaAgent/factor_cache"
DEFAULT_INDEX_FILE = "/mnt/DATA/quantagent/AlphaAgent/factor_cache_index.json"


def get_all_workspace_dirs() -> List[str]:
    """
    åŠ¨æ€å‘ç°æ‰€æœ‰å·¥ä½œç©ºé—´ç›®å½•
    æ”¯æŒ RD-Agent_workspace å’Œ RD-Agent_workspace_{EXPERIMENT_ID} æ ¼å¼
    """
    workspace_dirs = []
    
    for base_dir in WORKSPACE_BASE_DIRS:
        base_path = Path(base_dir)
        if not base_path.exists():
            continue
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é… RD-Agent_workspace* çš„ç›®å½•
        for ws_dir in base_path.iterdir():
            if ws_dir.is_dir() and ws_dir.name.startswith("RD-Agent_workspace"):
                workspace_dirs.append(str(ws_dir))
    
    # å»é‡å¹¶ä¿æŒé¡ºåºï¼ˆæ–°è·¯å¾„ä¼˜å…ˆï¼‰
    seen = set()
    result = []
    for d in workspace_dirs:
        if d not in seen:
            seen.add(d)
            result.append(d)
    
    return result


# åŠ¨æ€è·å–å·¥ä½œç©ºé—´ç›®å½•åˆ—è¡¨ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
DEFAULT_WORKSPACE_DIRS = get_all_workspace_dirs() or [
    "/mnt/DATA/quantagent/AlphaAgent/RD-Agent_workspace",  # é»˜è®¤å›é€€
]


def get_cache_key(expr: str) -> str:
    """
    ç”Ÿæˆç¼“å­˜é”®ï¼ˆä¸ backtest_v2/factor_calculator.py ä¸­çš„æ–¹æ³•ä¸€è‡´ï¼‰
    """
    return hashlib.md5(expr.encode()).hexdigest()


def find_coder_result_pkls(log_dir: Path, exp_id: Optional[str] = None) -> List[Path]:
    """
    æŸ¥æ‰¾æ‰€æœ‰ coder result çš„ pkl æ–‡ä»¶
    """
    pkl_files = []
    
    if exp_id:
        # æŒ‡å®šå®éªŒ ID
        exp_dirs = [log_dir / exp_id]
    else:
        # éå†æ‰€æœ‰å®éªŒç›®å½•
        exp_dirs = [d for d in log_dir.iterdir() if d.is_dir() and d.name.startswith("2026-")]
    
    for exp_dir in exp_dirs:
        if not exp_dir.exists():
            print(f"âš ï¸  å®éªŒç›®å½•ä¸å­˜åœ¨: {exp_dir}")
            continue
        
        # æŸ¥æ‰¾æ‰€æœ‰ "coder result" ç›®å½•ä¸‹çš„ pkl æ–‡ä»¶
        for pkl_file in exp_dir.rglob("*/d/coder result/*/*.pkl"):
            pkl_files.append(pkl_file)
    
    return pkl_files


def extract_factor_info_from_pkl(pkl_path: Path) -> List[Dict[str, Any]]:
    """
    ä» pkl æ–‡ä»¶ä¸­æå–å› å­ä¿¡æ¯
    æ”¯æŒåœ¨å¤šä¸ª workspace ç›®å½•ä¸­æŸ¥æ‰¾ result.h5
    """
    try:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
        
        if not isinstance(data, list):
            return []
        
        factors = []
        for item in data:
            if hasattr(item, 'workspace_path') and hasattr(item, 'target_task'):
                task = item.target_task
                original_workspace = Path(item.workspace_path)
                workspace_uuid = original_workspace.name  # UUID ç›®å½•å
            
                factor_info = {
                    'workspace_path': str(original_workspace),
                    'factor_name': getattr(task, 'factor_name', ''),
                    'factor_expression': getattr(task, 'factor_expression', ''),
                    'factor_description': getattr(task, 'factor_description', ''),
                    'pkl_source': str(pkl_path),
                }
                
                # åœ¨å¤šä¸ªå¯èƒ½çš„ workspace ç›®å½•ä¸­æŸ¥æ‰¾ result.h5
                result_h5_path = None
                for ws_dir in DEFAULT_WORKSPACE_DIRS:
                    candidate = Path(ws_dir) / workspace_uuid / "result.h5"
                    if candidate.exists():
                        result_h5_path = candidate
                        break
                
                # ä¹Ÿæ£€æŸ¥åŸå§‹è·¯å¾„
                if result_h5_path is None:
                    original_h5 = original_workspace / "result.h5"
                    if original_h5.exists():
                        result_h5_path = original_h5
                
                factor_info['result_h5_exists'] = result_h5_path is not None
                factor_info['result_h5_path'] = str(result_h5_path) if result_h5_path else None
                
                if factor_info['factor_expression']:
                    factors.append(factor_info)
        
        return factors
    except Exception as e:
        print(f"âš ï¸  è§£æ pkl æ–‡ä»¶å¤±è´¥: {pkl_path}, é”™è¯¯: {e}")
        return []


def copy_result_to_cache(
    factor_info: Dict[str, Any],
    output_dir: Path,
    use_symlink: bool = False
) -> Optional[str]:
    """
    å°† result.h5 å¤åˆ¶/é“¾æ¥åˆ°ç¼“å­˜ç›®å½•
    
    è¿”å›ç¼“å­˜æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰
    """
    if not factor_info.get('result_h5_exists') or not factor_info.get('result_h5_path'):
        return None
    
    result_h5_path = Path(factor_info['result_h5_path'])
    expr = factor_info['factor_expression']
    
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = get_cache_key(expr)
    cache_file = output_dir / f"{cache_key}.pkl"
    
    # å¦‚æœç¼“å­˜å·²å­˜åœ¨ï¼Œè·³è¿‡
    if cache_file.exists():
        return f"{cache_key}.pkl"
    
    try:
        # è¯»å– result.h5
        result = pd.read_hdf(result_h5_path, key='data')
        
        # ä¿å­˜ä¸º pklï¼ˆä¸ backtest_v2 çš„ç¼“å­˜æ ¼å¼ä¸€è‡´ï¼‰
        result.to_pickle(cache_file)
        
        return f"{cache_key}.pkl"
    except Exception as e:
        print(f"âš ï¸  å¤„ç†å› å­å¤±è´¥: {factor_info['factor_name']}, é”™è¯¯: {e}")
        return None
    

def extract_factors_to_cache(
    log_dir: Path = None,
    log_dirs: List[Path] = None,
    output_dir: Path = None,
    index_file: Path = None,
    exp_id: Optional[str] = None,
    verbose: bool = True
) -> int:
    """
    æå–å› å­åˆ°ç¼“å­˜ç›®å½• (å¯è¢«å…¶ä»–æ¨¡å—è°ƒç”¨çš„ API)
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•ï¼ˆå•ä¸ªï¼Œå‘åå…¼å®¹ï¼‰
        log_dirs: æ—¥å¿—ç›®å½•åˆ—è¡¨ï¼ˆå¤šä¸ªï¼Œä¼˜å…ˆä½¿ç”¨ï¼‰
        output_dir: ç¼“å­˜è¾“å‡ºç›®å½•
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„
        exp_id: æŒ‡å®šå®éªŒ ID (å¯é€‰)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        int: æ–°å¢çš„å› å­æ•°é‡
    """
    # ç¡®å®šè¦æœç´¢çš„æ—¥å¿—ç›®å½•åˆ—è¡¨
    if log_dirs is not None:
        search_log_dirs = [Path(d) for d in log_dirs]
    elif log_dir is not None:
        search_log_dirs = [Path(log_dir)]
    else:
        # é»˜è®¤æœç´¢æ–°æ—§ä¸¤ä¸ªæ—¥å¿—ç›®å½•
        search_log_dirs = [Path(d) for d in DEFAULT_LOG_DIRS]
    
    output_dir = output_dir or Path(DEFAULT_OUTPUT_DIR)
    index_file = index_file or Path(DEFAULT_INDEX_FILE)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
        
    # ä»æ‰€æœ‰æ—¥å¿—ç›®å½•æŸ¥æ‰¾ pkl æ–‡ä»¶
    if verbose:
        print("ğŸ“‚ æ‰«ææ—¥å¿—ç›®å½•ä¸­çš„å› å­...")
    pkl_files = []
    for search_dir in search_log_dirs:
        if search_dir.exists():
            if verbose:
                print(f"   æœç´¢: {search_dir}")
            pkl_files.extend(find_coder_result_pkls(search_dir, exp_id))
    
    if not pkl_files:
        if verbose:
            print("   æœªæ‰¾åˆ°ä»»ä½•å› å­æ•°æ®")
        return 0
    
    # æå–å› å­ä¿¡æ¯
    all_factors = []
    for pkl_file in pkl_files:
        factors = extract_factor_info_from_pkl(pkl_file)
        all_factors.extend(factors)
    
    # ç»Ÿè®¡æœ‰æ•ˆå› å­
    valid_factors = [f for f in all_factors if f.get('result_h5_exists')]
    
    if verbose:
        print(f"   æ‰¾åˆ° {len(valid_factors)} ä¸ªæœ‰æ•ˆå› å­")
    
    if not valid_factors:
        return 0
    
    # å»é‡
    unique_factors = {}
    for factor in valid_factors:
        expr = factor['factor_expression']
        if expr not in unique_factors:
            unique_factors[expr] = factor
    
    # åŠ è½½å·²æœ‰ç´¢å¼•
    factor_index = {}
    if index_file.exists():
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                factor_index = json.load(f)
        except Exception:
            pass
    
    # å¤„ç†æ¯ä¸ªå› å­
        success_count = 0
        
    for expr, factor in unique_factors.items():
        cache_key = get_cache_key(expr)
        cache_file = output_dir / f"{cache_key}.pkl"
                
        if cache_file.exists():
            # æ›´æ–°ç´¢å¼•
            if cache_key not in factor_index:
                factor_index[cache_key] = {
                    'factor_name': factor['factor_name'],
                    'factor_expression': expr,
                    'cache_file': f"{cache_key}.pkl",
                    'added_at': datetime.now().isoformat(),
                }
            continue
        
        result_file = copy_result_to_cache(factor, output_dir)
        if result_file:
            success_count += 1
            factor_index[cache_key] = {
                'factor_name': factor['factor_name'],
                'factor_expression': expr,
                'factor_description': factor.get('factor_description', ''),
                'cache_file': result_file,
                'source_workspace': factor['workspace_path'],
                'added_at': datetime.now().isoformat(),
            }
    
    # ä¿å­˜ç´¢å¼•
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(factor_index, f, ensure_ascii=False, indent=2)
        
    if verbose and success_count > 0:
        print(f"   âœ“ æ–°æå– {success_count} ä¸ªå› å­åˆ°ç¼“å­˜")
    
    return success_count


def main():
    parser = argparse.ArgumentParser(
        description='å› å­ç¼“å­˜æå–å™¨ - ä»ä¸»ç¨‹åºæ—¥å¿—æå–å·²è®¡ç®—çš„å› å­æ•°æ®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æå–æ‰€æœ‰å®éªŒçš„å› å­ç¼“å­˜
  python tools/factor_cache_extractor.py
  
  # æŒ‡å®šå®éªŒ ID
  python tools/factor_cache_extractor.py --exp-id 2026-01-16_17-24-17-907337
  
  # æŒ‡å®šè¾“å‡ºç›®å½•
  python tools/factor_cache_extractor.py --output-dir /mnt/DATA/quantagent/AlphaAgent/factor_cache
        """
    )
    
    parser.add_argument(
        '--log-dir',
        type=str,
        default=DEFAULT_LOG_DIR,
        help=f'æ—¥å¿—ç›®å½• (é»˜è®¤: {DEFAULT_LOG_DIR})'
    )
    
    parser.add_argument(
        '--exp-id',
        type=str,
        default=None,
        help='æŒ‡å®šå®éªŒ ID (å¦‚: 2026-01-16_17-24-17-907337)'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default=DEFAULT_OUTPUT_DIR,
        help=f'ç¼“å­˜è¾“å‡ºç›®å½• (é»˜è®¤: {DEFAULT_OUTPUT_DIR})'
    )
    
    parser.add_argument(
        '--index-file',
        type=str,
        default=DEFAULT_INDEX_FILE,
        help=f'å› å­ç´¢å¼•æ–‡ä»¶ (é»˜è®¤: {DEFAULT_INDEX_FILE})'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ä»…æ‰«æï¼Œä¸å¤åˆ¶æ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    log_dir = Path(args.log_dir)
    output_dir = Path(args.output_dir)
    index_file = Path(args.index_file)
    
    print("=" * 60)
    print("         å› å­ç¼“å­˜æå–å™¨")
    print("=" * 60)
    print(f"æ—¥å¿—ç›®å½•:     {log_dir}")
    print(f"è¾“å‡ºç›®å½•:     {output_dir}")
    print(f"ç´¢å¼•æ–‡ä»¶:     {index_file}")
    if args.exp_id:
        print(f"æŒ‡å®šå®éªŒ:     {args.exp_id}")
    if args.dry_run:
        print("æ¨¡å¼:         ä»…æ‰«æ (dry-run)")
    print()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰ pkl æ–‡ä»¶
    print("ğŸ“‚ æ‰«ææ—¥å¿—ç›®å½•...")
    pkl_files = find_coder_result_pkls(log_dir, args.exp_id)
    print(f"   æ‰¾åˆ° {len(pkl_files)} ä¸ª coder result pkl æ–‡ä»¶")
    
    # æå–å› å­ä¿¡æ¯
    print("\nğŸ“Š æå–å› å­ä¿¡æ¯...")
    all_factors = []
    for pkl_file in pkl_files:
        factors = extract_factor_info_from_pkl(pkl_file)
        all_factors.extend(factors)
    
    print(f"   æå–åˆ° {len(all_factors)} ä¸ªå› å­")
    
    # ç»Ÿè®¡æœ‰æ•ˆå› å­ï¼ˆæœ‰ result.h5 çš„ï¼‰
    valid_factors = [f for f in all_factors if f.get('result_h5_exists')]
    print(f"   å…¶ä¸­æœ‰æ•ˆå› å­ï¼ˆæœ‰ result.h5ï¼‰: {len(valid_factors)} ä¸ª")
    
    if args.dry_run:
        print("\nğŸ” Dry-run æ¨¡å¼ï¼Œæ˜¾ç¤ºå‰ 10 ä¸ªå› å­:")
        for i, factor in enumerate(valid_factors[:10]):
            print(f"\n  [{i+1}] {factor['factor_name']}")
            print(f"      è¡¨è¾¾å¼: {factor['factor_expression'][:60]}...")
            print(f"      å·¥ä½œç©ºé—´: {factor['workspace_path']}")
        if len(valid_factors) > 10:
            print(f"\n  ... è¿˜æœ‰ {len(valid_factors) - 10} ä¸ªå› å­")
        return
    
    # å¤åˆ¶å› å­åˆ°ç¼“å­˜ç›®å½•
    print(f"\nğŸ“¦ å¤åˆ¶å› å­åˆ°ç¼“å­˜ç›®å½•: {output_dir}")
    
    # å»é‡ï¼šåŒä¸€è¡¨è¾¾å¼åªä¿ç•™ä¸€ä¸ª
    unique_factors = {}
    for factor in valid_factors:
        expr = factor['factor_expression']
        if expr not in unique_factors:
            unique_factors[expr] = factor
    
    print(f"   å»é‡åå”¯ä¸€å› å­: {len(unique_factors)} ä¸ª")
    
    # åŠ è½½å·²æœ‰ç´¢å¼•
    factor_index = {}
    if index_file.exists():
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                factor_index = json.load(f)
            print(f"   åŠ è½½å·²æœ‰ç´¢å¼•: {len(factor_index)} ä¸ªå› å­")
        except Exception as e:
            print(f"   âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
    
    # å¤„ç†æ¯ä¸ªå› å­
    success_count = 0
    skip_count = 0
    fail_count = 0
    
    for expr, factor in unique_factors.items():
        cache_key = get_cache_key(expr)
        cache_file = output_dir / f"{cache_key}.pkl"
        
        if cache_file.exists():
            skip_count += 1
            # æ›´æ–°ç´¢å¼•
            if cache_key not in factor_index:
                factor_index[cache_key] = {
                    'factor_name': factor['factor_name'],
                    'factor_expression': expr,
                    'cache_file': f"{cache_key}.pkl",
                    'added_at': datetime.now().isoformat(),
                }
            continue
        
        result_file = copy_result_to_cache(factor, output_dir)
        if result_file:
            success_count += 1
            # æ›´æ–°ç´¢å¼•
            factor_index[cache_key] = {
                'factor_name': factor['factor_name'],
                'factor_expression': expr,
                'factor_description': factor.get('factor_description', ''),
                'cache_file': result_file,
                'source_workspace': factor['workspace_path'],
                'added_at': datetime.now().isoformat(),
            }
        else:
            fail_count += 1
    
    # ä¿å­˜ç´¢å¼•
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(factor_index, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æå–å®Œæˆ!")
    print(f"   æ–°å¢: {success_count} ä¸ª")
    print(f"   è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {skip_count} ä¸ª")
    print(f"   å¤±è´¥: {fail_count} ä¸ª")
    print(f"   ç´¢å¼•æ€»æ•°: {len(factor_index)} ä¸ª")
    print(f"\nğŸ“ ç¼“å­˜ç›®å½•: {output_dir}")
    print(f"ğŸ“‹ ç´¢å¼•æ–‡ä»¶: {index_file}")
    
    # æ˜¾ç¤ºç¼“å­˜ç›®å½•å¤§å°
    try:
        import subprocess
        result = subprocess.run(['du', '-sh', str(output_dir)], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"ğŸ’¾ ç¼“å­˜å¤§å°: {result.stdout.split()[0]}")
    except:
        pass


if __name__ == '__main__':
    main()

